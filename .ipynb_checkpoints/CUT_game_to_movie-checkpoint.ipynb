{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: numpy in c:\\users\\andre\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (1.19.3)\n",
      "Requirement already satisfied: opencv-python in c:\\users\\andre\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (4.4.0.46)\n",
      "Requirement already satisfied: dlib in c:\\users\\andre\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (19.21.1)\n",
      "Requirement already satisfied: matplotlib in c:\\users\\andre\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (3.3.3)\n",
      "Requirement already satisfied: boto3 in c:\\users\\andre\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (1.17.4)\n",
      "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.3 in c:\\users\\andre\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from matplotlib) (2.4.7)\n",
      "Requirement already satisfied: python-dateutil>=2.1 in c:\\users\\andre\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from matplotlib) (2.8.1)\n",
      "Requirement already satisfied: pillow>=6.2.0 in c:\\users\\andre\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from matplotlib) (8.0.1)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\andre\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from matplotlib) (0.10.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in c:\\users\\andre\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from matplotlib) (1.3.1)\n",
      "Requirement already satisfied: botocore<1.21.0,>=1.20.4 in c:\\users\\andre\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from boto3) (1.20.4)\n",
      "Requirement already satisfied: s3transfer<0.4.0,>=0.3.0 in c:\\users\\andre\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from boto3) (0.3.4)\n",
      "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in c:\\users\\andre\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from boto3) (0.10.0)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\andre\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from python-dateutil>=2.1->matplotlib) (1.15.0)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.25.4 in c:\\users\\andre\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from botocore<1.21.0,>=1.20.4->boto3) (1.26.2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 20.2.1; however, version 21.0.1 is available.\n",
      "You should consider upgrading via the 'c:\\users\\andre\\appdata\\local\\programs\\python\\python38\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    }
   ],
   "source": [
    "!pip install numpy opencv-python dlib matplotlib boto3\n",
    "try:\n",
    "    import torch\n",
    "except:\n",
    "    !pip install pip install torch==1.7.1+cu101 torchvision==0.8.2+cu101 torchaudio===0.7.2 -f https://download.pytorch.org/whl/torch_stable.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import numpy as np\n",
    "import cv2\n",
    "import os\n",
    "import glob\n",
    "import random\n",
    "import time, datetime, sys\n",
    "import zipfile as zf\n",
    "import boto3\n",
    "from PIL import Image\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch.autograd import Variable\n",
    "from torchvision import transforms, datasets\n",
    "from torchvision.utils import save_image, make_grid\n",
    "\n",
    "import dlib\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython import display\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the model name - determines where the outputs are saved\n",
    "model_name = 'G2M-frames-res1'\n",
    "\n",
    "# Variables/Hyperparameters\n",
    "dataset_size = 600\n",
    "generate_dataset = True\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "Tensor = torch.cuda.FloatTensor if torch.cuda.is_available() else torch.Tensor\n",
    "nce_layers = [0, 4, 8, 12, 16]\n",
    "lambda_NCE = 1.0\n",
    "lambda_GAN = 1.0\n",
    "batch_size = 1\n",
    "load_weights = True\n",
    "epoch = 0\n",
    "epochs = 100\n",
    "nonsaturating = False\n",
    "\n",
    "input_size = (3,144,256)\n",
    "res_blocks = 1 # def = 9\n",
    "learning_rate = 0.001\n",
    "betas = (0.9, 0.999)\n",
    "kernel_size = 3\n",
    "\n",
    "D_losses = []\n",
    "GAN_losses = []\n",
    "NCE_losses = []\n",
    "G_total_losses = []\n",
    "\n",
    "# Swap the game and movie datasets, so that translation goes from movie to game instead\n",
    "swap = False\n",
    "\n",
    "# Toggle whether translation is done only on faces\n",
    "faces = False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the original data (and the processed data)\n",
    "s3 = boto3.resource('s3', aws_access_key_id = 'AKIAIOPFTDXA3ZXLK5YA', aws_secret_access_key = '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')\n",
    "bucket = s3.Bucket('vision-dataset-vmrj42')\n",
    "\n",
    "for fname in ['Data.zip', 'datasets.zip']:\n",
    "    if not os.path.isfile(fname):\n",
    "        bucket.download_file(fname, fname)\n",
    "\n",
    "if not os.path.isdir('Data'):\n",
    "    files = zf.ZipFile('Data.zip', 'r')\n",
    "    files.extractall('')\n",
    "if not os.path.isdir('dataset') and not generate_dataset:\n",
    "    files = zf.ZipFile('datasets.zip', 'r')\n",
    "    files.extractall('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a basic dataset for full-frame translation\n",
    "\n",
    "for dname in ['dataset/train/game', \n",
    "              'dataset/train/movie', \n",
    "              'dataset/test/game', \n",
    "              'dataset/test/movie', \n",
    "              'face_dataset/train/game',\n",
    "              'face_dataset/train/movie',\n",
    "              'face_dataset/test/game',\n",
    "              'face_dataset/test/movie']:\n",
    "    if not os.path.isdir(dname):\n",
    "        os.makedirs(dname)\n",
    "\n",
    "if len(os.listdir('dataset/train/game')) < dataset_size:\n",
    "    # get some frames from the game footage\n",
    "    cap = cv2.VideoCapture('Data/game/MafiaVideogame.mp4')\n",
    "    length = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "    frame_count = 0\n",
    "    saved_frames = 0\n",
    "    \n",
    "    face_detector = dlib.get_frontal_face_detector()\n",
    "    faces = []\n",
    "\n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        frame_count += 1\n",
    "        # Save the entire frame as part of the dataset, alternating between the training and testing datasets\n",
    "        if frame_count % (length // (2 * dataset_size)) == 0 and ret:\n",
    "            if saved_frames < dataset_size:\n",
    "                fname = 'dataset/train/game/%d.png' % (saved_frames)\n",
    "            else:\n",
    "                fname = 'dataset/test/game/%d.png' % (saved_frames % dataset_size)\n",
    "            cv2.imwrite(fname, frame)\n",
    "            saved_frames += 1\n",
    "        \n",
    "        # Check if there is a face in every (length // (10 * dataset_size)) frame\n",
    "        if frame_count % (length // (6 * dataset_size)) == 0 and ret:\n",
    "            dets = face_detector(frame, 1)\n",
    "            for i, d in enumerate(dets):\n",
    "                left, top, right, bottom = d.left(), d.top(), d.right(), d.bottom()\n",
    "                if right - left > 60:\n",
    "                    face = frame[top:bottom, left:right]\n",
    "                    if len(face) > 0 and len(face[0]) > 0:\n",
    "                        faces.append(face)\n",
    "    cap.release()\n",
    "    \n",
    "    # Alternating between the training and testing datasets, save the extracted faces\n",
    "    saved_faces = 0\n",
    "    for i, face in enumerate(faces):\n",
    "        if i % (len(faces) // (2 * dataset_size)) == 0:\n",
    "            if saved_faces < dataset_size:\n",
    "                fname = 'face_dataset/train/game/%d.png' % (saved_faces)\n",
    "            else:\n",
    "                fname = 'face_dataset/test/game/%d.png' % (saved_faces % dataset_size)\n",
    "            cv2.imwrite(fname, cv2.resize(face, (input_size[1], input_size[1])))\n",
    "            saved_faces += 1\n",
    "\n",
    "    # get some frames from the movie footage\n",
    "    movie_dirs = ['Data/movie/TheGodfather.mp4', 'Data/movie/TheIrishman.mp4', 'Data/movie/TheSopranos.mp4']\n",
    "\n",
    "    saved_frames = 0\n",
    "    for movie in movie_dirs:\n",
    "        cap = cv2.VideoCapture(movie)\n",
    "        length = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "        frame_count = 0\n",
    "\n",
    "        while cap.isOpened():\n",
    "            ret, frame = cap.read()\n",
    "            if not ret:\n",
    "                break\n",
    "            frame_count += 1\n",
    "            if frame_count % (length // (2 * dataset_size / len(movie_dirs))) == 0 and ret:\n",
    "                if saved_frames < dataset_size:\n",
    "                    fname = 'dataset/train/movie/%d.png' % (saved_frames)\n",
    "                else:\n",
    "                    fname = 'dataset/test/movie/%d.png' % (saved_frames % dataset_size)\n",
    "                cv2.imwrite(fname, frame)\n",
    "                saved_frames += 1\n",
    "\n",
    "\n",
    "        cap.release()\n",
    "        \n",
    "    face_detector = dlib.get_frontal_face_detector()        \n",
    "    faces_dir = 'Data/faces/'\n",
    "    real_faces = os.listdir(faces_dir)\n",
    "    saved_faces = 0\n",
    "    current_face = 0\n",
    "    while saved_faces < dataset_size * 4:\n",
    "        real_face = cv2.imread(faces_dir + real_faces[current_face])\n",
    "        face = []\n",
    "        dets = face_detector(real_face, 1)\n",
    "        for d in dets:\n",
    "            left, top, right, bottom = d.left(), d.top(), d.right(), d.bottom()\n",
    "            if right - left > 60:\n",
    "                face = real_face[top : bottom, left : right]\n",
    "                if len(face) > 0 and len(face[0]) > 0:\n",
    "                    face = cv2.resize(face, (input_size[1], input_size[1]))\n",
    "                    if saved_faces < dataset_size * 2:\n",
    "                        fname = 'face_dataset/train/movie/%d.png' % (saved_faces)\n",
    "                    else:\n",
    "                        fname = 'face_dataset/test/movie/%d.png' % (saved_faces % (dataset_size * 2))\n",
    "                    cv2.imwrite(fname, face)\n",
    "                    saved_faces += 1\n",
    "                    break\n",
    "        current_face += 1\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helpful class for loading both game and movie samples as one dataset\n",
    "class ImageDataset(Dataset):\n",
    "    def __init__(self, root, swap = False, transforms_ = None, unaligned = False, mode = \"train\"):\n",
    "        self.transform = transforms.Compose(transforms_)\n",
    "        self.unaligned = unaligned\n",
    "        if swap:\n",
    "            self.files_game = sorted(glob.glob(os.path.join(root, \"%s/movie\" % mode) + \"/*.*\"))\n",
    "            self.files_movie = sorted(glob.glob(os.path.join(root, \"%s/game\" % mode) + \"/*.*\"))\n",
    "        else:\n",
    "            self.files_game = sorted(glob.glob(os.path.join(root, \"%s/game\" % mode) + \"/*.*\"))\n",
    "            self.files_movie = sorted(glob.glob(os.path.join(root, \"%s/movie\" % mode) + \"/*.*\"))\n",
    "        print(len(os.listdir(os.path.join(root, 'train/movie'))))\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        image_game = Image.open(self.files_game[index % len(self.files_game)])\n",
    "\n",
    "        if self.unaligned:\n",
    "            image_movie = Image.open(self.files_movie[random.randint(0, len(self.files_movie) - 1)])\n",
    "        else:\n",
    "            image_movie = Image.open(self.files_movie[index % len(self.files_movie)])\n",
    "\n",
    "        # Convert grayscale images to rgb\n",
    "        if image_game.mode != \"RGB\":\n",
    "            image_game = to_rgb(image_game)\n",
    "        if image_movie.mode != \"RGB\":\n",
    "            image_movie = to_rgb(image_movie)\n",
    "\n",
    "        item_game = self.transform(image_game)\n",
    "        item_movie = self.transform(image_movie)\n",
    "        \n",
    "        return {\"a\": item_game, \"b\": item_movie}\n",
    "\n",
    "    def __len__(self):\n",
    "        return max(len(self.files_game), len(self.files_movie))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "600\n",
      "600\n"
     ]
    }
   ],
   "source": [
    "# Loading the dataset\n",
    "# Define the dataset-wide transformations\n",
    "\n",
    "transforms_ = [\n",
    "    transforms.Resize(int(input_size[1] * 1.4), Image.BICUBIC),\n",
    "    transforms.RandomCrop((input_size[1],input_size[2])),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
    "]\n",
    "\n",
    "\n",
    "if faces:\n",
    "    dataset_dir = 'face_dataset'\n",
    "else:\n",
    "    dataset_dir = 'dataset'\n",
    "# Training data loader\n",
    "dataloader = DataLoader(\n",
    "    ImageDataset(dataset_dir, swap = swap, transforms_ = transforms_, unaligned = True),\n",
    "    batch_size = batch_size,\n",
    "    shuffle = True,\n",
    "    num_workers = 0,\n",
    ")\n",
    "# Test data loader\n",
    "val_dataloader = DataLoader(\n",
    "    ImageDataset(dataset_dir, swap = swap, transforms_ = transforms_, unaligned = True, mode = 'test'),\n",
    "    batch_size = 5,\n",
    "    shuffle = True,\n",
    "    num_workers = 0,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Other Utils\n",
    "\n",
    "def weights_init_normal(m):\n",
    "    classname = m.__class__.__name__\n",
    "    if classname.find(\"Conv\") != -1:\n",
    "        torch.nn.init.normal_(m.weight.data, 0.0, 0.02)\n",
    "        if hasattr(m, \"bias\") and m.bias is not None:\n",
    "            torch.nn.init.constant_(m.bias.data, 0.0)\n",
    "    elif classname.find(\"BatchNorm2d\") != -1:\n",
    "        torch.nn.init.normal_(m.weight.data, 1.0, 0.02)\n",
    "        torch.nn.init.constant_(m.bias.data, 0.0)\n",
    "        \n",
    "def sample_images(batches_done):\n",
    "    inv_normalize = transforms.Compose(\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(\n",
    "            mean=[-0.5/0.5, -0.5/0.5, -0.5/0.5],\n",
    "            std=[1/0.5, 1/0.5, 1/0.5]\n",
    "        )\n",
    "    )\n",
    "    \"\"\"Saves a generated sample from the test set\"\"\"\n",
    "    imgs = next(iter(val_dataloader))\n",
    "    G.eval()\n",
    "    real_A = Variable(imgs[\"a\"].type(Tensor))\n",
    "    fake_B = inv_normalize(G(real_A))\n",
    "    # Arange images along x-axis\n",
    "    real_A = make_grid(real_A, nrow=5, normalize=True)\n",
    "    fake_B = make_grid(fake_B, nrow=5, normalize=True)\n",
    "    # Arange images along y-axis\n",
    "    image_grid = torch.cat((real_A, fake_B), 1)\n",
    "    if not os.path.isdir('models/%s/samples' % model_name):\n",
    "        os.makedirs('models/%s/samples' % model_name)\n",
    "    save_image(image_grid, \"models/%s/samples/%s.png\" % (model_name, batches_done), normalize=False)\n",
    "    \n",
    "class Normalize(nn.Module):\n",
    "    def __init__(self, power=2):\n",
    "        super(Normalize, self).__init__()\n",
    "        self.power = power\n",
    "\n",
    "    def forward(self, x):\n",
    "        norm = x.pow(self.power).sum(1, keepdim=True).pow(1. / self.power)\n",
    "        out = x.div(norm + 1e-7)\n",
    "        return out\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generator and Discriminator models adapted from https://github.com/eriklindernoren/PyTorch-GAN\n",
    "# PatchSampleF adapted from https://github.com/taesungp/contrastive-unpaired-translation\n",
    "\n",
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, in_features):\n",
    "        super(ResidualBlock, self).__init__()\n",
    "\n",
    "        self.block = nn.Sequential(\n",
    "            nn.ReflectionPad2d(1),\n",
    "            nn.Conv2d(in_features, in_features, 3),\n",
    "            nn.InstanceNorm2d(in_features),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.ReflectionPad2d(1),\n",
    "            nn.Conv2d(in_features, in_features, 3),\n",
    "            nn.InstanceNorm2d(in_features),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x + self.block(x)\n",
    "\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self, input_shape, num_residual_blocks, kernel):\n",
    "        super(Generator, self).__init__()\n",
    "        channels = input_shape[0]\n",
    "\n",
    "        # Initial convolution block\n",
    "        out_features = 64\n",
    "        model = [\n",
    "            nn.ReflectionPad2d(channels),\n",
    "            nn.Conv2d(channels, out_features, 2 * kernel + 1),\n",
    "            nn.InstanceNorm2d(out_features),\n",
    "            nn.ReLU(inplace=True),\n",
    "        ]\n",
    "        in_features = out_features\n",
    "\n",
    "        # Downsampling\n",
    "        for _ in range(2):\n",
    "            out_features *= 2\n",
    "            model += [\n",
    "                nn.Conv2d(in_features, out_features, kernel, stride=2, padding=1),\n",
    "                nn.InstanceNorm2d(out_features),\n",
    "                nn.ReLU(inplace=True),\n",
    "            ]\n",
    "            in_features = out_features\n",
    "\n",
    "        # Residual blocks\n",
    "        for _ in range(num_residual_blocks):\n",
    "            model += [ResidualBlock(out_features)]\n",
    "\n",
    "        # Upsampling\n",
    "        for _ in range(2):\n",
    "            out_features //= 2\n",
    "            model += [\n",
    "                nn.Upsample(scale_factor=2),\n",
    "                nn.Conv2d(in_features, out_features, kernel, stride=1, padding=1),\n",
    "                nn.InstanceNorm2d(out_features),\n",
    "                nn.ReLU(inplace=True),\n",
    "            ]\n",
    "            in_features = out_features\n",
    "\n",
    "        # Output layer\n",
    "        model += [\n",
    "            nn.ReflectionPad2d(channels), \n",
    "            nn.Conv2d(out_features, channels, 2 * kernel + 1), \n",
    "            nn.Tanh()\n",
    "        ]\n",
    "\n",
    "        self.model = nn.Sequential(*model)\n",
    "\n",
    "    def forward(self, x, layers = [], encode_only = False):\n",
    "        if -1 in layers:\n",
    "            layers.append(len(self.model))\n",
    "        if len(layers) > 0:\n",
    "            feat = x\n",
    "            feats = []\n",
    "            for layer_id, layer in enumerate(self.model):\n",
    "                feat = layer(feat)\n",
    "                if layer_id in layers:\n",
    "                    feats.append(feat)\n",
    "                else:\n",
    "                    pass\n",
    "                if layer_id == layers[-1] and encode_only:\n",
    "                    return feats\n",
    "            return feat, feats\n",
    "        else:\n",
    "            return self.model(x)\n",
    "    \n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, input_shape, kernel):\n",
    "        super(Discriminator, self).__init__()\n",
    "        channels, height, width = input_shape\n",
    "        self.output_shape = (1, height // 2 ** 4, width // 2 ** 4)\n",
    "\n",
    "        def discriminator_block(in_filters, out_filters, normalize = True):\n",
    "            \"\"\"Returns downsampling layers of each discriminator block\"\"\"\n",
    "            layers = [nn.Conv2d(in_filters, out_filters, 4, stride=2, padding=1)]\n",
    "            if normalize:\n",
    "                layers.append(nn.InstanceNorm2d(out_filters))\n",
    "            layers.append(nn.LeakyReLU(0.2, inplace=True))\n",
    "            return layers\n",
    "\n",
    "        self.model = nn.Sequential(\n",
    "            *discriminator_block(channels, 64, normalize=False),\n",
    "            *discriminator_block(64, 128),\n",
    "            *discriminator_block(128, 256),\n",
    "            *discriminator_block(256, 512),\n",
    "            nn.ZeroPad2d((1, 0, 1, 0)),\n",
    "            nn.Conv2d(512, 1, 4, padding=1)\n",
    "        )\n",
    "\n",
    "    def forward(self, img):\n",
    "        return self.model(img)\n",
    "    \n",
    "class PatchSampleF(nn.Module):\n",
    "    def __init__(self, use_mlp = True, init_type = 'normal', init_gain = 0.02, nc = 256):\n",
    "        # potential issues: currently, we use the same patch_ids for multiple images in the batch\n",
    "        super(PatchSampleF, self).__init__()\n",
    "        self.l2norm = Normalize(2)\n",
    "        self.use_mlp = use_mlp\n",
    "        self.nc = nc  # hard-coded\n",
    "        self.mlp_init = False\n",
    "        self.init_type = init_type\n",
    "        self.init_gain = init_gain\n",
    "\n",
    "    def create_mlp(self, feats):\n",
    "        for mlp_id, feat in enumerate(feats):\n",
    "            input_nc = feat.shape[1]\n",
    "            mlp = nn.Sequential(*[nn.Linear(input_nc, self.nc), nn.ReLU(), nn.Linear(self.nc, self.nc)])\n",
    "            if torch.cuda.is_available():\n",
    "                mlp.cuda()\n",
    "            setattr(self, 'mlp_%d' % mlp_id, mlp)\n",
    "        if torch.cuda.is_available():\n",
    "            self.to(device)\n",
    "        self.apply(weights_init_normal)\n",
    "        self.mlp_init = True\n",
    "\n",
    "    def forward(self, feats, num_patches = 64, patch_ids = None):\n",
    "        return_ids = []\n",
    "        return_feats = []\n",
    "        if self.use_mlp and not self.mlp_init:\n",
    "            self.create_mlp(feats)\n",
    "        for feat_id, feat in enumerate(feats):\n",
    "            B, H, W = feat.shape[0], feat.shape[2], feat.shape[3]\n",
    "            feat_reshape = feat.permute(0, 2, 3, 1).flatten(1, 2)\n",
    "            if num_patches > 0:\n",
    "                if patch_ids is not None:\n",
    "                    patch_id = patch_ids[feat_id]\n",
    "                else:\n",
    "                    patch_id = torch.randperm(feat_reshape.shape[1], device=feats[0].device)\n",
    "                    patch_id = patch_id[:int(min(num_patches, patch_id.shape[0]))]  # .to(patch_ids.device)\n",
    "                x_sample = feat_reshape[:, patch_id, :].flatten(0, 1)  # reshape(-1, x.shape[1])\n",
    "            else:\n",
    "                x_sample = feat_reshape\n",
    "                patch_id = []\n",
    "            if self.use_mlp:\n",
    "                mlp = getattr(self, 'mlp_%d' % feat_id)\n",
    "                x_sample = mlp(x_sample)\n",
    "            return_ids.append(patch_id)\n",
    "            x_sample = self.l2norm(x_sample)\n",
    "\n",
    "            if num_patches == 0:\n",
    "                x_sample = x_sample.permute(0, 2, 1).reshape([B, x_sample.shape[-1], H, W])\n",
    "            return_feats.append(x_sample)\n",
    "        return return_feats, return_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Class to create separate losses for each feature, from https://github.com/taesungp/contrastive-unpaired-translation\n",
    "class PatchNCELoss(nn.Module):\n",
    "    def __init__(self, batch_size, nce_T = 0.07):\n",
    "        super().__init__()\n",
    "        self.batch_size = batch_size\n",
    "        self.nce_T = nce_T\n",
    "        self.cross_entropy_loss = torch.nn.CrossEntropyLoss(reduction='none')\n",
    "        self.mask_dtype = torch.bool\n",
    "\n",
    "    def forward(self, feat_q, feat_k):\n",
    "        batchSize = feat_q.shape[0]\n",
    "        dim = feat_q.shape[1]\n",
    "        feat_k = feat_k.detach()\n",
    "\n",
    "        # pos logit\n",
    "        l_pos = torch.bmm(feat_q.view(batchSize, 1, -1), feat_k.view(batchSize, -1, 1))\n",
    "        l_pos = l_pos.view(batchSize, 1)\n",
    "\n",
    "        batch_dim_for_bmm = self.batch_size\n",
    "\n",
    "        # reshape features to batch size\n",
    "        feat_q = feat_q.view(batch_dim_for_bmm, -1, dim)\n",
    "        feat_k = feat_k.view(batch_dim_for_bmm, -1, dim)\n",
    "        npatches = feat_q.size(1)\n",
    "        l_neg_curbatch = torch.bmm(feat_q, feat_k.transpose(2, 1))\n",
    "\n",
    "        # diagonal entries are similarity between same features, and hence meaningless.\n",
    "        # just fill the diagonal with very small number, which is exp(-10) and almost zero\n",
    "        diagonal = torch.eye(npatches, device=feat_q.device, dtype=self.mask_dtype)[None, :, :]\n",
    "        l_neg_curbatch.masked_fill_(diagonal, -10.0)\n",
    "        l_neg = l_neg_curbatch.view(-1, npatches)\n",
    "\n",
    "        out = torch.cat((l_pos, l_neg), dim=1) / self.nce_T\n",
    "\n",
    "        loss = self.cross_entropy_loss(out, torch.zeros(out.size(0), dtype=torch.long,\n",
    "                                                        device=feat_q.device))\n",
    "\n",
    "        return loss\n",
    "    \n",
    "def nonsaturating_loss(prediction, is_real):\n",
    "    if is_real.mean() == 1:\n",
    "        loss = F.softplus(-prediction).view(prediction.size(0), -1).mean(dim = 1)\n",
    "    else:\n",
    "        loss = F.softplus(prediction).view(prediction.size(0), -1).mean(dim = 1)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the networks, load the most recent saved models, and set the loss functions and optimizers\n",
    "\n",
    "if faces:\n",
    "    input_size = (input_size[0], input_size[1], input_size[1])\n",
    "G = Generator(input_size, res_blocks, kernel_size).to(device)\n",
    "D = Discriminator(input_size, kernel_size).to(device)\n",
    "Sampler = PatchSampleF(batch_size).to(device)\n",
    "\n",
    "if nonsaturating:\n",
    "    criterion_GAN = nonsaturating_loss\n",
    "else:\n",
    "    criterion_GAN = torch.nn.MSELoss().to(device)\n",
    "criterion_NCE = []\n",
    "\n",
    "for nce_layer in nce_layers:\n",
    "    criterion_NCE.append(PatchNCELoss(batch_size).to(device))\n",
    "\n",
    "G.apply(weights_init_normal)\n",
    "D.apply(weights_init_normal)\n",
    "\n",
    "optimizer_G = torch.optim.Adam(G.parameters(), lr = learning_rate, betas = betas)\n",
    "optimizer_D = torch.optim.Adam(D.parameters(), lr = learning_rate, betas = betas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters for the sampler are not made until the first forward pass through the Sampler network\n",
    "# Hence, we do a 'trial' training pass before setting the optimizer for the Sampler\n",
    "for i, batch in enumerate(dataloader):\n",
    "    x, y = Variable(batch[\"a\"].type(Tensor)), Variable(batch[\"b\"].type(Tensor))\n",
    "    \n",
    "    real = Variable(Tensor(np.ones((x.size(0), *D.output_shape))), requires_grad = False)\n",
    "    fake = Variable(Tensor(np.zeros((x.size(0), *D.output_shape))), requires_grad = False)\n",
    "    \n",
    "    D.eval()\n",
    "    G.eval()\n",
    "    Sampler.eval()\n",
    "    # get the fake loss\n",
    "    fake_y = G(x)\n",
    "    D_fake = D(fake_y.detach())\n",
    "    loss_D_fake = criterion_GAN(D_fake, fake).mean()\n",
    "    # get the real loss\n",
    "    D_real = D(y)\n",
    "    loss_D_real = criterion_GAN(D_real, real).mean()\n",
    "    # combine loss and calculate gradients\n",
    "    loss_D = (loss_D_fake + loss_D_real) * 0.5\n",
    "    loss_D.backward()\n",
    "\n",
    "    # get the fake GAN loss\n",
    "    D_fake = D(fake_y)\n",
    "    loss_G_GAN = lambda_GAN * criterion_GAN(D_fake, real).mean()\n",
    "    total_nce_loss = 0\n",
    "    for fake, real in [(fake_y, x), (y, G(y))]:\n",
    "        # get the NCE loss\n",
    "        feat_q = G(fake, nce_layers, encode_only = True)\n",
    "        feat_k = G(real, nce_layers, encode_only = True)\n",
    "\n",
    "        feat_k_pool, sample_ids = Sampler(feat_k, 256, None)\n",
    "        feat_q_pool, _ = Sampler(feat_q, 256, sample_ids)\n",
    "\n",
    "        total_nce_loss = 0.0\n",
    "        for f_q, f_k, crit, nce_layer in zip(feat_q_pool, feat_k_pool, criterion_NCE, nce_layers):\n",
    "            loss = crit(f_q, f_k) * lambda_NCE\n",
    "            total_nce_loss += loss.mean()\n",
    "\n",
    "        nce_loss = total_nce_loss / len(nce_layers)\n",
    "        total_nce_loss += nce_loss\n",
    "\n",
    "    loss_G = loss_G_GAN + total_nce_loss\n",
    "    loss_G.backward()\n",
    "    \n",
    "    break\n",
    "\n",
    "optimizer_Sampler = torch.optim.Adam(Sampler.parameters(), lr = learning_rate, betas = betas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the most recently saved models if available\n",
    "if os.path.isdir('models/%s' % model_name) and load_weights:\n",
    "    # Get the most recent model and load them\n",
    "    epoch = max([int(fname[8:-4]) for fname in os.listdir('models/%s' % model_name) if 'Sampler' in fname])\n",
    "    G.load_state_dict(torch.load('models/%s/G_%d.pth' % (model_name, epoch)))\n",
    "    D.load_state_dict(torch.load('models/%s/D_%d.pth' % (model_name, epoch)))\n",
    "    Sampler.load_state_dict(torch.load('models/%s/Sampler_%d.pth' % (model_name, epoch)))\n",
    "    # Load the losses as well, for plotting\n",
    "    losses = np.load('models/%s/losses_%d.npy' % (model_name, epoch))\n",
    "    D_losses = list(losses[0])\n",
    "    GAN_losses = list(losses[1])\n",
    "    NCE_losses = list(losses[2])\n",
    "    G_total_losses = list(losses[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 54/200] [Batch 426/600] [D loss: 0.171564] [GAN loss: 0.316007, NCE loss: 1.383622, Total: 4.466874] ETA: 3:09:02.95957588381"
     ]
    }
   ],
   "source": [
    "# Training Loop, adapted from https://github.com/taesungp/contrastive-unpaired-translation\n",
    "\n",
    "prev_time = time.time()\n",
    "for epoch in range(epoch, epochs):\n",
    "    total_D_loss = 0\n",
    "    total_GAN_loss = 0\n",
    "    total_NCE_loss = 0\n",
    "    total_G_loss = 0\n",
    "    for i, batch in enumerate(dataloader):\n",
    "        x, y = Variable(batch[\"a\"].type(Tensor)), Variable(batch[\"b\"].type(Tensor))\n",
    "        \n",
    "        # Adversarial ground truths\n",
    "        real = Variable(Tensor(np.ones((x.size(0), *D.output_shape))), requires_grad = False)\n",
    "        fake = Variable(Tensor(np.zeros((x.size(0), *D.output_shape))), requires_grad = False)\n",
    "        \n",
    "        # train discriminator\n",
    "        D.train()\n",
    "        optimizer_D.zero_grad()\n",
    "        # get the fake loss\n",
    "        fake_y = G(x)\n",
    "        D_fake = D(fake_y.detach())\n",
    "        loss_D_fake = criterion_GAN(D_fake, fake).mean()\n",
    "        # get the real loss\n",
    "        D_real = D(y)\n",
    "        loss_D_real = criterion_GAN(D_real, real).mean()\n",
    "        # combine loss and calculate gradients\n",
    "        loss_D = (loss_D_fake + loss_D_real) * 0.5\n",
    "        loss_D.backward()\n",
    "        optimizer_D.step()\n",
    "        \n",
    "        # train generator\n",
    "        G.train()\n",
    "        optimizer_G.zero_grad()\n",
    "        optimizer_Sampler.zero_grad()\n",
    "        # get the fake GAN loss\n",
    "        D_fake = D(fake_y)\n",
    "        loss_G_GAN = criterion_GAN(D_fake, real).mean()\n",
    "        # get the NCE loss\n",
    "        Sampler.train()\n",
    "        total_nce_loss = 0\n",
    "        for fake, real in [(fake_y, x), (y, G(y))]:\n",
    "            feat_q = G(fake_y, nce_layers, encode_only = True)\n",
    "            feat_k = G(x, nce_layers, encode_only = True)\n",
    "\n",
    "            feat_k_pool, sample_ids = Sampler(feat_k, 256, None)\n",
    "            feat_q_pool, _ = Sampler(feat_q, 256, sample_ids)\n",
    "\n",
    "            total_nce_loss = 0.0\n",
    "            for f_q, f_k, crit, nce_layer in zip(feat_q_pool, feat_k_pool, criterion_NCE, nce_layers):\n",
    "                loss = crit(f_q, f_k) * lambda_NCE\n",
    "                total_nce_loss += loss.mean()\n",
    "\n",
    "            nce_loss = total_nce_loss / len(nce_layers)\n",
    "            total_nce_loss += nce_loss\n",
    "        total_nce_loss *= 0.5\n",
    "        \n",
    "        loss_G = loss_G_GAN + total_nce_loss\n",
    "        loss_G.backward()\n",
    "        optimizer_G.step()\n",
    "        optimizer_Sampler.step()\n",
    "        \n",
    "        # --------------\n",
    "        #  Log Progress\n",
    "        # --------------\n",
    "\n",
    "        # Determine approximate time left\n",
    "        batches_done = epoch * len(dataloader) + i\n",
    "        batches_left = 200 * len(dataloader) - batches_done\n",
    "        time_left = datetime.timedelta(seconds=batches_left * (time.time() - prev_time))\n",
    "        prev_time = time.time()\n",
    "\n",
    "        # Print log\n",
    "        sys.stdout.write(\n",
    "            \"\\r[Epoch %d/%d] [Batch %d/%d] [D loss: %f] [GAN loss: %f, NCE loss: %f, Total: %f] ETA: %s\"\n",
    "            % (\n",
    "                epoch,\n",
    "                200,\n",
    "                i,\n",
    "                len(dataloader),\n",
    "                loss_D.item(),\n",
    "                loss_G_GAN.item(),\n",
    "                nce_loss.item(),\n",
    "                loss_G.item(),\n",
    "                time_left,\n",
    "            )\n",
    "        )\n",
    "        \n",
    "        total_D_loss += loss_D.item()\n",
    "        total_GAN_loss += loss_G_GAN.item()\n",
    "        total_NCE_loss += nce_loss.item()\n",
    "        total_G_loss += loss_G.item()\n",
    "        \n",
    "        # If at sample interval save image\n",
    "        if batches_done % 300 == 0:\n",
    "            sample_images(batches_done)\n",
    "            \n",
    "    D_losses.append(total_D_loss / len(dataloader))\n",
    "    GAN_losses.append(total_GAN_loss / len(dataloader))\n",
    "    NCE_losses.append(total_NCE_loss / len(dataloader))\n",
    "    G_total_losses.append(total_G_loss / len(dataloader))\n",
    "    display.clear_output(wait = True)\n",
    "    time.sleep(1)\n",
    "    plt.clf()\n",
    "    plt.ylabel('Mean Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.plot(D_losses, color = 'r')\n",
    "    plt.plot(GAN_losses, color = 'g')\n",
    "    plt.plot(NCE_losses, color = 'b')\n",
    "    plt.savefig('models/%s/loss_plot.png' % model_name)\n",
    "    display.display(plt.gcf())\n",
    "    \n",
    "    if not os.path.isdir('models'):\n",
    "        os.makedirs('models')\n",
    "    if not os.path.isdir('models/%s' % model_name):\n",
    "        os.makedirs('models/%s' % model_name)\n",
    "    torch.save(G.state_dict(), \"models/%s/G_%d.pth\" % (model_name, epoch))\n",
    "    torch.save(D.state_dict(), \"models/%s/D_%d.pth\" % (model_name, epoch))\n",
    "    torch.save(Sampler.state_dict(), \"models/%s/Sampler_%d.pth\" % (model_name, epoch))\n",
    "    np.save('models/%s/losses_%d.npy' % (model_name, epoch), np.array([D_losses, GAN_losses, NCE_losses, G_total_losses]))\n",
    "    try:\n",
    "        os.remove('models/%s/G_%d.pth' % (model_name, epoch - 1))\n",
    "        os.remove('models/%s/D_%d.pth' % (model_name, epoch - 1))\n",
    "        os.remove('models/%s/Sampler_%d.pth' % (model_name, epoch - 1))\n",
    "        os.remove('models/%s/losses_%d.npy' % (model_name, epoch - 1))\n",
    "    except:\n",
    "        pass\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
