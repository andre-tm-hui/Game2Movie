{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import numpy as np\n",
    "import cv2\n",
    "import os\n",
    "import glob\n",
    "import random\n",
    "import time, datetime, sys\n",
    "from PIL import Image\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch.autograd import Variable\n",
    "from torchvision import transforms, datasets\n",
    "from torchvision.utils import save_image, make_grid\n",
    "\n",
    "import dlib\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython import display\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Variables\n",
    "dataset_size = 900\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "Tensor = torch.cuda.FloatTensor if torch.cuda.is_available() else torch.Tensor\n",
    "nce_layers = [0, 4, 8, 12, 16]\n",
    "lambda_NCE = 1.0\n",
    "lambda_GAN = 1.0\n",
    "batch_size = 1\n",
    "input_size = (3,216,384)\n",
    "res_blocks = 9\n",
    "load_weights = True\n",
    "epoch = 0\n",
    "epochs = 200\n",
    "nonsaturating = False\n",
    "\n",
    "D_losses = []\n",
    "GAN_losses = []\n",
    "NCE_losses = []\n",
    "G_total_losses = []\n",
    "\n",
    "# Swap the game and movie datasets, so that translation goes from movie to game instead\n",
    "swap = False\n",
    "\n",
    "# Toggle whether translation is done only on faces\n",
    "faces = True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a basic dataset for full-frame translation\n",
    "\n",
    "if not os.path.isdir('./dataset'):\n",
    "    os.mkdir('./dataset')\n",
    "    os.mkdir('./dataset/train')\n",
    "    os.mkdir('./dataset/train/game')\n",
    "    os.mkdir('./dataset/train/movie')\n",
    "    os.mkdir('./dataset/test')\n",
    "    os.mkdir('./dataset/test/game')\n",
    "    os.mkdir('./dataset/test/movie')\n",
    "    os.mkdir('./face_dataset')\n",
    "    os.mkdir('./face_dataset/train')\n",
    "    os.mkdir('./face_dataset/train/game')\n",
    "    os.mkdir('./face_dataset/train/movie')\n",
    "    os.mkdir('./face_dataset/test')\n",
    "    os.mkdir('./face_dataset/test/game')\n",
    "    os.mkdir('./face_dataset/test/movie')\n",
    "\n",
    "    # get some frames from the game footage\n",
    "    cap = cv2.VideoCapture('./Data/game/MafiaVideogame.mp4')\n",
    "    length = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "    frame_count = 0\n",
    "    saved_frames = 0\n",
    "    \n",
    "    face_detector = dlib.get_frontal_face_detector()\n",
    "    faces = []\n",
    "\n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        frame_count += 1\n",
    "        # Save the entire frame as part of the dataset, alternating between the training and testing datasets\n",
    "        if frame_count % (length // (2 * dataset_size)) == 0 and ret:\n",
    "            if saved_frames % 2 == 0:\n",
    "                fname = './dataset/train/game/%d.png' % (saved_frames // 2)\n",
    "            else:\n",
    "                fname = './dataset/test/game/%d.png' % (saved_frames // 2)\n",
    "            cv2.imwrite(fname, frame)\n",
    "            saved_frames += 1\n",
    "        \n",
    "        # Check if there is a face in every (length // (10 * dataset_size)) frame\n",
    "        if frame_count % (length // (10 * dataset_size)) == 0 and ret:\n",
    "            dets = face_detector(frame, 1)\n",
    "            for i, d in enumerate(dets):\n",
    "                left, top, right, bottom = d.left(), d.top(), d.right(), d.bottom()\n",
    "                if right - left > 60:\n",
    "                    face = frame[top:bottom, left:right]\n",
    "                    if len(face) > 0 and len(face[0]) > 0:\n",
    "                        faces.append(face)\n",
    "    cap.release()\n",
    "    \n",
    "    # Alternating between the training and testing datasets, save the extracted faces\n",
    "    saved_faces = 0\n",
    "    for i, face in enumerate(faces):\n",
    "        if i % (len(faces) // (2 * dataset_size)) == 0:\n",
    "            if saved_faces % 2 == 0:\n",
    "                fname = './face_dataset/train/game/%d.png' % (saved_faces // 2)\n",
    "            else:\n",
    "                fname = './face_dataset/test/game/%d.png' % (saved_faces // 2)\n",
    "            cv2.imwrite(fname, cv2.resize(face, (input_size[1], input_size[1])))\n",
    "            saved_faces += 1\n",
    "\n",
    "    # get some frames from the movie footage\n",
    "    movie_dirs = ['./Data/movie/TheGodfather.mp4', './Data/movie/TheIrishman.mp4', './Data/movie/TheSopranos.mp4']\n",
    "\n",
    "    saved_frames = 0\n",
    "    for movie in movie_dirs:\n",
    "        cap = cv2.VideoCapture(movie)\n",
    "        length = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "        frame_count = 0\n",
    "\n",
    "        while cap.isOpened():\n",
    "            ret, frame = cap.read()\n",
    "            if not ret:\n",
    "                break\n",
    "            frame_count += 1\n",
    "            if frame_count % (length // (2 * dataset_size / len(movie_dirs))) == 0 and ret:\n",
    "                if saved_frames % 2 == 0:\n",
    "                    fname = './dataset/train/movie/%d.png' % (saved_frames // 2)\n",
    "                else:\n",
    "                    fname = './dataset/test/movie/%d.png' % (saved_frames // 2)\n",
    "                cv2.imwrite(fname, frame)\n",
    "                saved_frames += 1\n",
    "\n",
    "\n",
    "        cap.release()\n",
    "        \n",
    "    face_detector = dlib.get_frontal_face_detector()        \n",
    "    faces_dir = './Data/faces/'\n",
    "    real_faces = os.listdir(faces_dir)\n",
    "    saved_faces = 0\n",
    "    current_face = 0\n",
    "    while saved_faces < dataset_size * 20:\n",
    "        real_face = cv2.imread(faces_dir + real_faces[current_face])\n",
    "        face = []\n",
    "        dets = face_detector(real_face, 1)\n",
    "        for d in dets:\n",
    "            left, top, right, bottom = d.left(), d.top(), d.right(), d.bottom()\n",
    "            if right - left > 60:\n",
    "                face = real_face[top : bottom, left : right]\n",
    "                if len(face) > 0 and len(face[0]) > 0:\n",
    "                    face = cv2.resize(face, (input_size[1], input_size[1]))\n",
    "                    if saved_faces % 2 == 0:\n",
    "                        fname = './face_dataset/train/movie/%d.png' % (saved_faces // 2)\n",
    "                    else:\n",
    "                        fname = './face_dataset/test/movie/%d.png' % (saved_faces // 2)\n",
    "                    cv2.imwrite(fname, face)\n",
    "                    saved_faces += 1\n",
    "                    break\n",
    "        current_face += 1\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helpful class for loading both game and movie samples as one dataset\n",
    "class ImageDataset(Dataset):\n",
    "    def __init__(self, root, swap = False, transforms_ = None, unaligned = False, mode = \"train\"):\n",
    "        self.transform = transforms.Compose(transforms_)\n",
    "        self.unaligned = unaligned\n",
    "\n",
    "        self.files_game = sorted(glob.glob(os.path.join(root, \"%s/game\" % mode) + \"/*.*\"))\n",
    "        self.files_movie = sorted(glob.glob(os.path.join(root, \"%s/movie\" % mode) + \"/*.*\"))\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        image_game = Image.open(self.files_game[index % len(self.files_game)])\n",
    "\n",
    "        if self.unaligned:\n",
    "            image_movie = Image.open(self.files_movie[random.randint(0, len(self.files_movie) - 1)])\n",
    "        else:\n",
    "            image_movie = Image.open(self.files_movie[index % len(self.files_movie)])\n",
    "\n",
    "        # Convert grayscale images to rgb\n",
    "        if image_game.mode != \"RGB\":\n",
    "            image_game = to_rgb(image_game)\n",
    "        if image_movie.mode != \"RGB\":\n",
    "            image_movie = to_rgb(image_movie)\n",
    "\n",
    "        item_game = self.transform(image_game)\n",
    "        item_movie = self.transform(image_movie)\n",
    "        \n",
    "        return {\"a\": item_game, \"b\": item_movie}\n",
    "\n",
    "    def __len__(self):\n",
    "        return max(len(self.files_game), len(self.files_movie))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the dataset\n",
    "# Define the dataset-wide transformations\n",
    "\n",
    "transforms_ = [\n",
    "    transforms.Resize(int(input_size[1] * 1.4), Image.BICUBIC),\n",
    "    transforms.RandomCrop((input_size[1],input_size[1])),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
    "]\n",
    "\n",
    "if faces:\n",
    "    dataset_dir = './face_dataset'\n",
    "else:\n",
    "    dataset_dir = './dataset'\n",
    "# Training data loader\n",
    "dataloader = DataLoader(\n",
    "    ImageDataset(dataset_dir, swap = swap, transforms_ = transforms_, unaligned = True),\n",
    "    batch_size = batch_size,\n",
    "    shuffle = True,\n",
    "    num_workers = 0,\n",
    ")\n",
    "# Test data loader\n",
    "val_dataloader = DataLoader(\n",
    "    ImageDataset(dataset_dir, swap = swap, transforms_ = transforms_, unaligned = True, mode = 'test'),\n",
    "    batch_size = 5,\n",
    "    shuffle = True,\n",
    "    num_workers = 0,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Other Utils\n",
    "\n",
    "def weights_init_normal(m):\n",
    "    classname = m.__class__.__name__\n",
    "    if classname.find(\"Conv\") != -1:\n",
    "        torch.nn.init.normal_(m.weight.data, 0.0, 0.02)\n",
    "        if hasattr(m, \"bias\") and m.bias is not None:\n",
    "            torch.nn.init.constant_(m.bias.data, 0.0)\n",
    "    elif classname.find(\"BatchNorm2d\") != -1:\n",
    "        torch.nn.init.normal_(m.weight.data, 1.0, 0.02)\n",
    "        torch.nn.init.constant_(m.bias.data, 0.0)\n",
    "        \n",
    "def sample_images(batches_done):\n",
    "    \"\"\"Saves a generated sample from the test set\"\"\"\n",
    "    imgs = next(iter(val_dataloader))\n",
    "    G.eval()\n",
    "    real_A = Variable(imgs[\"a\"].type(Tensor))\n",
    "    fake_B = G(real_A)\n",
    "    # Arange images along x-axis\n",
    "    real_A = make_grid(real_A, nrow=5, normalize=True)\n",
    "    fake_B = make_grid(fake_B, nrow=5, normalize=True)\n",
    "    # Arange images along y-axis\n",
    "    image_grid = torch.cat((real_A, fake_B), 1)\n",
    "    if not os.path.isdir('images'):\n",
    "        os.mkdir('images')\n",
    "    save_image(image_grid, \"images/%s.png\" % (batches_done), normalize=False)\n",
    "    \n",
    "class Normalize(nn.Module):\n",
    "    def __init__(self, power=2):\n",
    "        super(Normalize, self).__init__()\n",
    "        self.power = power\n",
    "\n",
    "    def forward(self, x):\n",
    "        norm = x.pow(self.power).sum(1, keepdim=True).pow(1. / self.power)\n",
    "        out = x.div(norm + 1e-7)\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generator and Discriminator models adapted from https://github.com/eriklindernoren/PyTorch-GAN\n",
    "# PatchSampleF adapted from https://github.com/taesungp/contrastive-unpaired-translation\n",
    "\n",
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, in_features):\n",
    "        super(ResidualBlock, self).__init__()\n",
    "\n",
    "        self.block = nn.Sequential(\n",
    "            nn.ReflectionPad2d(1),\n",
    "            nn.Conv2d(in_features, in_features, 3),\n",
    "            nn.InstanceNorm2d(in_features),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.ReflectionPad2d(1),\n",
    "            nn.Conv2d(in_features, in_features, 3),\n",
    "            nn.InstanceNorm2d(in_features),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x + self.block(x)\n",
    "\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self, input_shape, num_residual_blocks):\n",
    "        super(Generator, self).__init__()\n",
    "        channels = input_shape[0]\n",
    "\n",
    "        # Initial convolution block\n",
    "        out_features = 64\n",
    "        model = [\n",
    "            nn.ReflectionPad2d(channels),\n",
    "            nn.Conv2d(channels, out_features, 7),\n",
    "            nn.InstanceNorm2d(out_features),\n",
    "            nn.ReLU(inplace=True),\n",
    "        ]\n",
    "        in_features = out_features\n",
    "\n",
    "        # Downsampling\n",
    "        for _ in range(2):\n",
    "            out_features *= 2\n",
    "            model += [\n",
    "                nn.Conv2d(in_features, out_features, 3, stride=2, padding=1),\n",
    "                nn.InstanceNorm2d(out_features),\n",
    "                nn.ReLU(inplace=True),\n",
    "            ]\n",
    "            in_features = out_features\n",
    "\n",
    "        # Residual blocks\n",
    "        for _ in range(num_residual_blocks):\n",
    "            model += [ResidualBlock(out_features)]\n",
    "\n",
    "        # Upsampling\n",
    "        for _ in range(2):\n",
    "            out_features //= 2\n",
    "            model += [\n",
    "                nn.Upsample(scale_factor=2),\n",
    "                nn.Conv2d(in_features, out_features, 3, stride=1, padding=1),\n",
    "                nn.InstanceNorm2d(out_features),\n",
    "                nn.ReLU(inplace=True),\n",
    "            ]\n",
    "            in_features = out_features\n",
    "\n",
    "        # Output layer\n",
    "        model += [\n",
    "            nn.ReflectionPad2d(channels), \n",
    "            nn.Conv2d(out_features, channels, 7), \n",
    "            nn.Tanh()\n",
    "        ]\n",
    "\n",
    "        self.model = nn.Sequential(*model)\n",
    "\n",
    "    def forward(self, x, layers = [], encode_only = False):\n",
    "        if -1 in layers:\n",
    "            layers.append(len(self.model))\n",
    "        if len(layers) > 0:\n",
    "            feat = x\n",
    "            feats = []\n",
    "            for layer_id, layer in enumerate(self.model):\n",
    "                feat = layer(feat)\n",
    "                if layer_id in layers:\n",
    "                    feats.append(feat)\n",
    "                else:\n",
    "                    pass\n",
    "                if layer_id == layers[-1] and encode_only:\n",
    "                    return feats\n",
    "            return feat, feats\n",
    "        else:\n",
    "            return self.model(x)\n",
    "    \n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, input_shape):\n",
    "        super(Discriminator, self).__init__()\n",
    "        channels, height, width = input_shape\n",
    "        self.output_shape = (1, height // 2 ** 4, width // 2 ** 4)\n",
    "\n",
    "        def discriminator_block(in_filters, out_filters, normalize = True):\n",
    "            \"\"\"Returns downsampling layers of each discriminator block\"\"\"\n",
    "            layers = [nn.Conv2d(in_filters, out_filters, 4, stride=2, padding=1)]\n",
    "            if normalize:\n",
    "                layers.append(nn.InstanceNorm2d(out_filters))\n",
    "            layers.append(nn.LeakyReLU(0.2, inplace=True))\n",
    "            return layers\n",
    "\n",
    "        self.model = nn.Sequential(\n",
    "            *discriminator_block(channels, 64, normalize=False),\n",
    "            *discriminator_block(64, 128),\n",
    "            *discriminator_block(128, 256),\n",
    "            *discriminator_block(256, 512),\n",
    "            nn.ZeroPad2d((1, 0, 1, 0)),\n",
    "            nn.Conv2d(512, 1, 4, padding=1)\n",
    "        )\n",
    "\n",
    "    def forward(self, img):\n",
    "        return self.model(img)\n",
    "    \n",
    "class PatchSampleF(nn.Module):\n",
    "    def __init__(self, use_mlp = True, init_type = 'normal', init_gain = 0.02, nc = 256):\n",
    "        # potential issues: currently, we use the same patch_ids for multiple images in the batch\n",
    "        super(PatchSampleF, self).__init__()\n",
    "        self.l2norm = Normalize(2)\n",
    "        self.use_mlp = use_mlp\n",
    "        self.nc = nc  # hard-coded\n",
    "        self.mlp_init = False\n",
    "        self.init_type = init_type\n",
    "        self.init_gain = init_gain\n",
    "\n",
    "    def create_mlp(self, feats):\n",
    "        for mlp_id, feat in enumerate(feats):\n",
    "            input_nc = feat.shape[1]\n",
    "            mlp = nn.Sequential(*[nn.Linear(input_nc, self.nc), nn.ReLU(), nn.Linear(self.nc, self.nc)])\n",
    "            if torch.cuda.is_available():\n",
    "                mlp.cuda()\n",
    "            setattr(self, 'mlp_%d' % mlp_id, mlp)\n",
    "        if torch.cuda.is_available():\n",
    "            self.to(device)\n",
    "        self.apply(weights_init_normal)\n",
    "        self.mlp_init = True\n",
    "\n",
    "    def forward(self, feats, num_patches = 64, patch_ids = None):\n",
    "        return_ids = []\n",
    "        return_feats = []\n",
    "        if self.use_mlp and not self.mlp_init:\n",
    "            self.create_mlp(feats)\n",
    "        for feat_id, feat in enumerate(feats):\n",
    "            B, H, W = feat.shape[0], feat.shape[2], feat.shape[3]\n",
    "            feat_reshape = feat.permute(0, 2, 3, 1).flatten(1, 2)\n",
    "            if num_patches > 0:\n",
    "                if patch_ids is not None:\n",
    "                    patch_id = patch_ids[feat_id]\n",
    "                else:\n",
    "                    patch_id = torch.randperm(feat_reshape.shape[1], device=feats[0].device)\n",
    "                    patch_id = patch_id[:int(min(num_patches, patch_id.shape[0]))]  # .to(patch_ids.device)\n",
    "                x_sample = feat_reshape[:, patch_id, :].flatten(0, 1)  # reshape(-1, x.shape[1])\n",
    "            else:\n",
    "                x_sample = feat_reshape\n",
    "                patch_id = []\n",
    "            if self.use_mlp:\n",
    "                mlp = getattr(self, 'mlp_%d' % feat_id)\n",
    "                x_sample = mlp(x_sample)\n",
    "            return_ids.append(patch_id)\n",
    "            x_sample = self.l2norm(x_sample)\n",
    "\n",
    "            if num_patches == 0:\n",
    "                x_sample = x_sample.permute(0, 2, 1).reshape([B, x_sample.shape[-1], H, W])\n",
    "            return_feats.append(x_sample)\n",
    "        return return_feats, return_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Class to create separate losses for each feature, from https://github.com/taesungp/contrastive-unpaired-translation\n",
    "class PatchNCELoss(nn.Module):\n",
    "    def __init__(self, batch_size, nce_T = 0.07):\n",
    "        super().__init__()\n",
    "        self.batch_size = batch_size\n",
    "        self.nce_T = nce_T\n",
    "        self.cross_entropy_loss = torch.nn.CrossEntropyLoss(reduction='none')\n",
    "        self.mask_dtype = torch.bool\n",
    "\n",
    "    def forward(self, feat_q, feat_k):\n",
    "        batchSize = feat_q.shape[0]\n",
    "        dim = feat_q.shape[1]\n",
    "        feat_k = feat_k.detach()\n",
    "\n",
    "        # pos logit\n",
    "        l_pos = torch.bmm(feat_q.view(batchSize, 1, -1), feat_k.view(batchSize, -1, 1))\n",
    "        l_pos = l_pos.view(batchSize, 1)\n",
    "\n",
    "        # neg logit\n",
    "\n",
    "        # Should the negatives from the other samples of a minibatch be utilized?\n",
    "        # In CUT and FastCUT, we found that it's best to only include negatives\n",
    "        # from the same image. Therefore, we set\n",
    "        # --nce_includes_all_negatives_from_minibatch as False\n",
    "        # However, for single-image translation, the minibatch consists of\n",
    "        # crops from the \"same\" high-resolution image.\n",
    "        # Therefore, we will include the negatives from the entire minibatch.\n",
    "\n",
    "        batch_dim_for_bmm = self.batch_size\n",
    "\n",
    "        # reshape features to batch size\n",
    "        feat_q = feat_q.view(batch_dim_for_bmm, -1, dim)\n",
    "        feat_k = feat_k.view(batch_dim_for_bmm, -1, dim)\n",
    "        npatches = feat_q.size(1)\n",
    "        l_neg_curbatch = torch.bmm(feat_q, feat_k.transpose(2, 1))\n",
    "\n",
    "        # diagonal entries are similarity between same features, and hence meaningless.\n",
    "        # just fill the diagonal with very small number, which is exp(-10) and almost zero\n",
    "        diagonal = torch.eye(npatches, device=feat_q.device, dtype=self.mask_dtype)[None, :, :]\n",
    "        l_neg_curbatch.masked_fill_(diagonal, -10.0)\n",
    "        l_neg = l_neg_curbatch.view(-1, npatches)\n",
    "\n",
    "        out = torch.cat((l_pos, l_neg), dim=1) / self.nce_T\n",
    "\n",
    "        loss = self.cross_entropy_loss(out, torch.zeros(out.size(0), dtype=torch.long,\n",
    "                                                        device=feat_q.device))\n",
    "\n",
    "        return loss\n",
    "    \n",
    "def nonsaturating_loss(prediction, is_real):\n",
    "    if is_real.mean() == 1:\n",
    "        loss = F.softplus(-prediction).view(prediction.size(0), -1).mean(dim = 1)\n",
    "    else:\n",
    "        loss = F.softplus(prediction).view(prediction.size(0), -1).mean(dim = 1)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the networks, load the most recent saved models, and set the loss functions and optimizers\n",
    "\n",
    "if faces:\n",
    "    input_size = (input_size[0], input_size[1], input_size[1])\n",
    "G = Generator(input_size, res_blocks).to(device)\n",
    "D = Discriminator(input_size).to(device)\n",
    "Sampler = PatchSampleF(batch_size).to(device)\n",
    "\n",
    "if nonsaturating:\n",
    "    criterion_GAN = nonsaturating_loss\n",
    "else:\n",
    "    criterion_GAN = torch.nn.MSELoss().to(device)\n",
    "criterion_NCE = []\n",
    "\n",
    "for nce_layer in nce_layers:\n",
    "    criterion_NCE.append(PatchNCELoss(batch_size).to(device))\n",
    "\n",
    "G.apply(weights_init_normal)\n",
    "D.apply(weights_init_normal)\n",
    "\n",
    "optimizer_G = torch.optim.Adam(G.parameters(), lr = 0.0002, betas = (0.5, 0.999))\n",
    "optimizer_D = torch.optim.Adam(D.parameters(), lr = 0.0002, betas = (0.5, 0.999))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters for the sampler are not made until the first forward pass through the Sampler network\n",
    "# Hence, we do a 'trial' training pass before setting the optimizer for the Sampler\n",
    "for i, batch in enumerate(dataloader):\n",
    "    x, y = Variable(batch[\"a\"].type(Tensor)), Variable(batch[\"b\"].type(Tensor))\n",
    "    \n",
    "    real = Variable(Tensor(np.ones((x.size(0), *D.output_shape))), requires_grad = False)\n",
    "    fake = Variable(Tensor(np.zeros((x.size(0), *D.output_shape))), requires_grad = False)\n",
    "    \n",
    "    # get the fake loss\n",
    "    fake_y = G(x)\n",
    "    D_fake = D(fake_y.detach())\n",
    "    loss_D_fake = criterion_GAN(D_fake, fake).mean()\n",
    "    # get the real loss\n",
    "    D_real = D(y)\n",
    "    loss_D_real = criterion_GAN(D_real, real).mean()\n",
    "    # combine loss and calculate gradients\n",
    "    loss_D = (loss_D_fake + loss_D_real) * 0.5\n",
    "    loss_D.backward()\n",
    "\n",
    "    # get the fake GAN loss\n",
    "    D_fake = D(fake_y)\n",
    "    loss_G_GAN = lambda_GAN * criterion_GAN(D_fake, real).mean()\n",
    "    total_nce_loss = 0\n",
    "    for fake, real in [(fake_y, x), (y, G(y))]:\n",
    "        # get the NCE loss\n",
    "        feat_q = G(fake, nce_layers, encode_only = True)\n",
    "        feat_k = G(real, nce_layers, encode_only = True)\n",
    "\n",
    "        feat_k_pool, sample_ids = Sampler(feat_k, 256, None)\n",
    "        feat_q_pool, _ = Sampler(feat_q, 256, sample_ids)\n",
    "\n",
    "        total_nce_loss = 0.0\n",
    "        for f_q, f_k, crit, nce_layer in zip(feat_q_pool, feat_k_pool, criterion_NCE, nce_layers):\n",
    "            loss = crit(f_q, f_k) * lambda_NCE\n",
    "            total_nce_loss += loss.mean()\n",
    "\n",
    "        nce_loss = total_nce_loss / len(nce_layers)\n",
    "        total_nce_loss += nce_loss\n",
    "\n",
    "    loss_G = loss_G_GAN + total_nce_loss\n",
    "    loss_G.backward()\n",
    "    \n",
    "    break\n",
    "\n",
    "optimizer_Sampler = torch.optim.Adam(Sampler.parameters(), lr = 0.0002, betas = (0.5, 0.999))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the most recently saved models if available\n",
    "if os.path.isdir('saved_models') and load_weights:\n",
    "    # Get the most recent model and load them\n",
    "    epoch = max([int(fname[8:-4]) for fname in os.listdir('saved_models') if 'Sampler' in fname])\n",
    "    G.load_state_dict(torch.load('saved_models/G_%d.pth' % (epoch)))\n",
    "    D.load_state_dict(torch.load('saved_models/D_%d.pth' % (epoch)))\n",
    "    Sampler.load_state_dict(torch.load('saved_models/Sampler_%d.pth' % (epoch)))\n",
    "    # Load the losses as well, for plotting\n",
    "    losses = np.load('saved_models/losses_%d.npy' % (epoch))\n",
    "    D_losses = list(losses[0])\n",
    "    GAN_losses = list(losses[1])\n",
    "    NCE_losses = list(losses[2])\n",
    "    G_total_losses = list(losses[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAEGCAYAAABo25JHAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/Il7ecAAAACXBIWXMAAAsTAAALEwEAmpwYAAAqKElEQVR4nO3deXxddZ3/8dfn3uxbmybpnpLSppRNtkxB7G+oSrUsgoKO4O44Iv7EcR/1Ny4zzM95qL+RGRcUER2XGWGQTUbBSgHBESikFpG20CVtaUtp0zR72iw3n98f33uTmzRJ0zY3N8l9Px+P7+Pce+6553xPenve53y/ZzF3R0REMlck3RUQEZH0UhCIiGQ4BYGISIZTEIiIZDgFgYhIhstKdwWOVXl5uVdVVaW7GiIik8q6desOuHvFUJ9NuiCoqqqitrY23dUQEZlUzGzncJ+paUhEJMMpCEREMpyCQEQkwykIREQynIJARCTDKQhERDKcgkBEJMNlTBBs2ACf+hQcOpTumoiITCwZEwQ7dsBNN8ETT6S7JiIiE0vGBMFf/iVkZcHDD6e7JiIiE0vGBEFxMSxbpiAQERksY4IA4PWvh9paaGpKd01ERCaOjAuC3l547LF010REZOLIqCC44ALIz1fzkIhIsowKgtxc+F//S0EgIpIso4IAQvPQxo2wd2+6ayIiMjFkZBAAPPJIeushIjJRZFwQnH02lJaqeUhEJCHjgiAahde+NgSBe7prIyKSfhkXBBCah156CbZtS3dNRETSL2ODANQ8JCICGRoES5bAvHkKAhERyNAgMAtHBY88Eq40FhHJZBkZBBCCoKEBnnsu3TUREUmvlAWBmf3IzPab2fNHme4vzKzHzN6aqroMRf0EIiJBKo8IfgysGmkCM4sCXwN+m8J6DGnePDjlFAWBiEjKgsDdHwcOHmWyjwJ3A/tTVY+RXHwxPP44dHWlY+kiIhND2voIzGwe8Bbge6OY9jozqzWz2vr6+jGrw+tfD+3t8PTTYzZLEZFJJ52dxf8GfNbdj3rejrvf6u417l5TUVExZhVYsQIiEVizZsxmKSIy6aQzCGqAO8xsB/BW4Ltm9ubxrEBpaXhGwW23QWPjeC5ZRGTiSFsQuPtCd69y9yrgLuB/u/t9412Pb34T9u2DG24Y7yWLiEwMqTx99HbgSeAUM9ttZh8ws+vN7PpULfN41NTAl74EP/853HlnumsjIjL+zCfZLThramq8trZ2TOfZ0wPLl8PmzfDnP4dTS0VEphIzW+fuNUN9lrFXFifLyoKf/hQOH4YPfEC3pxaRzKIgiFuyBL7xDVi9Gr531BNaRUSmDgVBkuuvh1Wr4NOfhhdfTHdtRETGh4IgiRn88IeQnw/vfne42ExEZKpTEAwydy58//vwzDOweDHccgt0d6e7ViIiqaMgGMJb3wpPPBGC4MMfhjPOgHvuUSeyiExNCoJhvPrV4YZ0v/xleOD91VfDhReGh9koEERkKlEQjMAMrrgiPLzmhz+EXbvCjepqasIFaGoyEpGpQEEwCllZ8Nd/DVu3wq23QkcHvPOdsGgR/Mu/QHNzumsoInL8FATHIC8PPvhB2LAB/vu/QxB85jNQWRk6ldVkJCKTkYLgOEQicPnl8OijUFsb7mD64Q+HcXv3prt2IiLHRkFwgs47L1yN/J3vhI7kM88MZxiJiEwWCoIxYAYf+QisXw9VVeEMo/e9T30HIjI5KAjG0NKl8OST8MUvws9+BqeeCj/4Qbi7qYjIRKUgGGPZ2XDjjeGCtIUL4brrwgVp996rzmQRmZgUBCly/vnwP/8D990XOpevuipckPb44+mumYjIQAqCFDKDK68MF6Tddlu4IO2ii8K4LVvSXTsRkUBBMA6yssIDb7ZsgX/+53B20WmnwSc+AY2N6a6diGQ6BcE4ys+Hz38+BML73gff/Ga4sd13vqPbVYhI+igI0mD27HA20fr1cNZZ8NGPQnV1uEp57Vro7U13DUUkkygI0uiss+Dhh8MdTk89NRwhXHABnHQSfOxjoWNZoSAiqZayIDCzH5nZfjN7fpjP32lmz5nZn83sCTM7K1V1mcgSdzh98EHYvx9++lM499zwcJyLLgoXqH3xi+GGdyIiqZDKI4IfA6tG+Hw7cJG7nwn8E3BrCusyKUyfHh6R+ctfQn19uNX16aeHDubqali+PJx91NKS7pqKyFSSsiBw98eBgyN8/oS7J86ZeQqYn6q6TEbFxXDtteFI4aWX4KtfhYaGcPfTOXPg/e8PVzHrIjUROVETpY/gA8CDw31oZteZWa2Z1dbX149jtSaGefPgs5+FjRvhqafgXe+Cu+4KF6ideWboWzg4bOSOzD0EzT33hPmsWaNTWkUyjXkKdynNrAr4lbufMcI0rwW+Cyx394ajzbOmpsZra2vHrpKTVFsb3HFHOPvo6achNzf0LSxdGjqely4NZf58aG0NG/emplAOHgyhsm5duI32UNm6cGG4s2pNTWiWmj0bZs0Kw8LC8V5bETlRZrbO3WuG/CydQWBmrwLuBS5x982jmaeC4EjPPQc//nE4HXXTJti37+jfiUZD/0NiY19TAwsWwPPPh4BIlLq6I79bWBiapxYuhJNPHlhmzQqfFxaG+y6JyMQwIYPAzBYAjwDvcfcnRjtPBcHRNTbCiy/CCy/Ayy/DtGlQWho6oxOlqgoKCkY3r507Q7i88kr/8OWXYfv2EBQHDgz93ZycEAglJeGU2EWLQlgkhpWVUFYWjmZEJLXSEgRmdjuwAigH9gFfBrIB3P0WM7sNuBrYGf9Kz3CVTKYgmHhaWkIobNsWQqG9PZS2tjBsaoIdO8LnQz3BrbgYystDmTkzXF+xbFm4cd/s2eO9NiJTU9qOCFJBQTC5dXT0h8bLL4fgaGgIwwMHwriNG/uf4VBZGQJh6dJwZFFUFEpxcSiLFoUmrcgIpz0klllaGpq0zMZnXUUmkpGCIGu8KyOZraAg9E2cfvrw0xw6FPo71q4NHeFr14azpIZTWBg6yE87Lcy3pKS/aeyFF0LTVmJ/Jz8/3N8pUSorQ6Ak+jUKC/vDJhE4hYWhTwXCfA4fDh3wra3hqCcvDyoqQpPbSIEkMlHpiEAmhd7esGff1hZKa2toctqyJRxBbNwIGzaEIwoIgZM4c2rp0rDRT0y/dWso27ZBV9folp+fH/o82togFht6mmg0NG9VVIS+j+LigaFSWBiCors7lJ6eMOztDfNPDqPCwrC8SGRgiUZhxoywjJkzw2uFj4yGjghk0otE+jeoyV772oHvm5pCSMybd/QNZCwWmqUSfRqD+zYSe/yJYWdn/1FCSUn/hv7QoXAKbn19uE1IfX2Y7+7d/fNLFPdwW/Ls7P4SiYR5tLcf+wWCkUgInbKycEQy+KSA5PeJ13PnqolMBlIQyJSS2OiNRjQa9qrHS2IjP9wGONHslAikrq4wrre3v3R3h5BJBE+iHDwYQrC+Phz1NDWFM76GO3opK4NXvSqUs84KTWru/deaNDVBc3MIqOTgysoKRyrTpvUHUHl5GBYUKFwmKwWByDg52kbSLDQR5eeHjeuJcu8/ayv5gsIdO+DPf4Y//SlckNjRceLLghAQyX0rieYwCEdTXV39w1gsnDacnx/6WBLDSKQ//BLDrKxwFFNZ2V/mzw/THz4cwiox7O4OgTS4HrFYCLbkkGttDTsDOTmhZGf3r0NFRfg3yJRTmxUEIlOUWf8Gcf4wd/KKxcK1IJs2hQ3h9Olhbz9xZJWX19+XkejX6OoKG9OGhv5y4EAIm+RmsEQxC/MpKQkb1pycsAE+fLh/A97SEq5R6e0NYWDWP+zqgj/8ISxnvJWU9Pf7FBWFkMnPD8OCgv7PB5eCgrCuiZKV1R/Mzc39JXEUl7hGJ1EaG8Pfuqcn/BslhtdfD3/3d2O/ngoCkQwWjYZbiFRXDz9NYo852dy5qa3XUDo6Qr/Lrl2hdHcPPJrIywthlnxSQaKPJxod2HcybVo4WujtDUHT1RXm19UVQmlw01vi+piGhhBcHR1h2Nw8uqcLJkJtuKY6COuRuJXLrFlhXaLRECKJYVXV2PwtB1MQiMikUFAAS5aEMlG4h6A5cKA/MA4cCCHR2TmwQAigwaWiImz4i4rS18eiIBAROU5moXmopCTcNmWy0hnIIiIZTkEgIpLhFAQiIhlOQSAikuEUBCIiGU5BICKS4RQEIiIZTkEgIpLhFAQiIhlOQSAikuEUBCIiGe6oQWBmbzOz4vjrL5jZPWZ2buqrJiIi42E0RwRfdPdWM1sOXAz8EPje0b5kZj8ys/1m9vwwn5uZfcvMtprZcwoXEZH0GE0QJO6gfRlwq7v/GsgZYfqEHwOrRvj8EqA6Xq5jFOEiIiJjbzRBsMfMvg+8HXjAzHJH8z13fxw4OMIkVwI/9eApYLqZzRlNpUVEZOyMJgj+ClgNvNHdm4AZwGfGYNnzgF1J73fHxx3BzK4zs1ozq62vrx+DRYuISMJogmAO8Gt332JmK4C3AU+nslKDufut7l7j7jUVFRXjuWgRkSlvNEFwNxAzs8XArUAl8PMxWPae+LwS5sfHiYjIOBpNEPS6ew9wFfBtd/8M4SjhRN0PvCd+9tAFQLO77x2D+YqIyDEYzTOLu83sWuA9wJvi47KP9iUzux1YAZSb2W7gy4nvufstwAPApcBWoAN4/7FWXkRETtxoguD9wPXAV9x9u5ktBH52tC+5+7VH+dyBj4yqliIikjKjOQ10I/Bp4M9mdgaw292/lvKaiYjIuDjqEUH8TKGfADsAAyrN7L3x6wRERGSSG03T0DeAN7j7iwBmtgS4HTgvlRUTEZHxMZqzhrITIQDg7psZRWexiIhMDqM5Iqg1s9uA/4i/fydQm7oqiYjIeBpNEHyYcHbP38bf/x64OWU1EhGRcXXUIHD3TuCmeAHAzP4AvCaF9RIRkXFyvE8oWzCmtRARkbQ53iDwMa2FiIikzbBNQ2Z21XAfAfmpqY6IiIy3kfoI3jTCZ78a64qIiEh6DBsE7q6bwImIZIDj7SMQEZEpQkEgIpLhFAQiIhluNFcWY2YXAlXJ07v7T1NUJxERGUejuQ31z4BFwLNALD7aAQWBiMgUMJojghrgtPgTxUREZIoZTR/B88DsVFdERETSYzRHBOXARjN7GuhMjHT3K1JWKxERGTejCYJ/ON6Zm9kq4JtAFLjN3b866PMFhMdgTo9P8zl3f+B4lyciIsduNLehfux4ZmxmUcJzC1YCu4FnzOx+d9+YNNkXgDvd/XtmdhrwAOHsJBERGSdH7SMwswvM7BkzazOzLjOLmVnLKOa9DNjq7nXu3gXcAVw5aBoHSuKvpwEvH0vlRUTkxI2maeg7wDXALwhnEL0HWDKK780DdiW93w2cP2iafwB+a2YfBQqBi0cxXxERGUOjurLY3bcCUXePufu/A6vGaPnXAj929/nApcDPzOyIOpnZdWZWa2a19fX1Y7RoERGB0QVBh5nlAM+a2dfN7BOj/N4eoDLp/fz4uGQfAO4EcPcngTzCWUoDuPut7l7j7jUVFRWjWLSIiIzWaDbo745PdwPQTti4Xz2K7z0DVJvZwniQXAPcP2ial4DXA5jZqYQg0C6/iMg4Gs1ZQzvNLB+Y4+7/ONoZu3uPmd0ArCacGvojd99gZjcCte5+P/Ap4AfxowwH3qcrmEVExtdo7jX0JuBfgBxgoZmdDdw4mgvK4tcEPDBo3JeSXm8EXnOMdRYRkTE0mqahfyCcCtoE4O7PAgtTViMRERlXowmCbndvHjROzTciIlPEaK4j2GBm7wCiZlYN/C3wRGqrJSIi42U0RwQfBU4n3HDudqAF+HgK6yQiIuNoNGcNdQB/Hy8iIjLFDBsEZjb4nP8BdBtqEZGpYaQjglcT7hV0O7AWsHGpkYiIjKuRgmA24RbS1wLvAH4N3O7uG8ajYiIiMj6G7SyO32DuN+7+XuACYCvwu/jVwiIiMkWM2FlsZrnAZYSjgirgW8C9qa+WiIiMl5E6i38KnEG4RcQ/uvvz41YrEREZNyMdEbyLcLfRjwF/a9bXV2yAu3vJcF8UEZHJY9ggcPdRPbRGREQmN23sRUQynIJARCTDKQhERDKcgkBEJMMpCEREMpyCQEQkwykIREQyXMYEQV1jHZ996LN0x7rTXRURkQklpUFgZqvM7EUz22pmnxtmmr8ys41mtsHMfp6qumzYv4GvP/F17tl0T6oWISIyKaUsCMwsCtwMXAKcBlxrZqcNmqYa+DzwGnc/nRQ+AvOyJZdRPaOabzz5Ddw9VYsREZl0UnlEsAzY6u517t4F3AFcOWiaDwI3u3sjgLvvT1VlIhbh4xd8nGdefoYndj2RqsWIiEw6qQyCeYQnnCXsjo9LtgRYYmZ/MLOnzGzVUDMys+vMrNbMauvr64+7Qu89672U5pXyr0/963HPQ0Rkqkl3Z3EWUA2sIDzz4AdmNn3wRO5+q7vXuHtNRUXFcS+sMKeQD533Ie594V7qGuuOez4iIlNJKoNgD1CZ9H5+fFyy3cD97t7t7tuBzYRgSJkblt1AxCJ8a+23UrkYEZFJI5VB8AxQbWYLzSwHuAa4f9A09xGOBjCzckJTUUp31eeVzOOaM67hh+t/SPPh5lQuSkRkUkhZELh7D3ADsBrYBNzp7hvM7EYzuyI+2Wqgwcw2Ao8Cn3H3hlTVKeETF3yCtq42bvvjbalelIjIhGeT7VTKmpoar62tPeH5rPjxCuoa66j7WB1ZkREf3SwiMumZ2Tp3rxnqs3R3FqfNJ1/9SXa17OLujXenuyoiImmVsUFw+ZLLWTxjMTc9dZMuMBORjJaxQRCxCB8//+M8vedpntz9ZLqrIyKSNhkbBADvO/t9lOaV8uXffZle7013dURE0iKjg6Awp5CvvO4rrKlbw01P3pTu6oiIpEVGBwHA9TXXc/WpV/P5hz/P2t1r010dEZFxl7GnjyZrOtzE2becjZmx/kPrmZ43fUznLyKZw90xs2P+Tmesk/audtq722k+3My+9n3sa9vHvvZ9vNL2Cvva93HJ4ku45oxrjqteI50+qhPogel507njrXew/EfLue6/r+O/3vpfx/wPKSLHr6O7g5ufvpnWrlaWlC3hlLJTqC6rTulO2fFssIeaR11jHWv3rGXt7rWs3bOWZ195ltL8UhaVLmLRjEVhWLqIktwSXmp+KZSWl9jZtJNdLbtoPtxMe3f7iP2UOdEcZhfN5syZZ55QfYejI4IkX/ufr/G5hz/HLZfdwodqPpSSZYjIQA9ueZCPPPARtjdtJ2KRARvEmYUzWVK2hCUzloRhvCyasYicaA5Nh5s4eOggDR0NHDx0kI7uDhZMWzBkiHT2dPLU7qdYU7eGNdvX8MyeZ8iJ5jA9bzrT8qaFYe40CnMKyY5kkx3NDsNINlmRLA71HKKtq4327va+PfcdTTs40HEAgILsAmrm1nDu7HNp6WxhW+M2tjVuY3fL7gH1yInmUFlSyUnTT6KypJLSvFIKcwopzC7sG5bkljCraBazCmcxq2gW03KnnXBojXREoCBI0uu9XPqfl/LYzsd4+m+e5sxZx5++h7oPEfMYRTlFY1hDkaljT8sePr7649y18S5OKTuF7132PS6svJC6xjo2N2zuKy82vMiWg1t4pe2Vvu8aYaPoDL/9Ki8op3pGNdVl1exv38/jOx+no7uDqEVZNm8Zyxcsx91p7mym6XBT37Cju4PuWDfdvd10xbrojnXT09tDfnY+hdmFFOUU9W2w5xTNYdm8ZVww/wJOn3n6kHcpONxzmO2N22ntamXBtAXMLJxJxMa/e1ZBcAz2t+/nrFvOYnredL59ybdZvmA5eVl5R/1eW1cbT+x6gsd2PBaCZM/TAKxctJKrT72aK0+5krKCspTVW2Sic3daOltoPNzI/S/ezxce+QJdsS6+8Jdf4DMXfobcrNwRv9/S2cKWhi19ARHzGDPyZ1CWXxaGBWXkZeWxo2kHWxq2sOVgvDRsoTi3mIsXXszFJ1/MiqoVTMubNk5rPXEoCI7RI9sf4U23v4mO7g7ys/K5qOoi3rjojbxx0RuZUzyHLQ1b2HpwK1sPbmXLwS1sOrCJ9XvXE/MYUYtSM7eGi066iJjHuHvT3exo2kHUorxu4et4y9K3MLtoNmbWt1djZuRl5VGWX0Z5QTnlBeUUZBeon0IG2Nu6lz/s+gNziuZwSvkplBeUp7tKAGxv3M4ttbfw+5d+T8xjuDu93ovjuDutXa00Hmqk6XATMY/1fe8Ni97AzZfezOIZi9NY+8yhIDgO7V3t/G7H71i9bTWrt61mc8PmIaebXzKf6hnVXDD/AlZUreDCygsHNAe5O3/c+0fu2ngXv9j4C7Y1bhvV8vOy8qgoqOCiqot4xxnv4OKTLyY7mj0m6yaTx7aD27j3hXu5Z9M9PLX7qQFNITPyZ/R1rM4tnktuNJfcrFxyo7nkZeWRFcmipbOFhkOh/bzhUAMNHQ1EI9HQZBJvNllStoSF0xfS3t3Oy60vDyi93surZr2Ks2efzbzieX07J7HeGL/Z+hu+W/tdHtzyIBGL8JoFryE/Kx8zI2IRDMPMKMopojSvlBn5MyjNK6U0v5STS0/mopMu0s7OOFIQjIEdTTt4aNtDNHc2s3jGYqpnVHNy6cnkZ+ePeh7uztaDW2nvbsfd+/5TuzuHeg7R0NHAgY4DNBwKw90tu3lw64M0HW6iLL+Mt532Nq4981qWL1ieljZGSb1e72Xdy+v41eZfcd+L9/HcvucAOGf2OVx16lWsPHklDYcaePHAi7zYEC8HXqS+o56e3p4h55kVyRrQhNLd283mhs00HW46prqVF5RzzuxzqJ5RzYNbH2R703ZmF83munOv44PnfZD5JfNPdPUlhRQEk1hnTyert63m9udv55cv/JJDPYeYWTiz7+yE8+aex3lzzmN+yXztXU1SrZ2trKlbw682/4pfb/k1+9r3EbEIF1ZeyFVLr+Itp76FqulVR51PrDdGV6yLwz2H6Yx10h3rZlreNIpzio/4bbg7DYca+trStzdupyS3hDnFc5hbPJe5xXOZUzSHXu/lT/v+xLOvPMv6vet5dt+zbKzfyPnzzucjf/ER3rz0zTpSnSQUBFNEW1cb9794P7/d9lvW7V3HxvqNfafaVRRUcMbMMzi1/FSWli/l1IownFc8D8fpinXR2dNJV6yLrlgXjhOxSF+JWpT87HwKsgvSvJZTX6w3xrq961hTt4aH6h7iiV1P0BXrYlruNC6pvoTLqy9n1eJVE/bkgrE4/17Gn4Jgiuro7uC5fc+x7uV1/HHvH9lQv4FNBzbR0tnSN83g87JHErEIy+YtY9WiVaxavIqauTVEI9FUVX9K6PVeYr0xsiJZR2wc27sGtrnvad3DU7uf4pHtj9B4uBGAs2efzcqTV3JZ9WVcWHmh9q4lZRQEGcTdeaXtFTYd2MQLB15gd8tucqI5fR2JOdEccqI5fQGRXPa37+ehuodYu3stjjMjfwYrT17J6RWnU5xbTHFOcd8wuRlhNKfXTgadPZ20dLbQ1tVGaX7psBfxbDu4re8kgke2P0JbVxsQ2uITFyL1em/f+GSVJZWsPHklKxet5HULX8fMwpkpXy8RUBDIMWroaGBN3Rp+s+03rN66mr1te0ecfkb+DOYVz2Nu8Vxyojm0drXS2tnaN0x0jgN9G1YjnFkSjUSJWpRoJEpWJIucaA7zS+b3XZafuEQ/PzufXc272NWyq2+4t20v84vnc8bMMzh95umcMfMMZhXOwsw4eOgg6/euZ/0roTy37zkOdR86ou49vT199ezu7R7wWXFOMQumLegrhrFm+xq2HtwKQNX0Kt5w8huYXzKfnt4eunu7+y5EMozZRbP72tvnFs9lTvGcMblCVOR4KAjkhHTHumnrahuwgW863MTe1r19TR6JYU9vz4Ajh+KcYgpzColYpC8MEmdLxXpjxDzWN+zp7aEz1snOpp1sa9zG/vb9Q9bHMOYWz2VW0Sxean6p7xJ/gLL8MgqyC9jVsqtvXGVJJWfNPotpuUdeRBSxCCW5JX11LsktoTC7kMbDjexs2slLLS/13R/mUPchVlStCNeULH4j1TOqtVGXSUM3nZMTkh3NpjQ/nP89nlo7W6lrrGNb4zYO9xymsqSSBdMWMLd47oC29P3t+3l+//M8v/95NuzfQFt3G2fNOotzZp/DOXPOGbMLr9RJKlNVSo8IzGwV8E0gCtzm7l8dZrqrgbuAv3D3EXf3dUQgInLsRjoiSNlVSWYWBW4GLgFOA641s9OGmK4Y+Bigp8KIiKRBKi9PXQZsdfc6d+8C7gCuHGK6fwK+BhxOYV1ERGQYqQyCecCupPe74+P6mNm5QKW7/3qkGZnZdWZWa2a19fX1Y19TEZEMlrYb1phZBLgJ+NTRpnX3W929xt1rKioqUl85EZEMksog2ANUJr2fHx+XUAycAfzOzHYAFwD3m9mQnRkiIpIaqQyCZ4BqM1toZjnANcD9iQ/dvdndy929yt2rgKeAK4521pCIiIytlAWBu/cANwCrgU3Ane6+wcxuNLMrUrVcERE5Nim9oMzdHwAeGDTuS8NMuyKVdRERkaHp6SYiIhlOQSAikuEUBCIiGU5BICKS4RQEIiIZTkEgIpLhFAQiIhlOQSAikuEUBCIiGU5BICKS4RQEIiIZTkEgIpLhFAQiIhlOQSAikuEUBCIiGU5BICKS4RQEIiIZTkEgIpLhFAQiIhkupc8snnLcw9AsvfUYSiwGPT2hju7Q29s/7OqC7u4wTBQzyMoKJRoNw95eOHAA6usHlrY26OwM3+vsDKW7u//vkRhCmFdODuTmhpKTE0okEpY5uCTX0z2sR0cHtLeH5ba1hdfuMH16f5k2DUpK4PBhaGkJpbk5DHt7oby8v5SVhdLTE+aVmHd7exhXUhLmlyglJWH9mpv759ncHOZbVQWLFoVSVRXWMVlPD7S2hnrl5kJ+PuTlTczfjEicgiBZezs8+CDcfTc8+igcOhT+YydKby8UF8PSpXDKKWG4dCksWQJz5kBpadgQDqW7O2xkDxwI7xMbyERJfC95oxqLwd69sHPnwPLKK2Fj09oaNmitraGuqRCNQlFR/0Y9sYHPzg4b94TEhq6nZ2BgJF4nAmpwiUT6QyLxuqAgLLOwMAzLysK8m5thzx5oagqvOzpCgCU25CUloUQisHkz/OEP4e8diw29XoWFYdjSMvQ0yRIb8+S/sxlUVoa/ReLfY7h/h7y8EAqlpTBzJsyaFcrg14lSWjrw7yuSQikNAjNbBXwTiAK3uftXB33+SeBvgB6gHvhrd9+ZyjoN4A7798NDD8E998BvfhP+I5eXw6WXhg1QYm85sefc0AAvvACPPQb/8R8D5xeJwIwZUFER5hGNwr59YRkNDSde39xcOOkkmDs3DIuKQjAVF4fX2dn9G9TEHnckcmToZGeH+SWHXGJDWF4e6p8o06dP3A1ST0/4G4+0t+0eQqOhIax3YWEoubn933MPoZJ8BJCTMzBccnL6fy/btsHWrWFYVxf+dol/h5KSMMzLCwF4+HD4TSXKwYPhN1FXB08+GY64ksM/IRoNv6XS0lCmTw/DkpKw3ol5Jwdud3f4rLs7lFgs1KOwMIRrYt0hhGlyaWkJQZV81DV9evjdNzcfOW1ZWfgNVlX1l5kzw85SLNZfenvD3y55+QUF/UdpMiGYD/UjHIsZm0WBzcBKYDfwDHCtu29Mmua1wFp37zCzDwMr3P3tI823pqbGa2trj71Cu3fD734HW7YMLM3N4fO5c+Gqq+Dqq2H58vAf4Gja2sKe5+bNYQORaFZJDGOxgXt9M2eGjSsc2VTT09O/YUoezpwZ/pOddFJ4rSaGqSUW6/+97N/fX/btC+HV2Bg2vo2NobS0hEDLzQ0b+eQmuOzsULKywjAaDWHR3t7f3JZoZkuES6IUF4dpBwdEd/eR4VBcHOq8cyfs2AEvvzx0mB3N/PlQUwPnndc/zM8P633wYP/w4MH+ZsLEUXBbW9hBSRw1JkImLy8EbvJ0iWkTYZq8Hl1dR4a1e6hHQUEYJspQR8BZWf3LTg5cs/4gTBSz/p2xxL9ZVlZY3uDtgXtYr6KiMdsRM7N17l4z1GepPCJYBmx197p4Je4ArgT6gsDdH02a/ingXSmrzZNPwrvfHf4xTjoJqqvhne8Mw/PPD+VY/+BFRXDuuaGIHI9otL9paLLq7IRdu0KYRaOhRCL9r7u6BoZRR0do3vzjH6G2Fu67b/TLKigIG/DCwrCxTIRbe/vA6SKR/g1pUVHYECcC9WjNgOPJ7OghWljYf8R5/fXwyU+OeTVSGQTzgF1J73cD548w/QeAB4f6wMyuA64DWLBgwfHVZuVK2LgRTj75yA4+ETl+ubmweHEox6O5Gdavh3Xrwka6rCw0iyWGiSaxgoLh++B6e/v36BNHBkMdPSfCo6kpHDXk5PR36CeGEMKto+PII4XEPBK6ugYebSWCDvrDMNH31dsb9vyT+866uvpPsEgu0H8ElFxStMMwITqLzexdQA1w0VCfu/utwK0QmoaOayGJw0ERmVimTYMVK0I5XolmokQfyHDM+o8SRpKX1x8KGSCVQbAHqEx6Pz8+bgAzuxj4e+Aid+9MYX1ERGQIqTwd5Bmg2swWmlkOcA1wf/IEZnYO8H3gCnffn8K6iIjIMFIWBO7eA9wArAY2AXe6+wYzu9HMrohP9v+AIuAXZvasmd0/zOxERCRFUtpH4O4PAA8MGvelpNcXp3L5IiJydBP0SiERERkvCgIRkQynIBARyXAKAhGRDJeyew2lipnVA8d7Y7py4MAYVmeimcrrp3WbvKby+k2mdTvJ3SuG+mDSBcGJMLPa4W66NBVM5fXTuk1eU3n9psq6qWlIRCTDKQhERDJcpgXBremuQIpN5fXTuk1eU3n9psS6ZVQfgYiIHCnTjghERGQQBYGISIbLmCAws1Vm9qKZbTWzz6W7PifKzH5kZvvN7PmkcTPM7CEz2xIflqazjsfLzCrN7FEz22hmG8zsY/Hxk379zCzPzJ42sz/F1+0f4+MXmtna+O/zv+K3bp+UzCxqZuvN7Ffx91Np3XaY2Z/jd0uujY+b9L/LjAgCM4sCNwOXAKcB15rZaemt1Qn7MbBq0LjPAQ+7ezXwcPz9ZNQDfMrdTwMuAD4S//eaCuvXCbzO3c8CzgZWmdkFwNeAf3X3xUAj4dGtk9XHCLeeT5hK6wbwWnc/O+n6gUn/u8yIIACWAVvdvc7du4A7gCvTXKcT4u6PAwcHjb4S+En89U+AN49nncaKu+919z/GX7cSNirzmALr50Fb/G12vDjwOuCu+PhJuW4AZjYfuAy4Lf7emCLrNoJJ/7vMlCCYB+xKer87Pm6qmeXue+OvXwFS86TrcWRmVcA5wFqmyPrFm06eBfYDDwHbgKb4w5xgcv8+/w34O6A3/r6MqbNuEEL7t2a2zsyui4+b9L/LCfHwehl77u5mNqnPDTazIuBu4OPu3hJ2LoPJvH7uHgPONrPpwL3A0vTWaGyY2eXAfndfZ2Yr0lydVFnu7nvMbCbwkJm9kPzhZP1dZsoRwR6gMun9/Pi4qWafmc0BiA8n7XOgzSybEAL/6e73xEdPmfUDcPcm4FHg1cB0M0vsmE3W3+drgCvMbAeh+fV1wDeZGusGgLvviQ/3E0J8GVPgd5kpQfAMUB0/eyEHuAaYis9Hvh94b/z1e4FfprEuxy3ervxDYJO735T00aRfPzOriB8JYGb5wEpCH8ijwFvjk03KdXP3z7v7fHevIvwfe8Td38kUWDcAMys0s+LEa+ANwPNMhd9lplxZbGaXEtovo8CP3P0r6a3RiTGz24EVhNvg7gO+DNwH3AksINyq+6/cfXCH8oRnZsuB3wN/pr+t+f8Q+gkm9fqZ2asIHYpRwo7Yne5+o5mdTNiLngGsB97l7p3pq+mJiTcNfdrdL58q6xZfj3vjb7OAn7v7V8ysjMn+u8yUIBARkaFlStOQiIgMQ0EgIpLhFAQiIhlOQSAikuEUBCIiGU5BIDKImcXid5dMlDG7iZiZVSXfMVZkItAtJkSOdMjdz053JUTGi44IREYpfi/6r8fvR/+0mS2Oj68ys0fM7Dkze9jMFsTHzzKze+PPHviTmV0Yn1XUzH4Qfx7Bb+NXGIukjYJA5Ej5g5qG3p70WbO7nwl8h3ClOsC3gZ+4+6uA/wS+FR//LeCx+LMHzgU2xMdXAze7++lAE3B1StdG5Ch0ZbHIIGbW5u5FQ4zfQXioTF38pnivuHuZmR0A5rh7d3z8XncvN7N6YH7y7RTit9V+KP4QE8zss0C2u//fcVg1kSHpiEDk2Pgwr49F8n12YqivTtJMQSBybN6eNHwy/voJwt02Ad5JuGEehMcWfhj6HkYzbbwqKXIstCcicqT8+BPEEn7j7olTSEvN7DnCXv218XEfBf7dzD4D1APvj4//GHCrmX2AsOf/YWAvIhOM+ghERineR1Dj7gfSXReRsaSmIRGRDKcjAhGRDKcjAhGRDKcgEBHJcAoCEZEMpyAQEclwCgIRkQz3/wHKNnffy/iX1wAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 56/200] [Batch 817/4500] [D loss: 0.193807] [GAN loss: 0.539709, NCE loss: 1.167506, Total: 4.042226] ETA: 1 day, 15:35:09.5822944"
     ]
    }
   ],
   "source": [
    "# Training Loop, adapted from https://github.com/taesungp/contrastive-unpaired-translation\n",
    "\n",
    "prev_time = time.time()\n",
    "for epoch in range(epoch, epochs):\n",
    "    total_D_loss = 0\n",
    "    total_GAN_loss = 0\n",
    "    total_NCE_loss = 0\n",
    "    total_G_loss = 0\n",
    "    for i, batch in enumerate(dataloader):\n",
    "        x, y = Variable(batch[\"a\"].type(Tensor)), Variable(batch[\"b\"].type(Tensor))\n",
    "        \n",
    "        # Adversarial ground truths\n",
    "        real = Variable(Tensor(np.ones((x.size(0), *D.output_shape))), requires_grad = False)\n",
    "        fake = Variable(Tensor(np.zeros((x.size(0), *D.output_shape))), requires_grad = False)\n",
    "        \n",
    "        # train discriminator\n",
    "        D.train()\n",
    "        optimizer_D.zero_grad()\n",
    "        # get the fake loss\n",
    "        fake_y = G(x)\n",
    "        D_fake = D(fake_y.detach())\n",
    "        loss_D_fake = criterion_GAN(D_fake, fake).mean()\n",
    "        # get the real loss\n",
    "        D_real = D(y)\n",
    "        loss_D_real = criterion_GAN(D_real, real).mean()\n",
    "        # combine loss and calculate gradients\n",
    "        loss_D = (loss_D_fake + loss_D_real) * 0.5\n",
    "        loss_D.backward()\n",
    "        optimizer_D.step()\n",
    "        \n",
    "        # train generator\n",
    "        G.train()\n",
    "        optimizer_G.zero_grad()\n",
    "        optimizer_Sampler.zero_grad()\n",
    "        # get the fake GAN loss\n",
    "        D_fake = D(fake_y)\n",
    "        loss_G_GAN = criterion_GAN(D_fake, real).mean()\n",
    "        # get the NCE loss\n",
    "        total_nce_loss = 0\n",
    "        for fake, real in [(fake_y, x), (y, G(y))]:\n",
    "            feat_q = G(fake_y, nce_layers, encode_only = True)\n",
    "            feat_k = G(x, nce_layers, encode_only = True)\n",
    "\n",
    "            feat_k_pool, sample_ids = Sampler(feat_k, 256, None)\n",
    "            feat_q_pool, _ = Sampler(feat_q, 256, sample_ids)\n",
    "\n",
    "            total_nce_loss = 0.0\n",
    "            for f_q, f_k, crit, nce_layer in zip(feat_q_pool, feat_k_pool, criterion_NCE, nce_layers):\n",
    "                loss = crit(f_q, f_k) * lambda_NCE\n",
    "                total_nce_loss += loss.mean()\n",
    "\n",
    "            nce_loss = total_nce_loss / len(nce_layers)\n",
    "            total_nce_loss += nce_loss\n",
    "        total_nce_loss *= 0.5\n",
    "        \n",
    "        loss_G = loss_G_GAN + total_nce_loss\n",
    "        loss_G.backward()\n",
    "        optimizer_G.step()\n",
    "        optimizer_Sampler.step()\n",
    "        \n",
    "        # --------------\n",
    "        #  Log Progress\n",
    "        # --------------\n",
    "\n",
    "        # Determine approximate time left\n",
    "        batches_done = epoch * len(dataloader) + i\n",
    "        batches_left = 200 * len(dataloader) - batches_done\n",
    "        time_left = datetime.timedelta(seconds=batches_left * (time.time() - prev_time))\n",
    "        prev_time = time.time()\n",
    "\n",
    "        # Print log\n",
    "        sys.stdout.write(\n",
    "            \"\\r[Epoch %d/%d] [Batch %d/%d] [D loss: %f] [GAN loss: %f, NCE loss: %f, Total: %f] ETA: %s\"\n",
    "            % (\n",
    "                epoch,\n",
    "                200,\n",
    "                i,\n",
    "                len(dataloader),\n",
    "                loss_D.item(),\n",
    "                loss_G_GAN.item(),\n",
    "                nce_loss.item(),\n",
    "                loss_G.item(),\n",
    "                time_left,\n",
    "            )\n",
    "        )\n",
    "        \n",
    "        total_D_loss += loss_D.item()\n",
    "        total_GAN_loss += loss_G_GAN.item()\n",
    "        total_NCE_loss += nce_loss.item()\n",
    "        total_G_loss += loss_G.item()\n",
    "        \n",
    "        # If at sample interval save image\n",
    "        if batches_done % 300 == 0:\n",
    "            sample_images(batches_done)\n",
    "            \n",
    "    D_losses.append(total_D_loss / len(dataloader))\n",
    "    GAN_losses.append(total_GAN_loss / len(dataloader))\n",
    "    NCE_losses.append(total_NCE_loss / len(dataloader))\n",
    "    G_total_losses.append(total_G_loss / len(dataloader))\n",
    "    display.clear_output(wait = True)\n",
    "    time.sleep(1)\n",
    "    plt.clf()\n",
    "    plt.ylabel('Mean Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.plot(D_losses, color = 'r')\n",
    "    plt.plot(GAN_losses, color = 'g')\n",
    "    plt.plot(NCE_losses, color = 'b')\n",
    "    display.display(plt.gcf())\n",
    "    \n",
    "    if epoch % 5 == 0:\n",
    "        if not os.path.isdir('saved_models'):\n",
    "            os.mkdir('saved_models')\n",
    "        torch.save(G.state_dict(), \"saved_models/G_%d.pth\" % (epoch))\n",
    "        torch.save(D.state_dict(), \"saved_models/D_%d.pth\" % (epoch))\n",
    "        torch.save(Sampler.state_dict(), \"saved_models/Sampler_%d.pth\" % (epoch))\n",
    "        np.save('saved_models/losses_%d.npy' % (epoch), np.array([D_losses, GAN_losses, NCE_losses, G_total_losses]))\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
