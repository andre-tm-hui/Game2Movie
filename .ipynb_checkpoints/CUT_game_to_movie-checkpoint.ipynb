{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: numpy in c:\\users\\andre\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (1.19.3)\n",
      "Requirement already satisfied: opencv-python in c:\\users\\andre\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (4.4.0.46)\n",
      "Requirement already satisfied: dlib in c:\\users\\andre\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (19.21.1)\n",
      "Requirement already satisfied: matplotlib in c:\\users\\andre\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (3.3.3)\n",
      "Requirement already satisfied: boto3 in c:\\users\\andre\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (1.17.4)\n",
      "Requirement already satisfied: pillow>=6.2.0 in c:\\users\\andre\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from matplotlib) (8.0.1)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in c:\\users\\andre\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from matplotlib) (1.3.1)\n",
      "Requirement already satisfied: python-dateutil>=2.1 in c:\\users\\andre\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from matplotlib) (2.8.1)\n",
      "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.3 in c:\\users\\andre\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from matplotlib) (2.4.7)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\andre\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from matplotlib) (0.10.0)\n",
      "Requirement already satisfied: botocore<1.21.0,>=1.20.4 in c:\\users\\andre\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from boto3) (1.20.4)\n",
      "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in c:\\users\\andre\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from boto3) (0.10.0)\n",
      "Requirement already satisfied: s3transfer<0.4.0,>=0.3.0 in c:\\users\\andre\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from boto3) (0.3.4)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\andre\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from python-dateutil>=2.1->matplotlib) (1.15.0)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.25.4 in c:\\users\\andre\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from botocore<1.21.0,>=1.20.4->boto3) (1.26.2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 20.2.1; however, version 21.0.1 is available.\n",
      "You should consider upgrading via the 'c:\\users\\andre\\appdata\\local\\programs\\python\\python38\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    }
   ],
   "source": [
    "!pip install numpy opencv-python dlib matplotlib boto3\n",
    "try:\n",
    "    import torch\n",
    "except:\n",
    "    !pip install pip install torch==1.7.1+cu101 torchvision==0.8.2+cu101 torchaudio===0.7.2 -f https://download.pytorch.org/whl/torch_stable.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import numpy as np\n",
    "import cv2\n",
    "import os\n",
    "import glob\n",
    "import random\n",
    "import time, datetime, sys\n",
    "import zipfile as zf\n",
    "import boto3\n",
    "from PIL import Image\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch.autograd import Variable\n",
    "from torchvision import transforms, datasets\n",
    "from torchvision.utils import save_image, make_grid\n",
    "\n",
    "import dlib\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython import display\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the model name - determines where the outputs are saved\n",
    "model_name = 'G2M-frames'\n",
    "\n",
    "# Variables/Hyperparameters\n",
    "dataset_size = 600\n",
    "generate_dataset = True\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "Tensor = torch.cuda.FloatTensor if torch.cuda.is_available() else torch.Tensor\n",
    "nce_layers = [0, 4, 8, 12, 16]\n",
    "lambda_NCE = 1.0\n",
    "lambda_GAN = 1.0\n",
    "batch_size = 1\n",
    "input_size = (3,216,384)\n",
    "res_blocks = 9\n",
    "load_weights = True\n",
    "epoch = 0\n",
    "epochs = 200\n",
    "nonsaturating = False\n",
    "\n",
    "learning_rate = 0.001\n",
    "betas = (0.9, 0.999)\n",
    "\n",
    "D_losses = []\n",
    "GAN_losses = []\n",
    "NCE_losses = []\n",
    "G_total_losses = []\n",
    "\n",
    "# Swap the game and movie datasets, so that translation goes from movie to game instead\n",
    "swap = False\n",
    "\n",
    "# Toggle whether translation is done only on faces\n",
    "faces = False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the original data (and the processed data)\n",
    "s3 = boto3.resource('s3', aws_access_key_id = 'AKIAIOPFTDXA3ZXLK5YA', aws_secret_access_key = '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')\n",
    "bucket = s3.Bucket('vision-dataset-vmrj42')\n",
    "\n",
    "for fname in ['Data.zip', 'datasets.zip']:\n",
    "    if not os.path.isfile(fname):\n",
    "        bucket.download_file(fname, fname)\n",
    "\n",
    "if not os.path.isdir('Data'):\n",
    "    files = zf.ZipFile('Data.zip', 'r')\n",
    "    files.extractall('')\n",
    "if not os.path.isdir('dataset') and not generate_dataset:\n",
    "    files = zf.ZipFile('datasets.zip', 'r')\n",
    "    files.extractall('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a basic dataset for full-frame translation\n",
    "\n",
    "for dname in ['dataset/train/game', \n",
    "              'dataset/train/movie', \n",
    "              'dataset/test/game', \n",
    "              'dataset/test/movie', \n",
    "              'face_dataset/train/game',\n",
    "              'face_dataset/train/movie',\n",
    "              'face_dataset/test/game',\n",
    "              'face_dataset/test/movie']:\n",
    "    if not os.path.isdir(dname):\n",
    "        os.makedirs(dname)\n",
    "\n",
    "if len(os.listdir('dataset/train/game')) < dataset_size:\n",
    "    # get some frames from the game footage\n",
    "    cap = cv2.VideoCapture('Data/game/MafiaVideogame.mp4')\n",
    "    length = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "    frame_count = 0\n",
    "    saved_frames = 0\n",
    "    \n",
    "    face_detector = dlib.get_frontal_face_detector()\n",
    "    faces = []\n",
    "\n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        frame_count += 1\n",
    "        # Save the entire frame as part of the dataset, alternating between the training and testing datasets\n",
    "        if frame_count % (length // (2 * dataset_size)) == 0 and ret:\n",
    "            if saved_frames < dataset_size:\n",
    "                fname = 'dataset/train/game/%d.png' % (saved_frames)\n",
    "            else:\n",
    "                fname = 'dataset/test/game/%d.png' % (saved_frames % dataset_size)\n",
    "            cv2.imwrite(fname, frame)\n",
    "            saved_frames += 1\n",
    "        \n",
    "        # Check if there is a face in every (length // (10 * dataset_size)) frame\n",
    "        if frame_count % (length // (6 * dataset_size)) == 0 and ret:\n",
    "            dets = face_detector(frame, 1)\n",
    "            for i, d in enumerate(dets):\n",
    "                left, top, right, bottom = d.left(), d.top(), d.right(), d.bottom()\n",
    "                if right - left > 60:\n",
    "                    face = frame[top:bottom, left:right]\n",
    "                    if len(face) > 0 and len(face[0]) > 0:\n",
    "                        faces.append(face)\n",
    "    cap.release()\n",
    "    \n",
    "    # Alternating between the training and testing datasets, save the extracted faces\n",
    "    saved_faces = 0\n",
    "    for i, face in enumerate(faces):\n",
    "        if i % (len(faces) // (2 * dataset_size)) == 0:\n",
    "            if saved_faces < dataset_size:\n",
    "                fname = 'face_dataset/train/game/%d.png' % (saved_faces)\n",
    "            else:\n",
    "                fname = 'face_dataset/test/game/%d.png' % (saved_faces % dataset_size)\n",
    "            cv2.imwrite(fname, cv2.resize(face, (input_size[1], input_size[1])))\n",
    "            saved_faces += 1\n",
    "\n",
    "    # get some frames from the movie footage\n",
    "    movie_dirs = ['Data/movie/TheGodfather.mp4', 'Data/movie/TheIrishman.mp4', 'Data/movie/TheSopranos.mp4']\n",
    "\n",
    "    saved_frames = 0\n",
    "    for movie in movie_dirs:\n",
    "        cap = cv2.VideoCapture(movie)\n",
    "        length = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "        frame_count = 0\n",
    "\n",
    "        while cap.isOpened():\n",
    "            ret, frame = cap.read()\n",
    "            if not ret:\n",
    "                break\n",
    "            frame_count += 1\n",
    "            if frame_count % (length // (2 * dataset_size / len(movie_dirs))) == 0 and ret:\n",
    "                if saved_frames < dataset_size:\n",
    "                    fname = 'dataset/train/movie/%d.png' % (saved_frames)\n",
    "                else:\n",
    "                    fname = 'dataset/test/movie/%d.png' % (saved_frames % dataset_size)\n",
    "                cv2.imwrite(fname, frame)\n",
    "                saved_frames += 1\n",
    "\n",
    "\n",
    "        cap.release()\n",
    "        \n",
    "    face_detector = dlib.get_frontal_face_detector()        \n",
    "    faces_dir = 'Data/faces/'\n",
    "    real_faces = os.listdir(faces_dir)\n",
    "    saved_faces = 0\n",
    "    current_face = 0\n",
    "    while saved_faces < dataset_size * 4:\n",
    "        real_face = cv2.imread(faces_dir + real_faces[current_face])\n",
    "        face = []\n",
    "        dets = face_detector(real_face, 1)\n",
    "        for d in dets:\n",
    "            left, top, right, bottom = d.left(), d.top(), d.right(), d.bottom()\n",
    "            if right - left > 60:\n",
    "                face = real_face[top : bottom, left : right]\n",
    "                if len(face) > 0 and len(face[0]) > 0:\n",
    "                    face = cv2.resize(face, (input_size[1], input_size[1]))\n",
    "                    if saved_faces < dataset_size * 2:\n",
    "                        fname = 'face_dataset/train/movie/%d.png' % (saved_faces)\n",
    "                    else:\n",
    "                        fname = 'face_dataset/test/movie/%d.png' % (saved_faces % (dataset_size * 2))\n",
    "                    cv2.imwrite(fname, face)\n",
    "                    saved_faces += 1\n",
    "                    break\n",
    "        current_face += 1\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helpful class for loading both game and movie samples as one dataset\n",
    "class ImageDataset(Dataset):\n",
    "    def __init__(self, root, swap = False, transforms_ = None, unaligned = False, mode = \"train\"):\n",
    "        self.transform = transforms.Compose(transforms_)\n",
    "        self.unaligned = unaligned\n",
    "        if swap:\n",
    "            self.files_game = sorted(glob.glob(os.path.join(root, \"%s/movie\" % mode) + \"/*.*\"))\n",
    "            self.files_movie = sorted(glob.glob(os.path.join(root, \"%s/game\" % mode) + \"/*.*\"))\n",
    "        else:\n",
    "            self.files_game = sorted(glob.glob(os.path.join(root, \"%s/game\" % mode) + \"/*.*\"))\n",
    "            self.files_movie = sorted(glob.glob(os.path.join(root, \"%s/movie\" % mode) + \"/*.*\"))\n",
    "        print(len(os.listdir(os.path.join(root, 'train/movie'))))\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        image_game = Image.open(self.files_game[index % len(self.files_game)])\n",
    "\n",
    "        if self.unaligned:\n",
    "            image_movie = Image.open(self.files_movie[random.randint(0, len(self.files_movie) - 1)])\n",
    "        else:\n",
    "            image_movie = Image.open(self.files_movie[index % len(self.files_movie)])\n",
    "\n",
    "        # Convert grayscale images to rgb\n",
    "        if image_game.mode != \"RGB\":\n",
    "            image_game = to_rgb(image_game)\n",
    "        if image_movie.mode != \"RGB\":\n",
    "            image_movie = to_rgb(image_movie)\n",
    "\n",
    "        item_game = self.transform(image_game)\n",
    "        item_movie = self.transform(image_movie)\n",
    "        \n",
    "        return {\"a\": item_game, \"b\": item_movie}\n",
    "\n",
    "    def __len__(self):\n",
    "        return max(len(self.files_game), len(self.files_movie))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "600\n",
      "600\n"
     ]
    }
   ],
   "source": [
    "# Loading the dataset\n",
    "# Define the dataset-wide transformations\n",
    "\n",
    "transforms_ = [\n",
    "    transforms.Resize(int(input_size[1] * 1.4), Image.BICUBIC),\n",
    "    transforms.RandomCrop((input_size[1],input_size[2])),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
    "]\n",
    "\n",
    "\n",
    "if faces:\n",
    "    dataset_dir = 'face_dataset'\n",
    "else:\n",
    "    dataset_dir = 'dataset'\n",
    "# Training data loader\n",
    "dataloader = DataLoader(\n",
    "    ImageDataset(dataset_dir, swap = swap, transforms_ = transforms_, unaligned = True),\n",
    "    batch_size = batch_size,\n",
    "    shuffle = True,\n",
    "    num_workers = 0,\n",
    ")\n",
    "# Test data loader\n",
    "val_dataloader = DataLoader(\n",
    "    ImageDataset(dataset_dir, swap = swap, transforms_ = transforms_, unaligned = True, mode = 'test'),\n",
    "    batch_size = 5,\n",
    "    shuffle = True,\n",
    "    num_workers = 0,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Other Utils\n",
    "\n",
    "def weights_init_normal(m):\n",
    "    classname = m.__class__.__name__\n",
    "    if classname.find(\"Conv\") != -1:\n",
    "        torch.nn.init.normal_(m.weight.data, 0.0, 0.02)\n",
    "        if hasattr(m, \"bias\") and m.bias is not None:\n",
    "            torch.nn.init.constant_(m.bias.data, 0.0)\n",
    "    elif classname.find(\"BatchNorm2d\") != -1:\n",
    "        torch.nn.init.normal_(m.weight.data, 1.0, 0.02)\n",
    "        torch.nn.init.constant_(m.bias.data, 0.0)\n",
    "        \n",
    "def sample_images(batches_done):\n",
    "    \"\"\"Saves a generated sample from the test set\"\"\"\n",
    "    imgs = next(iter(val_dataloader))\n",
    "    G.eval()\n",
    "    real_A = Variable(imgs[\"a\"].type(Tensor))\n",
    "    fake_B = G(real_A)\n",
    "    # Arange images along x-axis\n",
    "    real_A = make_grid(real_A, nrow=5, normalize=True)\n",
    "    fake_B = make_grid(fake_B, nrow=5, normalize=True)\n",
    "    # Arange images along y-axis\n",
    "    image_grid = torch.cat((real_A, fake_B), 1)\n",
    "    if not os.path.isdir('models/%s/samples' % model_name):\n",
    "        os.makedirs('models/%s/samples' % model_name)\n",
    "    save_image(image_grid, \"models/%s/samples/%s.png\" % (model_name, batches_done), normalize=False)\n",
    "    \n",
    "class Normalize(nn.Module):\n",
    "    def __init__(self, power=2):\n",
    "        super(Normalize, self).__init__()\n",
    "        self.power = power\n",
    "\n",
    "    def forward(self, x):\n",
    "        norm = x.pow(self.power).sum(1, keepdim=True).pow(1. / self.power)\n",
    "        out = x.div(norm + 1e-7)\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generator and Discriminator models adapted from https://github.com/eriklindernoren/PyTorch-GAN\n",
    "# PatchSampleF adapted from https://github.com/taesungp/contrastive-unpaired-translation\n",
    "\n",
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, in_features):\n",
    "        super(ResidualBlock, self).__init__()\n",
    "\n",
    "        self.block = nn.Sequential(\n",
    "            nn.ReflectionPad2d(1),\n",
    "            nn.Conv2d(in_features, in_features, 3),\n",
    "            nn.InstanceNorm2d(in_features),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.ReflectionPad2d(1),\n",
    "            nn.Conv2d(in_features, in_features, 3),\n",
    "            nn.InstanceNorm2d(in_features),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x + self.block(x)\n",
    "\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self, input_shape, num_residual_blocks):\n",
    "        super(Generator, self).__init__()\n",
    "        channels = input_shape[0]\n",
    "\n",
    "        # Initial convolution block\n",
    "        out_features = 64\n",
    "        model = [\n",
    "            nn.ReflectionPad2d(channels),\n",
    "            nn.Conv2d(channels, out_features, 7),\n",
    "            nn.InstanceNorm2d(out_features),\n",
    "            nn.ReLU(inplace=True),\n",
    "        ]\n",
    "        in_features = out_features\n",
    "\n",
    "        # Downsampling\n",
    "        for _ in range(2):\n",
    "            out_features *= 2\n",
    "            model += [\n",
    "                nn.Conv2d(in_features, out_features, 3, stride=2, padding=1),\n",
    "                nn.InstanceNorm2d(out_features),\n",
    "                nn.ReLU(inplace=True),\n",
    "            ]\n",
    "            in_features = out_features\n",
    "\n",
    "        # Residual blocks\n",
    "        for _ in range(num_residual_blocks):\n",
    "            model += [ResidualBlock(out_features)]\n",
    "\n",
    "        # Upsampling\n",
    "        for _ in range(2):\n",
    "            out_features //= 2\n",
    "            model += [\n",
    "                nn.Upsample(scale_factor=2),\n",
    "                nn.Conv2d(in_features, out_features, 3, stride=1, padding=1),\n",
    "                nn.InstanceNorm2d(out_features),\n",
    "                nn.ReLU(inplace=True),\n",
    "            ]\n",
    "            in_features = out_features\n",
    "\n",
    "        # Output layer\n",
    "        model += [\n",
    "            nn.ReflectionPad2d(channels), \n",
    "            nn.Conv2d(out_features, channels, 7), \n",
    "            nn.Tanh()\n",
    "        ]\n",
    "\n",
    "        self.model = nn.Sequential(*model)\n",
    "\n",
    "    def forward(self, x, layers = [], encode_only = False):\n",
    "        if -1 in layers:\n",
    "            layers.append(len(self.model))\n",
    "        if len(layers) > 0:\n",
    "            feat = x\n",
    "            feats = []\n",
    "            for layer_id, layer in enumerate(self.model):\n",
    "                feat = layer(feat)\n",
    "                if layer_id in layers:\n",
    "                    feats.append(feat)\n",
    "                else:\n",
    "                    pass\n",
    "                if layer_id == layers[-1] and encode_only:\n",
    "                    return feats\n",
    "            return feat, feats\n",
    "        else:\n",
    "            return self.model(x)\n",
    "    \n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, input_shape):\n",
    "        super(Discriminator, self).__init__()\n",
    "        channels, height, width = input_shape\n",
    "        self.output_shape = (1, height // 2 ** 4, width // 2 ** 4)\n",
    "\n",
    "        def discriminator_block(in_filters, out_filters, normalize = True):\n",
    "            \"\"\"Returns downsampling layers of each discriminator block\"\"\"\n",
    "            layers = [nn.Conv2d(in_filters, out_filters, 4, stride=2, padding=1)]\n",
    "            if normalize:\n",
    "                layers.append(nn.InstanceNorm2d(out_filters))\n",
    "            layers.append(nn.LeakyReLU(0.2, inplace=True))\n",
    "            return layers\n",
    "\n",
    "        self.model = nn.Sequential(\n",
    "            *discriminator_block(channels, 64, normalize=False),\n",
    "            *discriminator_block(64, 128),\n",
    "            *discriminator_block(128, 256),\n",
    "            *discriminator_block(256, 512),\n",
    "            nn.ZeroPad2d((1, 0, 1, 0)),\n",
    "            nn.Conv2d(512, 1, 4, padding=1)\n",
    "        )\n",
    "\n",
    "    def forward(self, img):\n",
    "        return self.model(img)\n",
    "    \n",
    "class PatchSampleF(nn.Module):\n",
    "    def __init__(self, use_mlp = True, init_type = 'normal', init_gain = 0.02, nc = 256):\n",
    "        # potential issues: currently, we use the same patch_ids for multiple images in the batch\n",
    "        super(PatchSampleF, self).__init__()\n",
    "        self.l2norm = Normalize(2)\n",
    "        self.use_mlp = use_mlp\n",
    "        self.nc = nc  # hard-coded\n",
    "        self.mlp_init = False\n",
    "        self.init_type = init_type\n",
    "        self.init_gain = init_gain\n",
    "\n",
    "    def create_mlp(self, feats):\n",
    "        for mlp_id, feat in enumerate(feats):\n",
    "            input_nc = feat.shape[1]\n",
    "            mlp = nn.Sequential(*[nn.Linear(input_nc, self.nc), nn.ReLU(), nn.Linear(self.nc, self.nc)])\n",
    "            if torch.cuda.is_available():\n",
    "                mlp.cuda()\n",
    "            setattr(self, 'mlp_%d' % mlp_id, mlp)\n",
    "        if torch.cuda.is_available():\n",
    "            self.to(device)\n",
    "        self.apply(weights_init_normal)\n",
    "        self.mlp_init = True\n",
    "\n",
    "    def forward(self, feats, num_patches = 64, patch_ids = None):\n",
    "        return_ids = []\n",
    "        return_feats = []\n",
    "        if self.use_mlp and not self.mlp_init:\n",
    "            self.create_mlp(feats)\n",
    "        for feat_id, feat in enumerate(feats):\n",
    "            B, H, W = feat.shape[0], feat.shape[2], feat.shape[3]\n",
    "            feat_reshape = feat.permute(0, 2, 3, 1).flatten(1, 2)\n",
    "            if num_patches > 0:\n",
    "                if patch_ids is not None:\n",
    "                    patch_id = patch_ids[feat_id]\n",
    "                else:\n",
    "                    patch_id = torch.randperm(feat_reshape.shape[1], device=feats[0].device)\n",
    "                    patch_id = patch_id[:int(min(num_patches, patch_id.shape[0]))]  # .to(patch_ids.device)\n",
    "                x_sample = feat_reshape[:, patch_id, :].flatten(0, 1)  # reshape(-1, x.shape[1])\n",
    "            else:\n",
    "                x_sample = feat_reshape\n",
    "                patch_id = []\n",
    "            if self.use_mlp:\n",
    "                mlp = getattr(self, 'mlp_%d' % feat_id)\n",
    "                x_sample = mlp(x_sample)\n",
    "            return_ids.append(patch_id)\n",
    "            x_sample = self.l2norm(x_sample)\n",
    "\n",
    "            if num_patches == 0:\n",
    "                x_sample = x_sample.permute(0, 2, 1).reshape([B, x_sample.shape[-1], H, W])\n",
    "            return_feats.append(x_sample)\n",
    "        return return_feats, return_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Class to create separate losses for each feature, from https://github.com/taesungp/contrastive-unpaired-translation\n",
    "class PatchNCELoss(nn.Module):\n",
    "    def __init__(self, batch_size, nce_T = 0.07):\n",
    "        super().__init__()\n",
    "        self.batch_size = batch_size\n",
    "        self.nce_T = nce_T\n",
    "        self.cross_entropy_loss = torch.nn.CrossEntropyLoss(reduction='none')\n",
    "        self.mask_dtype = torch.bool\n",
    "\n",
    "    def forward(self, feat_q, feat_k):\n",
    "        batchSize = feat_q.shape[0]\n",
    "        dim = feat_q.shape[1]\n",
    "        feat_k = feat_k.detach()\n",
    "\n",
    "        # pos logit\n",
    "        l_pos = torch.bmm(feat_q.view(batchSize, 1, -1), feat_k.view(batchSize, -1, 1))\n",
    "        l_pos = l_pos.view(batchSize, 1)\n",
    "\n",
    "        batch_dim_for_bmm = self.batch_size\n",
    "\n",
    "        # reshape features to batch size\n",
    "        feat_q = feat_q.view(batch_dim_for_bmm, -1, dim)\n",
    "        feat_k = feat_k.view(batch_dim_for_bmm, -1, dim)\n",
    "        npatches = feat_q.size(1)\n",
    "        l_neg_curbatch = torch.bmm(feat_q, feat_k.transpose(2, 1))\n",
    "\n",
    "        # diagonal entries are similarity between same features, and hence meaningless.\n",
    "        # just fill the diagonal with very small number, which is exp(-10) and almost zero\n",
    "        diagonal = torch.eye(npatches, device=feat_q.device, dtype=self.mask_dtype)[None, :, :]\n",
    "        l_neg_curbatch.masked_fill_(diagonal, -10.0)\n",
    "        l_neg = l_neg_curbatch.view(-1, npatches)\n",
    "\n",
    "        out = torch.cat((l_pos, l_neg), dim=1) / self.nce_T\n",
    "\n",
    "        loss = self.cross_entropy_loss(out, torch.zeros(out.size(0), dtype=torch.long,\n",
    "                                                        device=feat_q.device))\n",
    "\n",
    "        return loss\n",
    "    \n",
    "def nonsaturating_loss(prediction, is_real):\n",
    "    if is_real.mean() == 1:\n",
    "        loss = F.softplus(-prediction).view(prediction.size(0), -1).mean(dim = 1)\n",
    "    else:\n",
    "        loss = F.softplus(prediction).view(prediction.size(0), -1).mean(dim = 1)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the networks, load the most recent saved models, and set the loss functions and optimizers\n",
    "\n",
    "if faces:\n",
    "    input_size = (input_size[0], input_size[1], input_size[1])\n",
    "G = Generator(input_size, res_blocks).to(device)\n",
    "D = Discriminator(input_size).to(device)\n",
    "Sampler = PatchSampleF(batch_size).to(device)\n",
    "\n",
    "if nonsaturating:\n",
    "    criterion_GAN = nonsaturating_loss\n",
    "else:\n",
    "    criterion_GAN = torch.nn.MSELoss().to(device)\n",
    "criterion_NCE = []\n",
    "\n",
    "for nce_layer in nce_layers:\n",
    "    criterion_NCE.append(PatchNCELoss(batch_size).to(device))\n",
    "\n",
    "G.apply(weights_init_normal)\n",
    "D.apply(weights_init_normal)\n",
    "\n",
    "optimizer_G = torch.optim.Adam(G.parameters(), lr = learning_rate, betas = betas)\n",
    "optimizer_D = torch.optim.Adam(D.parameters(), lr = learning_rate, betas = betas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters for the sampler are not made until the first forward pass through the Sampler network\n",
    "# Hence, we do a 'trial' training pass before setting the optimizer for the Sampler\n",
    "for i, batch in enumerate(dataloader):\n",
    "    x, y = Variable(batch[\"a\"].type(Tensor)), Variable(batch[\"b\"].type(Tensor))\n",
    "    \n",
    "    real = Variable(Tensor(np.ones((x.size(0), *D.output_shape))), requires_grad = False)\n",
    "    fake = Variable(Tensor(np.zeros((x.size(0), *D.output_shape))), requires_grad = False)\n",
    "    \n",
    "    D.eval()\n",
    "    G.eval()\n",
    "    Sampler.eval()\n",
    "    # get the fake loss\n",
    "    fake_y = G(x)\n",
    "    D_fake = D(fake_y.detach())\n",
    "    loss_D_fake = criterion_GAN(D_fake, fake).mean()\n",
    "    # get the real loss\n",
    "    D_real = D(y)\n",
    "    loss_D_real = criterion_GAN(D_real, real).mean()\n",
    "    # combine loss and calculate gradients\n",
    "    loss_D = (loss_D_fake + loss_D_real) * 0.5\n",
    "    loss_D.backward()\n",
    "\n",
    "    # get the fake GAN loss\n",
    "    D_fake = D(fake_y)\n",
    "    loss_G_GAN = lambda_GAN * criterion_GAN(D_fake, real).mean()\n",
    "    total_nce_loss = 0\n",
    "    for fake, real in [(fake_y, x), (y, G(y))]:\n",
    "        # get the NCE loss\n",
    "        feat_q = G(fake, nce_layers, encode_only = True)\n",
    "        feat_k = G(real, nce_layers, encode_only = True)\n",
    "\n",
    "        feat_k_pool, sample_ids = Sampler(feat_k, 256, None)\n",
    "        feat_q_pool, _ = Sampler(feat_q, 256, sample_ids)\n",
    "\n",
    "        total_nce_loss = 0.0\n",
    "        for f_q, f_k, crit, nce_layer in zip(feat_q_pool, feat_k_pool, criterion_NCE, nce_layers):\n",
    "            loss = crit(f_q, f_k) * lambda_NCE\n",
    "            total_nce_loss += loss.mean()\n",
    "\n",
    "        nce_loss = total_nce_loss / len(nce_layers)\n",
    "        total_nce_loss += nce_loss\n",
    "\n",
    "    loss_G = loss_G_GAN + total_nce_loss\n",
    "    loss_G.backward()\n",
    "    \n",
    "    break\n",
    "\n",
    "optimizer_Sampler = torch.optim.Adam(Sampler.parameters(), lr = learning_rate, betas = betas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the most recently saved models if available\n",
    "if os.path.isdir('models/%s' % model_name) and load_weights:\n",
    "    # Get the most recent model and load them\n",
    "    epoch = max([int(fname[8:-4]) for fname in os.listdir('models/%s' % model_name) if 'Sampler' in fname])\n",
    "    G.load_state_dict(torch.load('models/%s/G_%d.pth' % (model_name, epoch)))\n",
    "    D.load_state_dict(torch.load('models/%s/D_%d.pth' % (model_name, epoch)))\n",
    "    Sampler.load_state_dict(torch.load('models/%s/Sampler_%d.pth' % (model_name, epoch)))\n",
    "    # Load the losses as well, for plotting\n",
    "    losses = np.load('models/%s/losses_%d.npy' % (model_name, epoch))\n",
    "    D_losses = list(losses[0])\n",
    "    GAN_losses = list(losses[1])\n",
    "    NCE_losses = list(losses[2])\n",
    "    G_total_losses = list(losses[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[WinError 2] The system cannot find the file specified: 'models/G2M-frames/G_14.pth'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-14-2e407c10eabd>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    118\u001b[0m     \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msave\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mSampler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstate_dict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"models/%s/Sampler_%d.pth\"\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mmodel_name\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    119\u001b[0m     \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msave\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'models/%s/losses_%d.npy'\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mmodel_name\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mD_losses\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mGAN_losses\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mNCE_losses\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mG_total_losses\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 120\u001b[1;33m     \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mremove\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'models/%s/G_%d.pth'\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mmodel_name\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepoch\u001b[0m \u001b[1;33m-\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    121\u001b[0m     \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mremove\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'models/%s/D_%d.pth'\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mmodel_name\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepoch\u001b[0m \u001b[1;33m-\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    122\u001b[0m     \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mremove\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'models/%s/Sampler_%d.pth'\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mmodel_name\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepoch\u001b[0m \u001b[1;33m-\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [WinError 2] The system cannot find the file specified: 'models/G2M-frames/G_14.pth'"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Training Loop, adapted from https://github.com/taesungp/contrastive-unpaired-translation\n",
    "\n",
    "prev_time = time.time()\n",
    "for epoch in range(epoch, epochs):\n",
    "    total_D_loss = 0\n",
    "    total_GAN_loss = 0\n",
    "    total_NCE_loss = 0\n",
    "    total_G_loss = 0\n",
    "    for i, batch in enumerate(dataloader):\n",
    "        x, y = Variable(batch[\"a\"].type(Tensor)), Variable(batch[\"b\"].type(Tensor))\n",
    "        \n",
    "        # Adversarial ground truths\n",
    "        real = Variable(Tensor(np.ones((x.size(0), *D.output_shape))), requires_grad = False)\n",
    "        fake = Variable(Tensor(np.zeros((x.size(0), *D.output_shape))), requires_grad = False)\n",
    "        \n",
    "        # train discriminator\n",
    "        D.train()\n",
    "        optimizer_D.zero_grad()\n",
    "        # get the fake loss\n",
    "        fake_y = G(x)\n",
    "        D_fake = D(fake_y.detach())\n",
    "        loss_D_fake = criterion_GAN(D_fake, fake).mean()\n",
    "        # get the real loss\n",
    "        D_real = D(y)\n",
    "        loss_D_real = criterion_GAN(D_real, real).mean()\n",
    "        # combine loss and calculate gradients\n",
    "        loss_D = (loss_D_fake + loss_D_real) * 0.5\n",
    "        loss_D.backward()\n",
    "        optimizer_D.step()\n",
    "        \n",
    "        # train generator\n",
    "        G.train()\n",
    "        optimizer_G.zero_grad()\n",
    "        optimizer_Sampler.zero_grad()\n",
    "        # get the fake GAN loss\n",
    "        D_fake = D(fake_y)\n",
    "        loss_G_GAN = criterion_GAN(D_fake, real).mean()\n",
    "        # get the NCE loss\n",
    "        Sampler.train()\n",
    "        total_nce_loss = 0\n",
    "        for fake, real in [(fake_y, x), (y, G(y))]:\n",
    "            feat_q = G(fake_y, nce_layers, encode_only = True)\n",
    "            feat_k = G(x, nce_layers, encode_only = True)\n",
    "\n",
    "            feat_k_pool, sample_ids = Sampler(feat_k, 256, None)\n",
    "            feat_q_pool, _ = Sampler(feat_q, 256, sample_ids)\n",
    "\n",
    "            total_nce_loss = 0.0\n",
    "            for f_q, f_k, crit, nce_layer in zip(feat_q_pool, feat_k_pool, criterion_NCE, nce_layers):\n",
    "                loss = crit(f_q, f_k) * lambda_NCE\n",
    "                total_nce_loss += loss.mean()\n",
    "\n",
    "            nce_loss = total_nce_loss / len(nce_layers)\n",
    "            total_nce_loss += nce_loss\n",
    "        total_nce_loss *= 0.5\n",
    "        \n",
    "        loss_G = loss_G_GAN + total_nce_loss\n",
    "        loss_G.backward()\n",
    "        optimizer_G.step()\n",
    "        optimizer_Sampler.step()\n",
    "        \n",
    "        # --------------\n",
    "        #  Log Progress\n",
    "        # --------------\n",
    "\n",
    "        # Determine approximate time left\n",
    "        batches_done = epoch * len(dataloader) + i\n",
    "        batches_left = 200 * len(dataloader) - batches_done\n",
    "        time_left = datetime.timedelta(seconds=batches_left * (time.time() - prev_time))\n",
    "        prev_time = time.time()\n",
    "\n",
    "        # Print log\n",
    "        sys.stdout.write(\n",
    "            \"\\r[Epoch %d/%d] [Batch %d/%d] [D loss: %f] [GAN loss: %f, NCE loss: %f, Total: %f] ETA: %s\"\n",
    "            % (\n",
    "                epoch,\n",
    "                200,\n",
    "                i,\n",
    "                len(dataloader),\n",
    "                loss_D.item(),\n",
    "                loss_G_GAN.item(),\n",
    "                nce_loss.item(),\n",
    "                loss_G.item(),\n",
    "                time_left,\n",
    "            )\n",
    "        )\n",
    "        \n",
    "        total_D_loss += loss_D.item()\n",
    "        total_GAN_loss += loss_G_GAN.item()\n",
    "        total_NCE_loss += nce_loss.item()\n",
    "        total_G_loss += loss_G.item()\n",
    "        \n",
    "        # If at sample interval save image\n",
    "        if batches_done % 300 == 0:\n",
    "            sample_images(batches_done)\n",
    "            \n",
    "    D_losses.append(total_D_loss / len(dataloader))\n",
    "    GAN_losses.append(total_GAN_loss / len(dataloader))\n",
    "    NCE_losses.append(total_NCE_loss / len(dataloader))\n",
    "    G_total_losses.append(total_G_loss / len(dataloader))\n",
    "    display.clear_output(wait = True)\n",
    "    time.sleep(1)\n",
    "    plt.clf()\n",
    "    plt.ylabel('Mean Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.plot(D_losses, color = 'r')\n",
    "    plt.plot(GAN_losses, color = 'g')\n",
    "    plt.plot(NCE_losses, color = 'b')\n",
    "    plt.savefig('models/%s/loss_plot.png' % model_name)\n",
    "    display.display(plt.gcf())\n",
    "    \n",
    "    if not os.path.isdir('models'):\n",
    "        os.makedirs('models')\n",
    "    if not os.path.isdir('models/%s' % model_name):\n",
    "        os.makedirs('models/%s' % model_name)\n",
    "    torch.save(G.state_dict(), \"models/%s/G_%d.pth\" % (model_name, epoch))\n",
    "    torch.save(D.state_dict(), \"models/%s/D_%d.pth\" % (model_name, epoch))\n",
    "    torch.save(Sampler.state_dict(), \"models/%s/Sampler_%d.pth\" % (model_name, epoch))\n",
    "    np.save('models/%s/losses_%d.npy' % (model_name, epoch), np.array([D_losses, GAN_losses, NCE_losses, G_total_losses]))\n",
    "    try:\n",
    "        os.remove('models/%s/G_%d.pth' % (model_name, epoch - 1))\n",
    "        os.remove('models/%s/D_%d.pth' % (model_name, epoch - 1))\n",
    "        os.remove('models/%s/Sampler_%d.pth' % (model_name, epoch - 1))\n",
    "        os.remove('models/%s/losses_%d.pth' % (model_name, epoch - 1))\n",
    "    except:\n",
    "        pass\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
