{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: numpy in c:\\users\\andre\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (1.19.3)\n",
      "Requirement already satisfied: opencv-python in c:\\users\\andre\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (4.4.0.46)\n",
      "Requirement already satisfied: dlib in c:\\users\\andre\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (19.21.1)\n",
      "Requirement already satisfied: matplotlib in c:\\users\\andre\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (3.3.3)\n",
      "Requirement already satisfied: boto3 in c:\\users\\andre\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (1.17.4)\n",
      "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.3 in c:\\users\\andre\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from matplotlib) (2.4.7)\n",
      "Requirement already satisfied: python-dateutil>=2.1 in c:\\users\\andre\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from matplotlib) (2.8.1)\n",
      "Requirement already satisfied: pillow>=6.2.0 in c:\\users\\andre\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from matplotlib) (8.0.1)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in c:\\users\\andre\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from matplotlib) (1.3.1)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\andre\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from matplotlib) (0.10.0)\n",
      "Requirement already satisfied: s3transfer<0.4.0,>=0.3.0 in c:\\users\\andre\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from boto3) (0.3.4)\n",
      "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in c:\\users\\andre\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from boto3) (0.10.0)\n",
      "Requirement already satisfied: botocore<1.21.0,>=1.20.4 in c:\\users\\andre\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from boto3) (1.20.4)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\andre\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from python-dateutil>=2.1->matplotlib) (1.15.0)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.25.4 in c:\\users\\andre\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from botocore<1.21.0,>=1.20.4->boto3) (1.26.2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 20.2.1; however, version 21.0.1 is available.\n",
      "You should consider upgrading via the 'c:\\users\\andre\\appdata\\local\\programs\\python\\python38\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    }
   ],
   "source": [
    "!pip install numpy opencv-python dlib matplotlib boto3\n",
    "try:\n",
    "    import torch\n",
    "except:\n",
    "    !pip install pip install torch==1.7.1+cu101 torchvision==0.8.2+cu101 torchaudio===0.7.2 -f https://download.pytorch.org/whl/torch_stable.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import numpy as np\n",
    "import cv2\n",
    "import os\n",
    "import glob\n",
    "import random\n",
    "import time, datetime, sys\n",
    "import zipfile as zf\n",
    "import boto3\n",
    "from PIL import Image\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch.autograd import Variable\n",
    "from torchvision import transforms, datasets\n",
    "from torchvision.utils import save_image, make_grid\n",
    "\n",
    "import dlib\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython import display\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the model name - determines where the outputs are saved\n",
    "model_name = 'G2M-frames-res11'\n",
    "\n",
    "# Variables/Hyperparameters\n",
    "dataset_size = 600\n",
    "generate_dataset = True\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "Tensor = torch.cuda.FloatTensor if torch.cuda.is_available() else torch.Tensor\n",
    "nce_layers = [0, 4, 8, 12, 16]\n",
    "lambda_NCE = 1.0\n",
    "lambda_GAN = 1.0\n",
    "batch_size = 1\n",
    "load_weights = True\n",
    "epoch = 0\n",
    "epochs = 100\n",
    "nonsaturating = False\n",
    "\n",
    "input_size = (3,144,256)\n",
    "res_blocks = 11 # def = 9\n",
    "learning_rate = 0.001\n",
    "betas = (0.9, 0.999)\n",
    "kernel_size = 3\n",
    "\n",
    "D_losses = []\n",
    "GAN_losses = []\n",
    "NCE_losses = []\n",
    "G_total_losses = []\n",
    "\n",
    "# Swap the game and movie datasets, so that translation goes from movie to game instead\n",
    "swap = False\n",
    "\n",
    "# Toggle whether translation is done only on faces\n",
    "faces = False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the original data (and the processed data)\n",
    "s3 = boto3.resource('s3', aws_access_key_id = 'AKIAIOPFTDXA3ZXLK5YA', aws_secret_access_key = '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')\n",
    "bucket = s3.Bucket('vision-dataset-vmrj42')\n",
    "\n",
    "for fname in ['Data.zip', 'datasets.zip']:\n",
    "    if not os.path.isfile(fname):\n",
    "        bucket.download_file(fname, fname)\n",
    "\n",
    "if not os.path.isdir('Data'):\n",
    "    files = zf.ZipFile('Data.zip', 'r')\n",
    "    files.extractall('')\n",
    "if not os.path.isdir('dataset') and not generate_dataset:\n",
    "    files = zf.ZipFile('datasets.zip', 'r')\n",
    "    files.extractall('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a basic dataset for full-frame translation\n",
    "\n",
    "for dname in ['dataset/train/game', \n",
    "              'dataset/train/movie', \n",
    "              'dataset/test/game', \n",
    "              'dataset/test/movie', \n",
    "              'face_dataset/train/game',\n",
    "              'face_dataset/train/movie',\n",
    "              'face_dataset/test/game',\n",
    "              'face_dataset/test/movie']:\n",
    "    if not os.path.isdir(dname):\n",
    "        os.makedirs(dname)\n",
    "\n",
    "if len(os.listdir('dataset/train/game')) < dataset_size:\n",
    "    # get some frames from the game footage\n",
    "    cap = cv2.VideoCapture('Data/game/MafiaVideogame.mp4')\n",
    "    length = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "    frame_count = 0\n",
    "    saved_frames = 0\n",
    "    \n",
    "    face_detector = dlib.get_frontal_face_detector()\n",
    "    faces = []\n",
    "\n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        frame_count += 1\n",
    "        # Save the entire frame as part of the dataset, alternating between the training and testing datasets\n",
    "        if frame_count % (length // (2 * dataset_size)) == 0 and ret:\n",
    "            if saved_frames < dataset_size:\n",
    "                fname = 'dataset/train/game/%d.png' % (saved_frames)\n",
    "            else:\n",
    "                fname = 'dataset/test/game/%d.png' % (saved_frames % dataset_size)\n",
    "            cv2.imwrite(fname, frame)\n",
    "            saved_frames += 1\n",
    "        \n",
    "        # Check if there is a face in every (length // (10 * dataset_size)) frame\n",
    "        if frame_count % (length // (6 * dataset_size)) == 0 and ret:\n",
    "            dets = face_detector(frame, 1)\n",
    "            for i, d in enumerate(dets):\n",
    "                left, top, right, bottom = d.left(), d.top(), d.right(), d.bottom()\n",
    "                if right - left > 60:\n",
    "                    face = frame[top:bottom, left:right]\n",
    "                    if len(face) > 0 and len(face[0]) > 0:\n",
    "                        faces.append(face)\n",
    "    cap.release()\n",
    "    \n",
    "    # Alternating between the training and testing datasets, save the extracted faces\n",
    "    saved_faces = 0\n",
    "    for i, face in enumerate(faces):\n",
    "        if i % (len(faces) // (2 * dataset_size)) == 0:\n",
    "            if saved_faces < dataset_size:\n",
    "                fname = 'face_dataset/train/game/%d.png' % (saved_faces)\n",
    "            else:\n",
    "                fname = 'face_dataset/test/game/%d.png' % (saved_faces % dataset_size)\n",
    "            cv2.imwrite(fname, cv2.resize(face, (input_size[1], input_size[1])))\n",
    "            saved_faces += 1\n",
    "\n",
    "    # get some frames from the movie footage\n",
    "    movie_dirs = ['Data/movie/TheGodfather.mp4', 'Data/movie/TheIrishman.mp4', 'Data/movie/TheSopranos.mp4']\n",
    "\n",
    "    saved_frames = 0\n",
    "    for movie in movie_dirs:\n",
    "        cap = cv2.VideoCapture(movie)\n",
    "        length = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "        frame_count = 0\n",
    "\n",
    "        while cap.isOpened():\n",
    "            ret, frame = cap.read()\n",
    "            if not ret:\n",
    "                break\n",
    "            frame_count += 1\n",
    "            if frame_count % (length // (2 * dataset_size / len(movie_dirs))) == 0 and ret:\n",
    "                if saved_frames < dataset_size:\n",
    "                    fname = 'dataset/train/movie/%d.png' % (saved_frames)\n",
    "                else:\n",
    "                    fname = 'dataset/test/movie/%d.png' % (saved_frames % dataset_size)\n",
    "                cv2.imwrite(fname, frame)\n",
    "                saved_frames += 1\n",
    "\n",
    "\n",
    "        cap.release()\n",
    "        \n",
    "    face_detector = dlib.get_frontal_face_detector()        \n",
    "    faces_dir = 'Data/faces/'\n",
    "    real_faces = os.listdir(faces_dir)\n",
    "    saved_faces = 0\n",
    "    current_face = 0\n",
    "    while saved_faces < dataset_size * 4:\n",
    "        real_face = cv2.imread(faces_dir + real_faces[current_face])\n",
    "        face = []\n",
    "        dets = face_detector(real_face, 1)\n",
    "        for d in dets:\n",
    "            left, top, right, bottom = d.left(), d.top(), d.right(), d.bottom()\n",
    "            if right - left > 60:\n",
    "                face = real_face[top : bottom, left : right]\n",
    "                if len(face) > 0 and len(face[0]) > 0:\n",
    "                    face = cv2.resize(face, (input_size[1], input_size[1]))\n",
    "                    if saved_faces < dataset_size * 2:\n",
    "                        fname = 'face_dataset/train/movie/%d.png' % (saved_faces)\n",
    "                    else:\n",
    "                        fname = 'face_dataset/test/movie/%d.png' % (saved_faces % (dataset_size * 2))\n",
    "                    cv2.imwrite(fname, face)\n",
    "                    saved_faces += 1\n",
    "                    break\n",
    "        current_face += 1\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helpful class for loading both game and movie samples as one dataset\n",
    "class ImageDataset(Dataset):\n",
    "    def __init__(self, root, swap = False, transforms_ = None, unaligned = False, mode = \"train\"):\n",
    "        self.transform = transforms.Compose(transforms_)\n",
    "        self.unaligned = unaligned\n",
    "        if swap:\n",
    "            self.files_game = sorted(glob.glob(os.path.join(root, \"%s/movie\" % mode) + \"/*.*\"))\n",
    "            self.files_movie = sorted(glob.glob(os.path.join(root, \"%s/game\" % mode) + \"/*.*\"))\n",
    "        else:\n",
    "            self.files_game = sorted(glob.glob(os.path.join(root, \"%s/game\" % mode) + \"/*.*\"))\n",
    "            self.files_movie = sorted(glob.glob(os.path.join(root, \"%s/movie\" % mode) + \"/*.*\"))\n",
    "        print(len(os.listdir(os.path.join(root, 'train/movie'))))\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        image_game = Image.open(self.files_game[index % len(self.files_game)])\n",
    "\n",
    "        if self.unaligned:\n",
    "            image_movie = Image.open(self.files_movie[random.randint(0, len(self.files_movie) - 1)])\n",
    "        else:\n",
    "            image_movie = Image.open(self.files_movie[index % len(self.files_movie)])\n",
    "\n",
    "        # Convert grayscale images to rgb\n",
    "        if image_game.mode != \"RGB\":\n",
    "            image_game = to_rgb(image_game)\n",
    "        if image_movie.mode != \"RGB\":\n",
    "            image_movie = to_rgb(image_movie)\n",
    "\n",
    "        item_game = self.transform(image_game)\n",
    "        item_movie = self.transform(image_movie)\n",
    "        \n",
    "        return {\"a\": item_game, \"b\": item_movie}\n",
    "\n",
    "    def __len__(self):\n",
    "        return max(len(self.files_game), len(self.files_movie))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "600\n",
      "600\n"
     ]
    }
   ],
   "source": [
    "# Loading the dataset\n",
    "# Define the dataset-wide transformations\n",
    "\n",
    "transforms_ = [\n",
    "    transforms.Resize(int(input_size[1] * 1.4), Image.BICUBIC),\n",
    "    transforms.RandomCrop((input_size[1],input_size[2])),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
    "]\n",
    "\n",
    "\n",
    "if faces:\n",
    "    dataset_dir = 'face_dataset'\n",
    "else:\n",
    "    dataset_dir = 'dataset'\n",
    "# Training data loader\n",
    "dataloader = DataLoader(\n",
    "    ImageDataset(dataset_dir, swap = swap, transforms_ = transforms_, unaligned = True),\n",
    "    batch_size = batch_size,\n",
    "    shuffle = True,\n",
    "    num_workers = 0,\n",
    ")\n",
    "# Test data loader\n",
    "val_dataloader = DataLoader(\n",
    "    ImageDataset(dataset_dir, swap = swap, transforms_ = transforms_, unaligned = True, mode = 'test'),\n",
    "    batch_size = 5,\n",
    "    shuffle = True,\n",
    "    num_workers = 0,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Other Utils\n",
    "\n",
    "inv_normalize = transforms.Normalize(\n",
    "    mean=[-0.5/0.5, -0.5/0.5, -0.5/0.5],\n",
    "    std=[1/0.5, 1/0.5, 1/0.5]\n",
    ")\n",
    "\n",
    "def weights_init_normal(m):\n",
    "    classname = m.__class__.__name__\n",
    "    if classname.find(\"Conv\") != -1:\n",
    "        torch.nn.init.normal_(m.weight.data, 0.0, 0.02)\n",
    "        if hasattr(m, \"bias\") and m.bias is not None:\n",
    "            torch.nn.init.constant_(m.bias.data, 0.0)\n",
    "    elif classname.find(\"BatchNorm2d\") != -1:\n",
    "        torch.nn.init.normal_(m.weight.data, 1.0, 0.02)\n",
    "        torch.nn.init.constant_(m.bias.data, 0.0)\n",
    "        \n",
    "def sample_images(batches_done):\n",
    "    \"\"\"Saves a generated sample from the test set\"\"\"\n",
    "    imgs = next(iter(val_dataloader))\n",
    "    G.eval()\n",
    "    real_A = Variable(imgs[\"a\"].type(Tensor))\n",
    "    fake_B = G(real_A)\n",
    "    for img in fake_B:\n",
    "        img = inv_normalize(img)\n",
    "    # Arange images along x-axis\n",
    "    real_A = make_grid(real_A, nrow=5, normalize=True)\n",
    "    fake_B = make_grid(fake_B, nrow=5, normalize=True)\n",
    "    # Arange images along y-axis\n",
    "    image_grid = torch.cat((real_A, fake_B), 1)\n",
    "    if not os.path.isdir('models/%s/samples' % model_name):\n",
    "        os.makedirs('models/%s/samples' % model_name)\n",
    "    save_image(image_grid, \"models/%s/samples/%s.png\" % (model_name, batches_done), normalize=False)\n",
    "    \n",
    "class Normalize(nn.Module):\n",
    "    def __init__(self, power=2):\n",
    "        super(Normalize, self).__init__()\n",
    "        self.power = power\n",
    "\n",
    "    def forward(self, x):\n",
    "        norm = x.pow(self.power).sum(1, keepdim=True).pow(1. / self.power)\n",
    "        out = x.div(norm + 1e-7)\n",
    "        return out\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generator and Discriminator models adapted from https://github.com/eriklindernoren/PyTorch-GAN\n",
    "# PatchSampleF adapted from https://github.com/taesungp/contrastive-unpaired-translation\n",
    "\n",
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, in_features, kernel):\n",
    "        super(ResidualBlock, self).__init__()\n",
    "\n",
    "        self.block = nn.Sequential(\n",
    "            nn.ReflectionPad2d(kernel // 2),\n",
    "            nn.Conv2d(in_features, in_features, kernel),\n",
    "            nn.InstanceNorm2d(in_features),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.ReflectionPad2d(kernel // 2),\n",
    "            nn.Conv2d(in_features, in_features, kernel),\n",
    "            nn.InstanceNorm2d(in_features),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x + self.block(x)\n",
    "\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self, input_shape, num_residual_blocks, kernel):\n",
    "        super(Generator, self).__init__()\n",
    "        channels = input_shape[0]\n",
    "\n",
    "        # Initial convolution block\n",
    "        out_features = 64\n",
    "        model = [\n",
    "            nn.ReflectionPad2d(kernel),\n",
    "            nn.Conv2d(channels, out_features, 2 * kernel + 1),\n",
    "            nn.InstanceNorm2d(out_features),\n",
    "            nn.ReLU(inplace=True),\n",
    "        ]\n",
    "        in_features = out_features\n",
    "\n",
    "        # Downsampling\n",
    "        for _ in range(2):\n",
    "            out_features *= 2\n",
    "            model += [\n",
    "                nn.Conv2d(in_features, out_features, kernel, stride=2, padding=kernel//2),\n",
    "                nn.InstanceNorm2d(out_features),\n",
    "                nn.ReLU(inplace=True),\n",
    "            ]\n",
    "            in_features = out_features\n",
    "\n",
    "        # Residual blocks\n",
    "        for _ in range(num_residual_blocks):\n",
    "            model += [ResidualBlock(out_features, kernel)]\n",
    "\n",
    "        # Upsampling\n",
    "        for _ in range(2):\n",
    "            out_features //= 2\n",
    "            model += [\n",
    "                nn.Upsample(scale_factor=2),\n",
    "                nn.Conv2d(in_features, out_features, kernel, stride=1, padding=kernel//2),\n",
    "                nn.InstanceNorm2d(out_features),\n",
    "                nn.ReLU(inplace=True),\n",
    "            ]\n",
    "            in_features = out_features\n",
    "\n",
    "        # Output layer\n",
    "        model += [\n",
    "            nn.ReflectionPad2d(kernel), \n",
    "            nn.Conv2d(out_features, channels, 2 * kernel + 1), \n",
    "            nn.Tanh()\n",
    "        ]\n",
    "\n",
    "        self.model = nn.Sequential(*model)\n",
    "\n",
    "    def forward(self, x, layers = [], encode_only = False):\n",
    "        if -1 in layers:\n",
    "            layers.append(len(self.model))\n",
    "        if len(layers) > 0:\n",
    "            feat = x\n",
    "            feats = []\n",
    "            for layer_id, layer in enumerate(self.model):\n",
    "                feat = layer(feat)\n",
    "                if layer_id in layers:\n",
    "                    feats.append(feat)\n",
    "                else:\n",
    "                    pass\n",
    "                if layer_id == layers[-1] and encode_only:\n",
    "                    return feats\n",
    "            return feat, feats\n",
    "        else:\n",
    "            return self.model(x)\n",
    "    \n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, input_shape):\n",
    "        super(Discriminator, self).__init__()\n",
    "        channels, height, width = input_shape\n",
    "        self.output_shape = (1, height // 2 ** 4, width // 2 ** 4)\n",
    "\n",
    "        def discriminator_block(in_filters, out_filters, normalize = True):\n",
    "            \"\"\"Returns downsampling layers of each discriminator block\"\"\"\n",
    "            layers = [nn.Conv2d(in_filters, out_filters, 4, stride=2, padding=1)]\n",
    "            if normalize:\n",
    "                layers.append(nn.InstanceNorm2d(out_filters))\n",
    "            layers.append(nn.LeakyReLU(0.2, inplace=True))\n",
    "            return layers\n",
    "\n",
    "        self.model = nn.Sequential(\n",
    "            *discriminator_block(channels, 64, normalize=False),\n",
    "            *discriminator_block(64, 128),\n",
    "            *discriminator_block(128, 256),\n",
    "            *discriminator_block(256, 512),\n",
    "            nn.ZeroPad2d((1, 0, 1, 0)),\n",
    "            nn.Conv2d(512, 1, 4, padding=1)\n",
    "        )\n",
    "\n",
    "    def forward(self, img):\n",
    "        return self.model(img)\n",
    "    \n",
    "class PatchSampleF(nn.Module):\n",
    "    def __init__(self, use_mlp = True, init_type = 'normal', init_gain = 0.02, nc = 256):\n",
    "        # potential issues: currently, we use the same patch_ids for multiple images in the batch\n",
    "        super(PatchSampleF, self).__init__()\n",
    "        self.l2norm = Normalize(2)\n",
    "        self.use_mlp = use_mlp\n",
    "        self.nc = nc  # hard-coded\n",
    "        self.mlp_init = False\n",
    "        self.init_type = init_type\n",
    "        self.init_gain = init_gain\n",
    "\n",
    "    def create_mlp(self, feats):\n",
    "        for mlp_id, feat in enumerate(feats):\n",
    "            input_nc = feat.shape[1]\n",
    "            mlp = nn.Sequential(*[nn.Linear(input_nc, self.nc), nn.ReLU(), nn.Linear(self.nc, self.nc)])\n",
    "            if torch.cuda.is_available():\n",
    "                mlp.cuda()\n",
    "            setattr(self, 'mlp_%d' % mlp_id, mlp)\n",
    "        if torch.cuda.is_available():\n",
    "            self.to(device)\n",
    "        self.apply(weights_init_normal)\n",
    "        self.mlp_init = True\n",
    "\n",
    "    def forward(self, feats, num_patches = 64, patch_ids = None):\n",
    "        return_ids = []\n",
    "        return_feats = []\n",
    "        if self.use_mlp and not self.mlp_init:\n",
    "            self.create_mlp(feats)\n",
    "        for feat_id, feat in enumerate(feats):\n",
    "            B, H, W = feat.shape[0], feat.shape[2], feat.shape[3]\n",
    "            feat_reshape = feat.permute(0, 2, 3, 1).flatten(1, 2)\n",
    "            if num_patches > 0:\n",
    "                if patch_ids is not None:\n",
    "                    patch_id = patch_ids[feat_id]\n",
    "                else:\n",
    "                    patch_id = torch.randperm(feat_reshape.shape[1], device=feats[0].device)\n",
    "                    patch_id = patch_id[:int(min(num_patches, patch_id.shape[0]))]  # .to(patch_ids.device)\n",
    "                x_sample = feat_reshape[:, patch_id, :].flatten(0, 1)  # reshape(-1, x.shape[1])\n",
    "            else:\n",
    "                x_sample = feat_reshape\n",
    "                patch_id = []\n",
    "            if self.use_mlp:\n",
    "                mlp = getattr(self, 'mlp_%d' % feat_id)\n",
    "                x_sample = mlp(x_sample)\n",
    "            return_ids.append(patch_id)\n",
    "            x_sample = self.l2norm(x_sample)\n",
    "\n",
    "            if num_patches == 0:\n",
    "                x_sample = x_sample.permute(0, 2, 1).reshape([B, x_sample.shape[-1], H, W])\n",
    "            return_feats.append(x_sample)\n",
    "        return return_feats, return_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Class to create separate losses for each feature, from https://github.com/taesungp/contrastive-unpaired-translation\n",
    "class PatchNCELoss(nn.Module):\n",
    "    def __init__(self, batch_size, nce_T = 0.07):\n",
    "        super().__init__()\n",
    "        self.batch_size = batch_size\n",
    "        self.nce_T = nce_T\n",
    "        self.cross_entropy_loss = torch.nn.CrossEntropyLoss(reduction='none')\n",
    "        self.mask_dtype = torch.bool\n",
    "\n",
    "    def forward(self, feat_q, feat_k):\n",
    "        batchSize = feat_q.shape[0]\n",
    "        dim = feat_q.shape[1]\n",
    "        feat_k = feat_k.detach()\n",
    "\n",
    "        # pos logit\n",
    "        l_pos = torch.bmm(feat_q.view(batchSize, 1, -1), feat_k.view(batchSize, -1, 1))\n",
    "        l_pos = l_pos.view(batchSize, 1)\n",
    "\n",
    "        batch_dim_for_bmm = self.batch_size\n",
    "\n",
    "        # reshape features to batch size\n",
    "        feat_q = feat_q.view(batch_dim_for_bmm, -1, dim)\n",
    "        feat_k = feat_k.view(batch_dim_for_bmm, -1, dim)\n",
    "        npatches = feat_q.size(1)\n",
    "        l_neg_curbatch = torch.bmm(feat_q, feat_k.transpose(2, 1))\n",
    "\n",
    "        # diagonal entries are similarity between same features, and hence meaningless.\n",
    "        # just fill the diagonal with very small number, which is exp(-10) and almost zero\n",
    "        diagonal = torch.eye(npatches, device=feat_q.device, dtype=self.mask_dtype)[None, :, :]\n",
    "        l_neg_curbatch.masked_fill_(diagonal, -10.0)\n",
    "        l_neg = l_neg_curbatch.view(-1, npatches)\n",
    "\n",
    "        out = torch.cat((l_pos, l_neg), dim=1) / self.nce_T\n",
    "\n",
    "        loss = self.cross_entropy_loss(out, torch.zeros(out.size(0), dtype=torch.long,\n",
    "                                                        device=feat_q.device))\n",
    "\n",
    "        return loss\n",
    "    \n",
    "def nonsaturating_loss(prediction, is_real):\n",
    "    if is_real.mean() == 1:\n",
    "        loss = F.softplus(-prediction).view(prediction.size(0), -1).mean(dim = 1)\n",
    "    else:\n",
    "        loss = F.softplus(prediction).view(prediction.size(0), -1).mean(dim = 1)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the networks, load the most recent saved models, and set the loss functions and optimizers\n",
    "\n",
    "if faces:\n",
    "    input_size = (input_size[0], input_size[1], input_size[1])\n",
    "G = Generator(input_size, res_blocks, kernel_size).to(device)\n",
    "D = Discriminator(input_size).to(device)\n",
    "Sampler = PatchSampleF(batch_size).to(device)\n",
    "\n",
    "if nonsaturating:\n",
    "    criterion_GAN = nonsaturating_loss\n",
    "else:\n",
    "    criterion_GAN = torch.nn.MSELoss().to(device)\n",
    "criterion_NCE = []\n",
    "\n",
    "for nce_layer in nce_layers:\n",
    "    criterion_NCE.append(PatchNCELoss(batch_size).to(device))\n",
    "\n",
    "G.apply(weights_init_normal)\n",
    "D.apply(weights_init_normal)\n",
    "\n",
    "optimizer_G = torch.optim.Adam(G.parameters(), lr = learning_rate, betas = betas)\n",
    "optimizer_D = torch.optim.Adam(D.parameters(), lr = learning_rate, betas = betas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters for the sampler are not made until the first forward pass through the Sampler network\n",
    "# Hence, we do a 'trial' training pass before setting the optimizer for the Sampler\n",
    "for i, batch in enumerate(dataloader):\n",
    "    x, y = Variable(batch[\"a\"].type(Tensor)), Variable(batch[\"b\"].type(Tensor))\n",
    "    \n",
    "    real = Variable(Tensor(np.ones((x.size(0), *D.output_shape))), requires_grad = False)\n",
    "    fake = Variable(Tensor(np.zeros((x.size(0), *D.output_shape))), requires_grad = False)\n",
    "    \n",
    "    D.eval()\n",
    "    G.eval()\n",
    "    Sampler.eval()\n",
    "    # get the fake loss\n",
    "    fake_y = G(x)\n",
    "    D_fake = D(fake_y.detach())\n",
    "    loss_D_fake = criterion_GAN(D_fake, fake).mean()\n",
    "    # get the real loss\n",
    "    D_real = D(y)\n",
    "    loss_D_real = criterion_GAN(D_real, real).mean()\n",
    "    # combine loss and calculate gradients\n",
    "    loss_D = (loss_D_fake + loss_D_real) * 0.5\n",
    "    loss_D.backward()\n",
    "\n",
    "    # get the fake GAN loss\n",
    "    D_fake = D(fake_y)\n",
    "    loss_G_GAN = lambda_GAN * criterion_GAN(D_fake, real).mean()\n",
    "    total_nce_loss = 0\n",
    "    for fake, real in [(fake_y, x), (y, G(y))]:\n",
    "        # get the NCE loss\n",
    "        feat_q = G(fake, nce_layers, encode_only = True)\n",
    "        feat_k = G(real, nce_layers, encode_only = True)\n",
    "\n",
    "        feat_k_pool, sample_ids = Sampler(feat_k, 256, None)\n",
    "        feat_q_pool, _ = Sampler(feat_q, 256, sample_ids)\n",
    "\n",
    "        total_nce_loss = 0.0\n",
    "        for f_q, f_k, crit, nce_layer in zip(feat_q_pool, feat_k_pool, criterion_NCE, nce_layers):\n",
    "            loss = crit(f_q, f_k) * lambda_NCE\n",
    "            total_nce_loss += loss.mean()\n",
    "\n",
    "        nce_loss = total_nce_loss / len(nce_layers)\n",
    "        total_nce_loss += nce_loss\n",
    "\n",
    "    loss_G = loss_G_GAN + total_nce_loss\n",
    "    loss_G.backward()\n",
    "    \n",
    "    break\n",
    "\n",
    "optimizer_Sampler = torch.optim.Adam(Sampler.parameters(), lr = learning_rate, betas = betas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the most recently saved models if available\n",
    "if os.path.isdir('models/%s' % model_name) and load_weights:\n",
    "    # Get the most recent model and load them\n",
    "    epoch = max([int(fname[8:-4]) for fname in os.listdir('models/%s' % model_name) if 'Sampler' in fname])\n",
    "    G.load_state_dict(torch.load('models/%s/G_%d.pth' % (model_name, epoch)))\n",
    "    D.load_state_dict(torch.load('models/%s/D_%d.pth' % (model_name, epoch)))\n",
    "    Sampler.load_state_dict(torch.load('models/%s/Sampler_%d.pth' % (model_name, epoch)))\n",
    "    # Load the losses as well, for plotting\n",
    "    losses = np.load('models/%s/losses_%d.npy' % (model_name, epoch))\n",
    "    D_losses = list(losses[0])\n",
    "    GAN_losses = list(losses[1])\n",
    "    NCE_losses = list(losses[2])\n",
    "    G_total_losses = list(losses[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 77/100] [Batch 529/600] [D loss: 0.233506] [GAN loss: 0.214324, NCE loss: 1.063670, Total: 3.405333] ETA: 3:49:47.50567674609"
     ]
    }
   ],
   "source": [
    "# Training Loop, adapted from https://github.com/taesungp/contrastive-unpaired-translation\n",
    "\n",
    "prev_time = time.time()\n",
    "for epoch in range(epoch, epochs):\n",
    "    total_D_loss = 0\n",
    "    total_GAN_loss = 0\n",
    "    total_NCE_loss = 0\n",
    "    total_G_loss = 0\n",
    "    for i, batch in enumerate(dataloader):\n",
    "        x, y = Variable(batch[\"a\"].type(Tensor)), Variable(batch[\"b\"].type(Tensor))\n",
    "        \n",
    "        # Adversarial ground truths\n",
    "        real = Variable(Tensor(np.ones((x.size(0), *D.output_shape))), requires_grad = False)\n",
    "        fake = Variable(Tensor(np.zeros((x.size(0), *D.output_shape))), requires_grad = False)\n",
    "        \n",
    "        # train discriminator\n",
    "        D.train()\n",
    "        optimizer_D.zero_grad()\n",
    "        # get the fake loss\n",
    "        fake_y = G(x)\n",
    "        D_fake = D(fake_y.detach())\n",
    "        loss_D_fake = criterion_GAN(D_fake, fake).mean()\n",
    "        # get the real loss\n",
    "        D_real = D(y)\n",
    "        loss_D_real = criterion_GAN(D_real, real).mean()\n",
    "        # combine loss and calculate gradients\n",
    "        loss_D = (loss_D_fake + loss_D_real) * 0.5\n",
    "        loss_D.backward()\n",
    "        optimizer_D.step()\n",
    "        \n",
    "        # train generator\n",
    "        G.train()\n",
    "        optimizer_G.zero_grad()\n",
    "        optimizer_Sampler.zero_grad()\n",
    "        # get the fake GAN loss\n",
    "        D_fake = D(fake_y)\n",
    "        loss_G_GAN = criterion_GAN(D_fake, real).mean()\n",
    "        # get the NCE loss\n",
    "        Sampler.train()\n",
    "        total_nce_loss = 0\n",
    "        for fake, real in [(fake_y, x), (y, G(y))]:\n",
    "            feat_q = G(fake_y, nce_layers, encode_only = True)\n",
    "            feat_k = G(x, nce_layers, encode_only = True)\n",
    "\n",
    "            feat_k_pool, sample_ids = Sampler(feat_k, 256, None)\n",
    "            feat_q_pool, _ = Sampler(feat_q, 256, sample_ids)\n",
    "\n",
    "            total_nce_loss = 0.0\n",
    "            for f_q, f_k, crit, nce_layer in zip(feat_q_pool, feat_k_pool, criterion_NCE, nce_layers):\n",
    "                loss = crit(f_q, f_k) * lambda_NCE\n",
    "                total_nce_loss += loss.mean()\n",
    "\n",
    "            nce_loss = total_nce_loss / len(nce_layers)\n",
    "            total_nce_loss += nce_loss\n",
    "        total_nce_loss *= 0.5\n",
    "        \n",
    "        loss_G = loss_G_GAN + total_nce_loss\n",
    "        loss_G.backward()\n",
    "        optimizer_G.step()\n",
    "        optimizer_Sampler.step()\n",
    "        \n",
    "        # --------------\n",
    "        #  Log Progress\n",
    "        # --------------\n",
    "\n",
    "        # Determine approximate time left\n",
    "        batches_done = epoch * len(dataloader) + i\n",
    "        batches_left = epochs * len(dataloader) - batches_done\n",
    "        time_left = datetime.timedelta(seconds=batches_left * (time.time() - prev_time))\n",
    "        prev_time = time.time()\n",
    "\n",
    "        # Print log\n",
    "        sys.stdout.write(\n",
    "            \"\\r[Epoch %d/%d] [Batch %d/%d] [D loss: %f] [GAN loss: %f, NCE loss: %f, Total: %f] ETA: %s\"\n",
    "            % (\n",
    "                epoch,\n",
    "                epochs,\n",
    "                i,\n",
    "                len(dataloader),\n",
    "                loss_D.item(),\n",
    "                loss_G_GAN.item(),\n",
    "                nce_loss.item(),\n",
    "                loss_G.item(),\n",
    "                time_left,\n",
    "            )\n",
    "        )\n",
    "        \n",
    "        total_D_loss += loss_D.item()\n",
    "        total_GAN_loss += loss_G_GAN.item()\n",
    "        total_NCE_loss += nce_loss.item()\n",
    "        total_G_loss += loss_G.item()\n",
    "        \n",
    "        # If at sample interval save image\n",
    "        if batches_done % 300 == 0:\n",
    "            sample_images(batches_done)\n",
    "            \n",
    "    D_losses.append(total_D_loss / len(dataloader))\n",
    "    GAN_losses.append(total_GAN_loss / len(dataloader))\n",
    "    NCE_losses.append(total_NCE_loss / len(dataloader))\n",
    "    G_total_losses.append(total_G_loss / len(dataloader))\n",
    "    display.clear_output(wait = True)\n",
    "    time.sleep(1)\n",
    "    plt.clf()\n",
    "    plt.ylabel('Mean Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.plot(D_losses, color = 'r')\n",
    "    plt.plot(GAN_losses, color = 'g')\n",
    "    plt.plot(NCE_losses, color = 'b')\n",
    "    plt.savefig('models/%s/loss_plot.png' % model_name)\n",
    "    display.display(plt.gcf())\n",
    "    \n",
    "    if not os.path.isdir('models'):\n",
    "        os.makedirs('models')\n",
    "    if not os.path.isdir('models/%s' % model_name):\n",
    "        os.makedirs('models/%s' % model_name)\n",
    "    torch.save(G.state_dict(), \"models/%s/G_%d.pth\" % (model_name, epoch))\n",
    "    torch.save(D.state_dict(), \"models/%s/D_%d.pth\" % (model_name, epoch))\n",
    "    torch.save(Sampler.state_dict(), \"models/%s/Sampler_%d.pth\" % (model_name, epoch))\n",
    "    np.save('models/%s/losses_%d.npy' % (model_name, epoch), np.array([D_losses, GAN_losses, NCE_losses, G_total_losses]))\n",
    "    try:\n",
    "        os.remove('models/%s/G_%d.pth' % (model_name, epoch - 1))\n",
    "        os.remove('models/%s/D_%d.pth' % (model_name, epoch - 1))\n",
    "        os.remove('models/%s/Sampler_%d.pth' % (model_name, epoch - 1))\n",
    "        os.remove('models/%s/losses_%d.npy' % (model_name, epoch - 1))\n",
    "    except:\n",
    "        pass\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imgs = next(iter(val_dataloader))\n",
    "real_A = Variable(imgs[\"a\"].type(Tensor))\n",
    "\n",
    "values = [1, 3, 5, 7, 9, 11]\n",
    "dir_name = 'G2M-frames-res'\n",
    "total_e = 99\n",
    "output_dir = 'G2M-resblocks'\n",
    "\n",
    "for n in values:\n",
    "    G_compare = Generator(input_size, res_blocks, kernel_size).to(device)\n",
    "    G_compare.load_state_dict(torch.load('models/%s%d/G_%d.pth' % (dir_name, n, total_e)))\n",
    "    G_compare.eval()\n",
    "    \n",
    "    fake_B = G_compare(real_A)\n",
    "    # Arange images along x-axis\n",
    "    real_A = make_grid(real_A, nrow=5, normalize=True)\n",
    "    fake_B = make_grid(fake_B, nrow=5, normalize=True)\n",
    "    # Arange images along y-axis\n",
    "    image_grid = torch.cat((real_A, fake_B), 1)\n",
    "    if not os.path.isdir('comparisons/%s' % output_dir):\n",
    "        os.makedirs('comparisons/%s' % output_dir)\n",
    "    save_image(image_grid, \"comparisons/%s/%d.png\" % (output_dir, n), normalize=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
