{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: numpy in c:\\users\\andre\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (1.19.3)\n",
      "Requirement already satisfied: opencv-python in c:\\users\\andre\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (4.4.0.46)\n",
      "Requirement already satisfied: dlib in c:\\users\\andre\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (19.21.1)\n",
      "Requirement already satisfied: matplotlib in c:\\users\\andre\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (3.3.3)\n",
      "Requirement already satisfied: boto3 in c:\\users\\andre\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (1.17.4)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\andre\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from matplotlib) (0.10.0)\n",
      "Requirement already satisfied: python-dateutil>=2.1 in c:\\users\\andre\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from matplotlib) (2.8.1)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in c:\\users\\andre\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from matplotlib) (1.3.1)\n",
      "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.3 in c:\\users\\andre\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from matplotlib) (2.4.7)\n",
      "Requirement already satisfied: pillow>=6.2.0 in c:\\users\\andre\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from matplotlib) (8.0.1)\n",
      "Requirement already satisfied: s3transfer<0.4.0,>=0.3.0 in c:\\users\\andre\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from boto3) (0.3.4)\n",
      "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in c:\\users\\andre\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from boto3) (0.10.0)\n",
      "Requirement already satisfied: botocore<1.21.0,>=1.20.4 in c:\\users\\andre\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from boto3) (1.20.4)\n",
      "Requirement already satisfied: six in c:\\users\\andre\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from cycler>=0.10->matplotlib) (1.15.0)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.25.4 in c:\\users\\andre\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from botocore<1.21.0,>=1.20.4->boto3) (1.26.2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 20.2.1; however, version 21.0.1 is available.\n",
      "You should consider upgrading via the 'c:\\users\\andre\\appdata\\local\\programs\\python\\python38\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    }
   ],
   "source": [
    "!pip install numpy opencv-python dlib matplotlib boto3\n",
    "try:\n",
    "    import torch\n",
    "except:\n",
    "    !pip install pip install torch==1.7.1+cu101 torchvision==0.8.2+cu101 torchaudio===0.7.2 -f https://download.pytorch.org/whl/torch_stable.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import numpy as np\n",
    "import cv2\n",
    "import os\n",
    "import glob\n",
    "import random\n",
    "import time, datetime, sys\n",
    "import zipfile as zf\n",
    "import boto3\n",
    "from PIL import Image\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch.autograd import Variable\n",
    "from torchvision import transforms, datasets\n",
    "from torchvision.utils import save_image, make_grid\n",
    "\n",
    "import dlib\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython import display\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the model name - determines where the outputs are saved\n",
    "model_name = 'G2M-frames-conv5'\n",
    "\n",
    "# Variables/Hyperparameters\n",
    "dataset_size = 600\n",
    "generate_dataset = True\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "Tensor = torch.cuda.FloatTensor if torch.cuda.is_available() else torch.Tensor\n",
    "nce_layers = [0, 4, 8, 12, 16]\n",
    "lambda_NCE = 1.0\n",
    "lambda_GAN = 1.0\n",
    "batch_size = 1\n",
    "load_weights = True\n",
    "epoch = 0\n",
    "epochs = 100\n",
    "nonsaturating = False\n",
    "\n",
    "input_size = (3,144,256)\n",
    "res_blocks = 9 # def = 9\n",
    "learning_rate = 0.002\n",
    "kernel_size = 5\n",
    "init_kernel_size = 7\n",
    "\n",
    "D_losses = []\n",
    "GAN_losses = []\n",
    "NCE_losses = []\n",
    "G_total_losses = []\n",
    "\n",
    "# Swap the game and movie datasets, so that translation goes from movie to game instead\n",
    "swap = False\n",
    "\n",
    "# Toggle whether translation is done only on faces\n",
    "faces = False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the original data (and the processed data)\n",
    "s3 = boto3.resource('s3', aws_access_key_id = 'AKIAIOPFTDXA3ZXLK5YA', aws_secret_access_key = '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')\n",
    "bucket = s3.Bucket('vision-dataset-vmrj42')\n",
    "\n",
    "for fname in ['Data.zip', 'datasets.zip']:\n",
    "    if not os.path.isfile(fname):\n",
    "        bucket.download_file(fname, fname)\n",
    "\n",
    "if not os.path.isdir('Data'):\n",
    "    files = zf.ZipFile('Data.zip', 'r')\n",
    "    files.extractall('')\n",
    "if not os.path.isdir('dataset') and not generate_dataset:\n",
    "    files = zf.ZipFile('datasets.zip', 'r')\n",
    "    files.extractall('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a basic dataset for full-frame translation\n",
    "\n",
    "for dname in ['dataset/train/game', \n",
    "              'dataset/train/movie', \n",
    "              'dataset/test/game', \n",
    "              'dataset/test/movie', \n",
    "              'face_dataset/train/game',\n",
    "              'face_dataset/train/movie',\n",
    "              'face_dataset/test/game',\n",
    "              'face_dataset/test/movie']:\n",
    "    if not os.path.isdir(dname):\n",
    "        os.makedirs(dname)\n",
    "\n",
    "if len(os.listdir('dataset/train/game')) < dataset_size:\n",
    "    # get some frames from the game footage\n",
    "    cap = cv2.VideoCapture('Data/game/MafiaVideogame.mp4')\n",
    "    length = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "    frame_count = 0\n",
    "    saved_frames = 0\n",
    "    \n",
    "    face_detector = dlib.get_frontal_face_detector()\n",
    "    faces = []\n",
    "\n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        frame_count += 1\n",
    "        # Save the entire frame as part of the dataset, alternating between the training and testing datasets\n",
    "        if frame_count % (length // (2 * dataset_size)) == 0 and ret:\n",
    "            if saved_frames < dataset_size:\n",
    "                fname = 'dataset/train/game/%d.png' % (saved_frames)\n",
    "            else:\n",
    "                fname = 'dataset/test/game/%d.png' % (saved_frames % dataset_size)\n",
    "            cv2.imwrite(fname, frame)\n",
    "            saved_frames += 1\n",
    "        \n",
    "        # Check if there is a face in every (length // (10 * dataset_size)) frame\n",
    "        if frame_count % (length // (6 * dataset_size)) == 0 and ret:\n",
    "            dets = face_detector(frame, 1)\n",
    "            for i, d in enumerate(dets):\n",
    "                left, top, right, bottom = d.left(), d.top(), d.right(), d.bottom()\n",
    "                if right - left > 60:\n",
    "                    face = frame[top:bottom, left:right]\n",
    "                    if len(face) > 0 and len(face[0]) > 0:\n",
    "                        faces.append(face)\n",
    "    cap.release()\n",
    "    \n",
    "    # Alternating between the training and testing datasets, save the extracted faces\n",
    "    saved_faces = 0\n",
    "    for i, face in enumerate(faces):\n",
    "        if i % (len(faces) // (2 * dataset_size)) == 0:\n",
    "            if saved_faces < dataset_size:\n",
    "                fname = 'face_dataset/train/game/%d.png' % (saved_faces)\n",
    "            else:\n",
    "                fname = 'face_dataset/test/game/%d.png' % (saved_faces % dataset_size)\n",
    "            cv2.imwrite(fname, cv2.resize(face, (input_size[1], input_size[1])))\n",
    "            saved_faces += 1\n",
    "\n",
    "    # get some frames from the movie footage\n",
    "    movie_dirs = ['Data/movie/TheGodfather.mp4', 'Data/movie/TheIrishman.mp4', 'Data/movie/TheSopranos.mp4']\n",
    "\n",
    "    saved_frames = 0\n",
    "    for movie in movie_dirs:\n",
    "        cap = cv2.VideoCapture(movie)\n",
    "        length = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "        frame_count = 0\n",
    "\n",
    "        while cap.isOpened():\n",
    "            ret, frame = cap.read()\n",
    "            if not ret:\n",
    "                break\n",
    "            frame_count += 1\n",
    "            if frame_count % (length // (2 * dataset_size / len(movie_dirs))) == 0 and ret:\n",
    "                if saved_frames < dataset_size:\n",
    "                    fname = 'dataset/train/movie/%d.png' % (saved_frames)\n",
    "                else:\n",
    "                    fname = 'dataset/test/movie/%d.png' % (saved_frames % dataset_size)\n",
    "                cv2.imwrite(fname, frame)\n",
    "                saved_frames += 1\n",
    "\n",
    "\n",
    "        cap.release()\n",
    "        \n",
    "    face_detector = dlib.get_frontal_face_detector()        \n",
    "    faces_dir = 'Data/faces/'\n",
    "    real_faces = os.listdir(faces_dir)\n",
    "    saved_faces = 0\n",
    "    current_face = 0\n",
    "    while saved_faces < dataset_size * 4:\n",
    "        real_face = cv2.imread(faces_dir + real_faces[current_face])\n",
    "        face = []\n",
    "        dets = face_detector(real_face, 1)\n",
    "        for d in dets:\n",
    "            left, top, right, bottom = d.left(), d.top(), d.right(), d.bottom()\n",
    "            if right - left > 60:\n",
    "                face = real_face[top : bottom, left : right]\n",
    "                if len(face) > 0 and len(face[0]) > 0:\n",
    "                    face = cv2.resize(face, (input_size[1], input_size[1]))\n",
    "                    if saved_faces < dataset_size * 2:\n",
    "                        fname = 'face_dataset/train/movie/%d.png' % (saved_faces)\n",
    "                    else:\n",
    "                        fname = 'face_dataset/test/movie/%d.png' % (saved_faces % (dataset_size * 2))\n",
    "                    cv2.imwrite(fname, face)\n",
    "                    saved_faces += 1\n",
    "                    break\n",
    "        current_face += 1\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helpful class for loading both game and movie samples as one dataset\n",
    "class ImageDataset(Dataset):\n",
    "    def __init__(self, root, swap = False, transforms_ = None, unaligned = False, mode = \"train\"):\n",
    "        self.transform = transforms.Compose(transforms_)\n",
    "        self.unaligned = unaligned\n",
    "        if swap:\n",
    "            self.files_game = sorted(glob.glob(os.path.join(root, \"%s/movie\" % mode) + \"/*.*\"))\n",
    "            self.files_movie = sorted(glob.glob(os.path.join(root, \"%s/game\" % mode) + \"/*.*\"))\n",
    "        else:\n",
    "            self.files_game = sorted(glob.glob(os.path.join(root, \"%s/game\" % mode) + \"/*.*\"))\n",
    "            self.files_movie = sorted(glob.glob(os.path.join(root, \"%s/movie\" % mode) + \"/*.*\"))\n",
    "        print(len(os.listdir(os.path.join(root, 'train/movie'))))\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        image_game = Image.open(self.files_game[index % len(self.files_game)])\n",
    "\n",
    "        if self.unaligned:\n",
    "            image_movie = Image.open(self.files_movie[random.randint(0, len(self.files_movie) - 1)])\n",
    "        else:\n",
    "            image_movie = Image.open(self.files_movie[index % len(self.files_movie)])\n",
    "\n",
    "        # Convert grayscale images to rgb\n",
    "        if image_game.mode != \"RGB\":\n",
    "            image_game = to_rgb(image_game)\n",
    "        if image_movie.mode != \"RGB\":\n",
    "            image_movie = to_rgb(image_movie)\n",
    "\n",
    "        item_game = self.transform(image_game)\n",
    "        item_movie = self.transform(image_movie)\n",
    "        \n",
    "        return {\"a\": item_game, \"b\": item_movie}\n",
    "\n",
    "    def __len__(self):\n",
    "        return max(len(self.files_game), len(self.files_movie))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "600\n",
      "600\n"
     ]
    }
   ],
   "source": [
    "# Loading the dataset\n",
    "# Define the dataset-wide transformations\n",
    "\n",
    "transforms_ = [\n",
    "    transforms.Resize(int(input_size[1] * 1.4), Image.BICUBIC),\n",
    "    transforms.RandomCrop((input_size[1],input_size[2])),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
    "]\n",
    "\n",
    "\n",
    "if faces:\n",
    "    dataset_dir = 'face_dataset'\n",
    "else:\n",
    "    dataset_dir = 'dataset'\n",
    "# Training data loader\n",
    "dataloader = DataLoader(\n",
    "    ImageDataset(dataset_dir, swap = swap, transforms_ = transforms_, unaligned = True),\n",
    "    batch_size = batch_size,\n",
    "    shuffle = True,\n",
    "    num_workers = 0,\n",
    ")\n",
    "# Test data loader\n",
    "val_dataloader = DataLoader(\n",
    "    ImageDataset(dataset_dir, swap = swap, transforms_ = transforms_, unaligned = True, mode = 'test'),\n",
    "    batch_size = 5,\n",
    "    shuffle = True,\n",
    "    num_workers = 0,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Other Utils\n",
    "\n",
    "inv_normalize = transforms.Normalize(\n",
    "    mean=[-0.5/0.5, -0.5/0.5, -0.5/0.5],\n",
    "    std=[1/0.5, 1/0.5, 1/0.5]\n",
    ")\n",
    "\n",
    "def weights_init_normal(m):\n",
    "    classname = m.__class__.__name__\n",
    "    if classname.find(\"Conv\") != -1:\n",
    "        torch.nn.init.normal_(m.weight.data, 0.0, 0.02)\n",
    "        if hasattr(m, \"bias\") and m.bias is not None:\n",
    "            torch.nn.init.constant_(m.bias.data, 0.0)\n",
    "    elif classname.find(\"BatchNorm2d\") != -1:\n",
    "        torch.nn.init.normal_(m.weight.data, 1.0, 0.02)\n",
    "        torch.nn.init.constant_(m.bias.data, 0.0)\n",
    "        \n",
    "def sample_images(batches_done):\n",
    "    \"\"\"Saves a generated sample from the test set\"\"\"\n",
    "    imgs = next(iter(val_dataloader))\n",
    "    G.eval()\n",
    "    real_A = Variable(imgs[\"a\"].type(Tensor))\n",
    "    fake_B = G(real_A)\n",
    "    for img in fake_B:\n",
    "        img = inv_normalize(img)\n",
    "    # Arange images along x-axis\n",
    "    real_A = make_grid(real_A, nrow=5, normalize=True)\n",
    "    fake_B = make_grid(fake_B, nrow=5, normalize=True)\n",
    "    # Arange images along y-axis\n",
    "    image_grid = torch.cat((real_A, fake_B), 1)\n",
    "    if not os.path.isdir('models/%s/samples' % model_name):\n",
    "        os.makedirs('models/%s/samples' % model_name)\n",
    "    save_image(image_grid, \"models/%s/samples/%s.png\" % (model_name, batches_done), normalize=False)\n",
    "    \n",
    "class Normalize(nn.Module):\n",
    "    def __init__(self, power=2):\n",
    "        super(Normalize, self).__init__()\n",
    "        self.power = power\n",
    "\n",
    "    def forward(self, x):\n",
    "        norm = x.pow(self.power).sum(1, keepdim=True).pow(1. / self.power)\n",
    "        out = x.div(norm + 1e-7)\n",
    "        return out\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generator and Discriminator models adapted from https://github.com/eriklindernoren/PyTorch-GAN\n",
    "# PatchSampleF adapted from https://github.com/taesungp/contrastive-unpaired-translation\n",
    "\n",
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, in_features, kernel):\n",
    "        super(ResidualBlock, self).__init__()\n",
    "\n",
    "        self.block = nn.Sequential(\n",
    "            nn.ReflectionPad2d(kernel // 2),\n",
    "            nn.Conv2d(in_features, in_features, kernel),\n",
    "            nn.InstanceNorm2d(in_features),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.ReflectionPad2d(kernel // 2),\n",
    "            nn.Conv2d(in_features, in_features, kernel),\n",
    "            nn.InstanceNorm2d(in_features),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x + self.block(x)\n",
    "\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self, input_shape, num_residual_blocks, kernel, init_kernel):\n",
    "        super(Generator, self).__init__()\n",
    "        channels = input_shape[0]\n",
    "\n",
    "        # Initial convolution block\n",
    "        out_features = 64\n",
    "        model = [\n",
    "            nn.ReflectionPad2d(init_kernel // 2),\n",
    "            nn.Conv2d(channels, out_features, init_kernel),\n",
    "            nn.InstanceNorm2d(out_features),\n",
    "            nn.ReLU(inplace=True),\n",
    "        ]\n",
    "        in_features = out_features\n",
    "\n",
    "        # Downsampling\n",
    "        for _ in range(2):\n",
    "            out_features *= 2\n",
    "            model += [\n",
    "                nn.Conv2d(in_features, out_features, kernel, stride=2, padding=kernel//2),\n",
    "                nn.InstanceNorm2d(out_features),\n",
    "                nn.ReLU(inplace=True),\n",
    "            ]\n",
    "            in_features = out_features\n",
    "\n",
    "        # Residual blocks\n",
    "        for _ in range(num_residual_blocks):\n",
    "            model += [ResidualBlock(out_features, kernel)]\n",
    "\n",
    "        # Upsampling\n",
    "        for _ in range(2):\n",
    "            out_features //= 2\n",
    "            model += [\n",
    "                nn.Upsample(scale_factor=2),\n",
    "                nn.Conv2d(in_features, out_features, kernel, stride=1, padding=kernel//2),\n",
    "                nn.InstanceNorm2d(out_features),\n",
    "                nn.ReLU(inplace=True),\n",
    "            ]\n",
    "            in_features = out_features\n",
    "\n",
    "        # Output layer\n",
    "        model += [\n",
    "            nn.ReflectionPad2d(init_kernel // 2), \n",
    "            nn.Conv2d(out_features, channels, init_kernel), \n",
    "            nn.Tanh()\n",
    "        ]\n",
    "\n",
    "        self.model = nn.Sequential(*model)\n",
    "\n",
    "    def forward(self, x, layers = [], encode_only = False):\n",
    "        if -1 in layers:\n",
    "            layers.append(len(self.model))\n",
    "        if len(layers) > 0:\n",
    "            feat = x\n",
    "            feats = []\n",
    "            for layer_id, layer in enumerate(self.model):\n",
    "                feat = layer(feat)\n",
    "                if layer_id in layers:\n",
    "                    feats.append(feat)\n",
    "                else:\n",
    "                    pass\n",
    "                if layer_id == layers[-1] and encode_only:\n",
    "                    return feats\n",
    "            return feat, feats\n",
    "        else:\n",
    "            return self.model(x)\n",
    "    \n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, input_shape):\n",
    "        super(Discriminator, self).__init__()\n",
    "        channels, height, width = input_shape\n",
    "        self.output_shape = (1, height // 2 ** 4, width // 2 ** 4)\n",
    "\n",
    "        def discriminator_block(in_filters, out_filters, normalize = True):\n",
    "            \"\"\"Returns downsampling layers of each discriminator block\"\"\"\n",
    "            layers = [nn.Conv2d(in_filters, out_filters, 4, stride=2, padding=1)]\n",
    "            if normalize:\n",
    "                layers.append(nn.InstanceNorm2d(out_filters))\n",
    "            layers.append(nn.LeakyReLU(0.2, inplace=True))\n",
    "            return layers\n",
    "\n",
    "        self.model = nn.Sequential(\n",
    "            *discriminator_block(channels, 64, normalize=False),\n",
    "            *discriminator_block(64, 128),\n",
    "            *discriminator_block(128, 256),\n",
    "            *discriminator_block(256, 512),\n",
    "            nn.ZeroPad2d((1, 0, 1, 0)),\n",
    "            nn.Conv2d(512, 1, 4, padding=1)\n",
    "        )\n",
    "\n",
    "    def forward(self, img):\n",
    "        return self.model(img)\n",
    "    \n",
    "class PatchSampleF(nn.Module):\n",
    "    def __init__(self, use_mlp = True, init_type = 'normal', init_gain = 0.02, nc = 256):\n",
    "        # potential issues: currently, we use the same patch_ids for multiple images in the batch\n",
    "        super(PatchSampleF, self).__init__()\n",
    "        self.l2norm = Normalize(2)\n",
    "        self.use_mlp = use_mlp\n",
    "        self.nc = nc  # hard-coded\n",
    "        self.mlp_init = False\n",
    "        self.init_type = init_type\n",
    "        self.init_gain = init_gain\n",
    "\n",
    "    def create_mlp(self, feats):\n",
    "        for mlp_id, feat in enumerate(feats):\n",
    "            input_nc = feat.shape[1]\n",
    "            mlp = nn.Sequential(*[nn.Linear(input_nc, self.nc), nn.ReLU(), nn.Linear(self.nc, self.nc)])\n",
    "            if torch.cuda.is_available():\n",
    "                mlp.cuda()\n",
    "            setattr(self, 'mlp_%d' % mlp_id, mlp)\n",
    "        if torch.cuda.is_available():\n",
    "            self.to(device)\n",
    "        self.apply(weights_init_normal)\n",
    "        self.mlp_init = True\n",
    "\n",
    "    def forward(self, feats, num_patches = 64, patch_ids = None):\n",
    "        return_ids = []\n",
    "        return_feats = []\n",
    "        if self.use_mlp and not self.mlp_init:\n",
    "            self.create_mlp(feats)\n",
    "        for feat_id, feat in enumerate(feats):\n",
    "            B, H, W = feat.shape[0], feat.shape[2], feat.shape[3]\n",
    "            feat_reshape = feat.permute(0, 2, 3, 1).flatten(1, 2)\n",
    "            if num_patches > 0:\n",
    "                if patch_ids is not None:\n",
    "                    patch_id = patch_ids[feat_id]\n",
    "                else:\n",
    "                    patch_id = torch.randperm(feat_reshape.shape[1], device=feats[0].device)\n",
    "                    patch_id = patch_id[:int(min(num_patches, patch_id.shape[0]))]  # .to(patch_ids.device)\n",
    "                x_sample = feat_reshape[:, patch_id, :].flatten(0, 1)  # reshape(-1, x.shape[1])\n",
    "            else:\n",
    "                x_sample = feat_reshape\n",
    "                patch_id = []\n",
    "            if self.use_mlp:\n",
    "                mlp = getattr(self, 'mlp_%d' % feat_id)\n",
    "                x_sample = mlp(x_sample)\n",
    "            return_ids.append(patch_id)\n",
    "            x_sample = self.l2norm(x_sample)\n",
    "\n",
    "            if num_patches == 0:\n",
    "                x_sample = x_sample.permute(0, 2, 1).reshape([B, x_sample.shape[-1], H, W])\n",
    "            return_feats.append(x_sample)\n",
    "        return return_feats, return_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Class to create separate losses for each feature, from https://github.com/taesungp/contrastive-unpaired-translation\n",
    "class PatchNCELoss(nn.Module):\n",
    "    def __init__(self, batch_size, nce_T = 0.07):\n",
    "        super().__init__()\n",
    "        self.batch_size = batch_size\n",
    "        self.nce_T = nce_T\n",
    "        self.cross_entropy_loss = torch.nn.CrossEntropyLoss(reduction='none')\n",
    "        self.mask_dtype = torch.bool\n",
    "\n",
    "    def forward(self, feat_q, feat_k):\n",
    "        batchSize = feat_q.shape[0]\n",
    "        dim = feat_q.shape[1]\n",
    "        feat_k = feat_k.detach()\n",
    "\n",
    "        # pos logit\n",
    "        l_pos = torch.bmm(feat_q.view(batchSize, 1, -1), feat_k.view(batchSize, -1, 1))\n",
    "        l_pos = l_pos.view(batchSize, 1)\n",
    "\n",
    "        batch_dim_for_bmm = self.batch_size\n",
    "\n",
    "        # reshape features to batch size\n",
    "        feat_q = feat_q.view(batch_dim_for_bmm, -1, dim)\n",
    "        feat_k = feat_k.view(batch_dim_for_bmm, -1, dim)\n",
    "        npatches = feat_q.size(1)\n",
    "        l_neg_curbatch = torch.bmm(feat_q, feat_k.transpose(2, 1))\n",
    "\n",
    "        # diagonal entries are similarity between same features, and hence meaningless.\n",
    "        # just fill the diagonal with very small number, which is exp(-10) and almost zero\n",
    "        diagonal = torch.eye(npatches, device=feat_q.device, dtype=self.mask_dtype)[None, :, :]\n",
    "        l_neg_curbatch.masked_fill_(diagonal, -10.0)\n",
    "        l_neg = l_neg_curbatch.view(-1, npatches)\n",
    "\n",
    "        out = torch.cat((l_pos, l_neg), dim=1) / self.nce_T\n",
    "\n",
    "        loss = self.cross_entropy_loss(out, torch.zeros(out.size(0), dtype=torch.long,\n",
    "                                                        device=feat_q.device))\n",
    "\n",
    "        return loss\n",
    "    \n",
    "def nonsaturating_loss(prediction, is_real):\n",
    "    if is_real.mean() == 1:\n",
    "        loss = F.softplus(-prediction).view(prediction.size(0), -1).mean(dim = 1)\n",
    "    else:\n",
    "        loss = F.softplus(prediction).view(prediction.size(0), -1).mean(dim = 1)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the networks, load the most recent saved models, and set the loss functions and optimizers\n",
    "\n",
    "if faces:\n",
    "    input_size = (input_size[0], input_size[1], input_size[1])\n",
    "G = Generator(input_size, res_blocks, kernel_size, init_kernel_size).to(device)\n",
    "D = Discriminator(input_size).to(device)\n",
    "Sampler = PatchSampleF(batch_size).to(device)\n",
    "\n",
    "if nonsaturating:\n",
    "    criterion_GAN = nonsaturating_loss\n",
    "else:\n",
    "    criterion_GAN = torch.nn.MSELoss().to(device)\n",
    "criterion_NCE = []\n",
    "\n",
    "for nce_layer in nce_layers:\n",
    "    criterion_NCE.append(PatchNCELoss(batch_size).to(device))\n",
    "\n",
    "G.apply(weights_init_normal)\n",
    "D.apply(weights_init_normal)\n",
    "\n",
    "optimizer_G = torch.optim.Adam(G.parameters(), lr = learning_rate)\n",
    "optimizer_D = torch.optim.Adam(D.parameters(), lr = learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters for the sampler are not made until the first forward pass through the Sampler network\n",
    "# Hence, we do a 'trial' training pass before setting the optimizer for the Sampler\n",
    "for i, batch in enumerate(dataloader):\n",
    "    x, y = Variable(batch[\"a\"].type(Tensor)), Variable(batch[\"b\"].type(Tensor))\n",
    "    \n",
    "    real = Variable(Tensor(np.ones((x.size(0), *D.output_shape))), requires_grad = False)\n",
    "    fake = Variable(Tensor(np.zeros((x.size(0), *D.output_shape))), requires_grad = False)\n",
    "    \n",
    "    D.eval()\n",
    "    G.eval()\n",
    "    Sampler.eval()\n",
    "    # get the fake loss\n",
    "    fake_y = G(x)\n",
    "    D_fake = D(fake_y.detach())\n",
    "    loss_D_fake = criterion_GAN(D_fake, fake).mean()\n",
    "    # get the real loss\n",
    "    D_real = D(y)\n",
    "    loss_D_real = criterion_GAN(D_real, real).mean()\n",
    "    # combine loss and calculate gradients\n",
    "    loss_D = (loss_D_fake + loss_D_real) * 0.5\n",
    "    loss_D.backward()\n",
    "\n",
    "    # get the fake GAN loss\n",
    "    D_fake = D(fake_y)\n",
    "    loss_G_GAN = lambda_GAN * criterion_GAN(D_fake, real).mean()\n",
    "    total_nce_loss = 0\n",
    "    for fake, real in [(fake_y, x), (y, G(y))]:\n",
    "        # get the NCE loss\n",
    "        feat_q = G(fake, nce_layers, encode_only = True)\n",
    "        feat_k = G(real, nce_layers, encode_only = True)\n",
    "\n",
    "        feat_k_pool, sample_ids = Sampler(feat_k, 256, None)\n",
    "        feat_q_pool, _ = Sampler(feat_q, 256, sample_ids)\n",
    "\n",
    "        total_nce_loss = 0.0\n",
    "        for f_q, f_k, crit, nce_layer in zip(feat_q_pool, feat_k_pool, criterion_NCE, nce_layers):\n",
    "            loss = crit(f_q, f_k) * lambda_NCE\n",
    "            total_nce_loss += loss.mean()\n",
    "\n",
    "        nce_loss = total_nce_loss / len(nce_layers)\n",
    "        total_nce_loss += nce_loss\n",
    "\n",
    "    loss_G = loss_G_GAN + total_nce_loss\n",
    "    loss_G.backward()\n",
    "    \n",
    "    break\n",
    "\n",
    "optimizer_Sampler = torch.optim.Adam(Sampler.parameters(), lr = learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the most recently saved models if available\n",
    "if os.path.isdir('models/%s' % model_name) and load_weights:\n",
    "    # Get the most recent model and load them\n",
    "    epoch = max([int(fname[8:-4]) for fname in os.listdir('models/%s' % model_name) if 'Sampler' in fname])\n",
    "    G.load_state_dict(torch.load('models/%s/G_%d.pth' % (model_name, epoch)))\n",
    "    D.load_state_dict(torch.load('models/%s/D_%d.pth' % (model_name, epoch)))\n",
    "    Sampler.load_state_dict(torch.load('models/%s/Sampler_%d.pth' % (model_name, epoch)))\n",
    "    # Load the losses as well, for plotting\n",
    "    losses = np.load('models/%s/losses_%d.npy' % (model_name, epoch))\n",
    "    D_losses = list(losses[0])\n",
    "    GAN_losses = list(losses[1])\n",
    "    NCE_losses = list(losses[2])\n",
    "    G_total_losses = list(losses[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 15/100] [Batch 508/600] [D loss: 0.269632] [GAN loss: 0.352639, NCE loss: 1.323734, Total: 4.323842] ETA: 7:02:54.90537602840"
     ]
    }
   ],
   "source": [
    "# Training Loop, adapted from https://github.com/taesungp/contrastive-unpaired-translation\n",
    "\n",
    "prev_time = time.time()\n",
    "for epoch in range(epoch, epochs):\n",
    "    total_D_loss = 0\n",
    "    total_GAN_loss = 0\n",
    "    total_NCE_loss = 0\n",
    "    total_G_loss = 0\n",
    "    for i, batch in enumerate(dataloader):\n",
    "        x, y = Variable(batch[\"a\"].type(Tensor)), Variable(batch[\"b\"].type(Tensor))\n",
    "        \n",
    "        # Adversarial ground truths\n",
    "        real = Variable(Tensor(np.ones((x.size(0), *D.output_shape))), requires_grad = False)\n",
    "        fake = Variable(Tensor(np.zeros((x.size(0), *D.output_shape))), requires_grad = False)\n",
    "        \n",
    "        # train discriminator\n",
    "        D.train()\n",
    "        optimizer_D.zero_grad()\n",
    "        # get the fake loss\n",
    "        fake_y = G(x)\n",
    "        D_fake = D(fake_y.detach())\n",
    "        loss_D_fake = criterion_GAN(D_fake, fake).mean()\n",
    "        # get the real loss\n",
    "        D_real = D(y)\n",
    "        loss_D_real = criterion_GAN(D_real, real).mean()\n",
    "        # combine loss and calculate gradients\n",
    "        loss_D = (loss_D_fake + loss_D_real) * 0.5\n",
    "        loss_D.backward()\n",
    "        optimizer_D.step()\n",
    "        \n",
    "        # train generator\n",
    "        G.train()\n",
    "        optimizer_G.zero_grad()\n",
    "        optimizer_Sampler.zero_grad()\n",
    "        # get the fake GAN loss\n",
    "        D_fake = D(fake_y)\n",
    "        loss_G_GAN = criterion_GAN(D_fake, real).mean()\n",
    "        # get the NCE loss\n",
    "        Sampler.train()\n",
    "        total_nce_loss = 0\n",
    "        for fake, real in [(fake_y, x), (y, G(y))]:\n",
    "            feat_q = G(fake_y, nce_layers, encode_only = True)\n",
    "            feat_k = G(x, nce_layers, encode_only = True)\n",
    "\n",
    "            feat_k_pool, sample_ids = Sampler(feat_k, 256, None)\n",
    "            feat_q_pool, _ = Sampler(feat_q, 256, sample_ids)\n",
    "\n",
    "            total_nce_loss = 0.0\n",
    "            for f_q, f_k, crit, nce_layer in zip(feat_q_pool, feat_k_pool, criterion_NCE, nce_layers):\n",
    "                loss = crit(f_q, f_k) * lambda_NCE\n",
    "                total_nce_loss += loss.mean()\n",
    "\n",
    "            nce_loss = total_nce_loss / len(nce_layers)\n",
    "            total_nce_loss += nce_loss\n",
    "        total_nce_loss *= 0.5\n",
    "        \n",
    "        loss_G = loss_G_GAN + total_nce_loss\n",
    "        loss_G.backward()\n",
    "        optimizer_G.step()\n",
    "        optimizer_Sampler.step()\n",
    "        \n",
    "        # --------------\n",
    "        #  Log Progress\n",
    "        # --------------\n",
    "\n",
    "        # Determine approximate time left\n",
    "        batches_done = epoch * len(dataloader) + i\n",
    "        batches_left = epochs * len(dataloader) - batches_done\n",
    "        time_left = datetime.timedelta(seconds=batches_left * (time.time() - prev_time))\n",
    "        prev_time = time.time()\n",
    "\n",
    "        # Print log\n",
    "        sys.stdout.write(\n",
    "            \"\\r[Epoch %d/%d] [Batch %d/%d] [D loss: %f] [GAN loss: %f, NCE loss: %f, Total: %f] ETA: %s\"\n",
    "            % (\n",
    "                epoch,\n",
    "                epochs,\n",
    "                i,\n",
    "                len(dataloader),\n",
    "                loss_D.item(),\n",
    "                loss_G_GAN.item(),\n",
    "                nce_loss.item(),\n",
    "                loss_G.item(),\n",
    "                time_left,\n",
    "            )\n",
    "        )\n",
    "        \n",
    "        total_D_loss += loss_D.item()\n",
    "        total_GAN_loss += loss_G_GAN.item()\n",
    "        total_NCE_loss += nce_loss.item()\n",
    "        total_G_loss += loss_G.item()\n",
    "        \n",
    "        # If at sample interval save image\n",
    "        if batches_done % 300 == 0:\n",
    "            sample_images(batches_done)\n",
    "            \n",
    "    D_losses.append(total_D_loss / len(dataloader))\n",
    "    GAN_losses.append(total_GAN_loss / len(dataloader))\n",
    "    NCE_losses.append(total_NCE_loss / len(dataloader))\n",
    "    G_total_losses.append(total_G_loss / len(dataloader))\n",
    "    display.clear_output(wait = True)\n",
    "    time.sleep(1)\n",
    "    plt.clf()\n",
    "    plt.ylabel('Mean Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.plot(D_losses, color = 'r')\n",
    "    plt.plot(GAN_losses, color = 'g')\n",
    "    plt.plot(NCE_losses, color = 'b')\n",
    "    plt.savefig('models/%s/loss_plot.png' % model_name)\n",
    "    display.display(plt.gcf())\n",
    "    \n",
    "    if not os.path.isdir('models'):\n",
    "        os.makedirs('models')\n",
    "    if not os.path.isdir('models/%s' % model_name):\n",
    "        os.makedirs('models/%s' % model_name)\n",
    "    torch.save(G.state_dict(), \"models/%s/G_%d.pth\" % (model_name, epoch))\n",
    "    torch.save(D.state_dict(), \"models/%s/D_%d.pth\" % (model_name, epoch))\n",
    "    torch.save(Sampler.state_dict(), \"models/%s/Sampler_%d.pth\" % (model_name, epoch))\n",
    "    np.save('models/%s/losses_%d.npy' % (model_name, epoch), np.array([D_losses, GAN_losses, NCE_losses, G_total_losses]))\n",
    "    try:\n",
    "        os.remove('models/%s/G_%d.pth' % (model_name, epoch - 1))\n",
    "        os.remove('models/%s/D_%d.pth' % (model_name, epoch - 1))\n",
    "        os.remove('models/%s/Sampler_%d.pth' % (model_name, epoch - 1))\n",
    "        os.remove('models/%s/losses_%d.npy' % (model_name, epoch - 1))\n",
    "    except:\n",
    "        pass\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imgs = next(iter(val_dataloader))\n",
    "real_A = Variable(imgs[\"a\"].type(Tensor)).detach()\n",
    "\n",
    "values = ['init-conv5', 'lr2e-3']\n",
    "dir_name = 'G2M-frames-'\n",
    "total_e = 99\n",
    "output_dir = 'G2M-init-conv'\n",
    "\n",
    "for n in values:\n",
    "    G_compare = Generator(input_size, 9, kernel_size).to(device)\n",
    "    G_compare.load_state_dict(torch.load('models/%s%s/G_%d.pth' % (dir_name, n, total_e)))\n",
    "    G_compare.eval()\n",
    "    \n",
    "    fake_B = G_compare(real_A.detach())\n",
    "    # Arange images along x-axis\n",
    "    real_A_grid = make_grid(real_A, nrow=5, normalize=True)\n",
    "    fake_B = make_grid(fake_B, nrow=5, normalize=True)\n",
    "    # Arange images along y-axis\n",
    "    image_grid = torch.cat((real_A_grid, fake_B), 1)\n",
    "    if not os.path.isdir('comparisons/%s' % output_dir):\n",
    "        os.makedirs('comparisons/%s' % output_dir)\n",
    "    save_image(image_grid, \"comparisons/%s/%s.png\" % (output_dir, n), normalize=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
