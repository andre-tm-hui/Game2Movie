{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: numpy in c:\\users\\andre\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (1.19.3)\n",
      "Requirement already satisfied: opencv-python in c:\\users\\andre\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (4.4.0.46)\n",
      "Requirement already satisfied: dlib in c:\\users\\andre\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (19.21.1)\n",
      "Requirement already satisfied: matplotlib in c:\\users\\andre\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (3.3.3)\n",
      "Requirement already satisfied: boto3 in c:\\users\\andre\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (1.17.4)\n",
      "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.3 in c:\\users\\andre\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from matplotlib) (2.4.7)\n",
      "Requirement already satisfied: python-dateutil>=2.1 in c:\\users\\andre\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from matplotlib) (2.8.1)\n",
      "Requirement already satisfied: pillow>=6.2.0 in c:\\users\\andre\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from matplotlib) (8.0.1)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\andre\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from matplotlib) (0.10.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in c:\\users\\andre\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from matplotlib) (1.3.1)\n",
      "Requirement already satisfied: botocore<1.21.0,>=1.20.4 in c:\\users\\andre\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from boto3) (1.20.4)\n",
      "Requirement already satisfied: s3transfer<0.4.0,>=0.3.0 in c:\\users\\andre\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from boto3) (0.3.4)\n",
      "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in c:\\users\\andre\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from boto3) (0.10.0)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\andre\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from python-dateutil>=2.1->matplotlib) (1.15.0)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.25.4 in c:\\users\\andre\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from botocore<1.21.0,>=1.20.4->boto3) (1.26.2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 20.2.1; however, version 21.0.1 is available.\n",
      "You should consider upgrading via the 'c:\\users\\andre\\appdata\\local\\programs\\python\\python38\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    }
   ],
   "source": [
    "!pip install numpy opencv-python dlib matplotlib boto3\n",
    "try:\n",
    "    import torch\n",
    "except:\n",
    "    !pip install pip install torch==1.7.1+cu101 torchvision==0.8.2+cu101 torchaudio===0.7.2 -f https://download.pytorch.org/whl/torch_stable.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import numpy as np\n",
    "import cv2\n",
    "import os\n",
    "import glob\n",
    "import random\n",
    "import time, datetime, sys\n",
    "import zipfile as zf\n",
    "import boto3\n",
    "from PIL import Image\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch.autograd import Variable\n",
    "from torchvision import transforms, datasets\n",
    "from torchvision.utils import save_image, make_grid\n",
    "\n",
    "import dlib\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython import display\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the model name - determines where the outputs are saved\n",
    "model_name = 'G2M-frames-res1'\n",
    "\n",
    "# Variables/Hyperparameters\n",
    "dataset_size = 600\n",
    "generate_dataset = True\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "Tensor = torch.cuda.FloatTensor if torch.cuda.is_available() else torch.Tensor\n",
    "nce_layers = [0, 4, 8, 12, 16]\n",
    "lambda_NCE = 1.0\n",
    "lambda_GAN = 1.0\n",
    "batch_size = 1\n",
    "load_weights = True\n",
    "epoch = 0\n",
    "epochs = 100\n",
    "nonsaturating = False\n",
    "\n",
    "input_size = (3,144,256)\n",
    "res_blocks = 1 # def = 9\n",
    "learning_rate = 0.001\n",
    "betas = (0.9, 0.999)\n",
    "kernel_size = 3\n",
    "\n",
    "D_losses = []\n",
    "GAN_losses = []\n",
    "NCE_losses = []\n",
    "G_total_losses = []\n",
    "\n",
    "# Swap the game and movie datasets, so that translation goes from movie to game instead\n",
    "swap = False\n",
    "\n",
    "# Toggle whether translation is done only on faces\n",
    "faces = False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the original data (and the processed data)\n",
    "s3 = boto3.resource('s3', aws_access_key_id = 'AKIAIOPFTDXA3ZXLK5YA', aws_secret_access_key = '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')\n",
    "bucket = s3.Bucket('vision-dataset-vmrj42')\n",
    "\n",
    "for fname in ['Data.zip', 'datasets.zip']:\n",
    "    if not os.path.isfile(fname):\n",
    "        bucket.download_file(fname, fname)\n",
    "\n",
    "if not os.path.isdir('Data'):\n",
    "    files = zf.ZipFile('Data.zip', 'r')\n",
    "    files.extractall('')\n",
    "if not os.path.isdir('dataset') and not generate_dataset:\n",
    "    files = zf.ZipFile('datasets.zip', 'r')\n",
    "    files.extractall('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a basic dataset for full-frame translation\n",
    "\n",
    "for dname in ['dataset/train/game', \n",
    "              'dataset/train/movie', \n",
    "              'dataset/test/game', \n",
    "              'dataset/test/movie', \n",
    "              'face_dataset/train/game',\n",
    "              'face_dataset/train/movie',\n",
    "              'face_dataset/test/game',\n",
    "              'face_dataset/test/movie']:\n",
    "    if not os.path.isdir(dname):\n",
    "        os.makedirs(dname)\n",
    "\n",
    "if len(os.listdir('dataset/train/game')) < dataset_size:\n",
    "    # get some frames from the game footage\n",
    "    cap = cv2.VideoCapture('Data/game/MafiaVideogame.mp4')\n",
    "    length = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "    frame_count = 0\n",
    "    saved_frames = 0\n",
    "    \n",
    "    face_detector = dlib.get_frontal_face_detector()\n",
    "    faces = []\n",
    "\n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        frame_count += 1\n",
    "        # Save the entire frame as part of the dataset, alternating between the training and testing datasets\n",
    "        if frame_count % (length // (2 * dataset_size)) == 0 and ret:\n",
    "            if saved_frames < dataset_size:\n",
    "                fname = 'dataset/train/game/%d.png' % (saved_frames)\n",
    "            else:\n",
    "                fname = 'dataset/test/game/%d.png' % (saved_frames % dataset_size)\n",
    "            cv2.imwrite(fname, frame)\n",
    "            saved_frames += 1\n",
    "        \n",
    "        # Check if there is a face in every (length // (10 * dataset_size)) frame\n",
    "        if frame_count % (length // (6 * dataset_size)) == 0 and ret:\n",
    "            dets = face_detector(frame, 1)\n",
    "            for i, d in enumerate(dets):\n",
    "                left, top, right, bottom = d.left(), d.top(), d.right(), d.bottom()\n",
    "                if right - left > 60:\n",
    "                    face = frame[top:bottom, left:right]\n",
    "                    if len(face) > 0 and len(face[0]) > 0:\n",
    "                        faces.append(face)\n",
    "    cap.release()\n",
    "    \n",
    "    # Alternating between the training and testing datasets, save the extracted faces\n",
    "    saved_faces = 0\n",
    "    for i, face in enumerate(faces):\n",
    "        if i % (len(faces) // (2 * dataset_size)) == 0:\n",
    "            if saved_faces < dataset_size:\n",
    "                fname = 'face_dataset/train/game/%d.png' % (saved_faces)\n",
    "            else:\n",
    "                fname = 'face_dataset/test/game/%d.png' % (saved_faces % dataset_size)\n",
    "            cv2.imwrite(fname, cv2.resize(face, (input_size[1], input_size[1])))\n",
    "            saved_faces += 1\n",
    "\n",
    "    # get some frames from the movie footage\n",
    "    movie_dirs = ['Data/movie/TheGodfather.mp4', 'Data/movie/TheIrishman.mp4', 'Data/movie/TheSopranos.mp4']\n",
    "\n",
    "    saved_frames = 0\n",
    "    for movie in movie_dirs:\n",
    "        cap = cv2.VideoCapture(movie)\n",
    "        length = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "        frame_count = 0\n",
    "\n",
    "        while cap.isOpened():\n",
    "            ret, frame = cap.read()\n",
    "            if not ret:\n",
    "                break\n",
    "            frame_count += 1\n",
    "            if frame_count % (length // (2 * dataset_size / len(movie_dirs))) == 0 and ret:\n",
    "                if saved_frames < dataset_size:\n",
    "                    fname = 'dataset/train/movie/%d.png' % (saved_frames)\n",
    "                else:\n",
    "                    fname = 'dataset/test/movie/%d.png' % (saved_frames % dataset_size)\n",
    "                cv2.imwrite(fname, frame)\n",
    "                saved_frames += 1\n",
    "\n",
    "\n",
    "        cap.release()\n",
    "        \n",
    "    face_detector = dlib.get_frontal_face_detector()        \n",
    "    faces_dir = 'Data/faces/'\n",
    "    real_faces = os.listdir(faces_dir)\n",
    "    saved_faces = 0\n",
    "    current_face = 0\n",
    "    while saved_faces < dataset_size * 4:\n",
    "        real_face = cv2.imread(faces_dir + real_faces[current_face])\n",
    "        face = []\n",
    "        dets = face_detector(real_face, 1)\n",
    "        for d in dets:\n",
    "            left, top, right, bottom = d.left(), d.top(), d.right(), d.bottom()\n",
    "            if right - left > 60:\n",
    "                face = real_face[top : bottom, left : right]\n",
    "                if len(face) > 0 and len(face[0]) > 0:\n",
    "                    face = cv2.resize(face, (input_size[1], input_size[1]))\n",
    "                    if saved_faces < dataset_size * 2:\n",
    "                        fname = 'face_dataset/train/movie/%d.png' % (saved_faces)\n",
    "                    else:\n",
    "                        fname = 'face_dataset/test/movie/%d.png' % (saved_faces % (dataset_size * 2))\n",
    "                    cv2.imwrite(fname, face)\n",
    "                    saved_faces += 1\n",
    "                    break\n",
    "        current_face += 1\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helpful class for loading both game and movie samples as one dataset\n",
    "class ImageDataset(Dataset):\n",
    "    def __init__(self, root, swap = False, transforms_ = None, unaligned = False, mode = \"train\"):\n",
    "        self.transform = transforms.Compose(transforms_)\n",
    "        self.unaligned = unaligned\n",
    "        if swap:\n",
    "            self.files_game = sorted(glob.glob(os.path.join(root, \"%s/movie\" % mode) + \"/*.*\"))\n",
    "            self.files_movie = sorted(glob.glob(os.path.join(root, \"%s/game\" % mode) + \"/*.*\"))\n",
    "        else:\n",
    "            self.files_game = sorted(glob.glob(os.path.join(root, \"%s/game\" % mode) + \"/*.*\"))\n",
    "            self.files_movie = sorted(glob.glob(os.path.join(root, \"%s/movie\" % mode) + \"/*.*\"))\n",
    "        print(len(os.listdir(os.path.join(root, 'train/movie'))))\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        image_game = Image.open(self.files_game[index % len(self.files_game)])\n",
    "\n",
    "        if self.unaligned:\n",
    "            image_movie = Image.open(self.files_movie[random.randint(0, len(self.files_movie) - 1)])\n",
    "        else:\n",
    "            image_movie = Image.open(self.files_movie[index % len(self.files_movie)])\n",
    "\n",
    "        # Convert grayscale images to rgb\n",
    "        if image_game.mode != \"RGB\":\n",
    "            image_game = to_rgb(image_game)\n",
    "        if image_movie.mode != \"RGB\":\n",
    "            image_movie = to_rgb(image_movie)\n",
    "\n",
    "        item_game = self.transform(image_game)\n",
    "        item_movie = self.transform(image_movie)\n",
    "        \n",
    "        return {\"a\": item_game, \"b\": item_movie}\n",
    "\n",
    "    def __len__(self):\n",
    "        return max(len(self.files_game), len(self.files_movie))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "600\n",
      "600\n"
     ]
    }
   ],
   "source": [
    "# Loading the dataset\n",
    "# Define the dataset-wide transformations\n",
    "\n",
    "transforms_ = [\n",
    "    transforms.Resize(int(input_size[1] * 1.4), Image.BICUBIC),\n",
    "    transforms.RandomCrop((input_size[1],input_size[2])),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
    "]\n",
    "\n",
    "\n",
    "if faces:\n",
    "    dataset_dir = 'face_dataset'\n",
    "else:\n",
    "    dataset_dir = 'dataset'\n",
    "# Training data loader\n",
    "dataloader = DataLoader(\n",
    "    ImageDataset(dataset_dir, swap = swap, transforms_ = transforms_, unaligned = True),\n",
    "    batch_size = batch_size,\n",
    "    shuffle = True,\n",
    "    num_workers = 0,\n",
    ")\n",
    "# Test data loader\n",
    "val_dataloader = DataLoader(\n",
    "    ImageDataset(dataset_dir, swap = swap, transforms_ = transforms_, unaligned = True, mode = 'test'),\n",
    "    batch_size = 5,\n",
    "    shuffle = True,\n",
    "    num_workers = 0,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Other Utils\n",
    "\n",
    "def weights_init_normal(m):\n",
    "    classname = m.__class__.__name__\n",
    "    if classname.find(\"Conv\") != -1:\n",
    "        torch.nn.init.normal_(m.weight.data, 0.0, 0.02)\n",
    "        if hasattr(m, \"bias\") and m.bias is not None:\n",
    "            torch.nn.init.constant_(m.bias.data, 0.0)\n",
    "    elif classname.find(\"BatchNorm2d\") != -1:\n",
    "        torch.nn.init.normal_(m.weight.data, 1.0, 0.02)\n",
    "        torch.nn.init.constant_(m.bias.data, 0.0)\n",
    "        \n",
    "def sample_images(batches_done):\n",
    "    inv_normalize = transforms.Normalize(\n",
    "        mean=[-0.5/0.5, -0.5/0.5, -0.5/0.5],\n",
    "        std=[1/0.5, 1/0.5, 1/0.5]\n",
    "    )\n",
    "    \"\"\"Saves a generated sample from the test set\"\"\"\n",
    "    imgs = next(iter(val_dataloader))\n",
    "    G.eval()\n",
    "    real_A = Variable(imgs[\"a\"].type(Tensor))\n",
    "    fake_B = inv_normalize(G(real_A))\n",
    "    # Arange images along x-axis\n",
    "    real_A = make_grid(real_A, nrow=5, normalize=True)\n",
    "    fake_B = make_grid(fake_B, nrow=5, normalize=True)\n",
    "    # Arange images along y-axis\n",
    "    image_grid = torch.cat((real_A, fake_B), 1)\n",
    "    if not os.path.isdir('models/%s/samples' % model_name):\n",
    "        os.makedirs('models/%s/samples' % model_name)\n",
    "    save_image(image_grid, \"models/%s/samples/%s.png\" % (model_name, batches_done), normalize=False)\n",
    "    \n",
    "class Normalize(nn.Module):\n",
    "    def __init__(self, power=2):\n",
    "        super(Normalize, self).__init__()\n",
    "        self.power = power\n",
    "\n",
    "    def forward(self, x):\n",
    "        norm = x.pow(self.power).sum(1, keepdim=True).pow(1. / self.power)\n",
    "        out = x.div(norm + 1e-7)\n",
    "        return out\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generator and Discriminator models adapted from https://github.com/eriklindernoren/PyTorch-GAN\n",
    "# PatchSampleF adapted from https://github.com/taesungp/contrastive-unpaired-translation\n",
    "\n",
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, in_features):\n",
    "        super(ResidualBlock, self).__init__()\n",
    "\n",
    "        self.block = nn.Sequential(\n",
    "            nn.ReflectionPad2d(1),\n",
    "            nn.Conv2d(in_features, in_features, 3),\n",
    "            nn.InstanceNorm2d(in_features),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.ReflectionPad2d(1),\n",
    "            nn.Conv2d(in_features, in_features, 3),\n",
    "            nn.InstanceNorm2d(in_features),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x + self.block(x)\n",
    "\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self, input_shape, num_residual_blocks, kernel):\n",
    "        super(Generator, self).__init__()\n",
    "        channels = input_shape[0]\n",
    "\n",
    "        # Initial convolution block\n",
    "        out_features = 64\n",
    "        model = [\n",
    "            nn.ReflectionPad2d(channels),\n",
    "            nn.Conv2d(channels, out_features, 2 * kernel + 1),\n",
    "            nn.InstanceNorm2d(out_features),\n",
    "            nn.ReLU(inplace=True),\n",
    "        ]\n",
    "        in_features = out_features\n",
    "\n",
    "        # Downsampling\n",
    "        for _ in range(2):\n",
    "            out_features *= 2\n",
    "            model += [\n",
    "                nn.Conv2d(in_features, out_features, kernel, stride=2, padding=1),\n",
    "                nn.InstanceNorm2d(out_features),\n",
    "                nn.ReLU(inplace=True),\n",
    "            ]\n",
    "            in_features = out_features\n",
    "\n",
    "        # Residual blocks\n",
    "        for _ in range(num_residual_blocks):\n",
    "            model += [ResidualBlock(out_features)]\n",
    "\n",
    "        # Upsampling\n",
    "        for _ in range(2):\n",
    "            out_features //= 2\n",
    "            model += [\n",
    "                nn.Upsample(scale_factor=2),\n",
    "                nn.Conv2d(in_features, out_features, kernel, stride=1, padding=1),\n",
    "                nn.InstanceNorm2d(out_features),\n",
    "                nn.ReLU(inplace=True),\n",
    "            ]\n",
    "            in_features = out_features\n",
    "\n",
    "        # Output layer\n",
    "        model += [\n",
    "            nn.ReflectionPad2d(channels), \n",
    "            nn.Conv2d(out_features, channels, 2 * kernel + 1), \n",
    "            nn.Tanh()\n",
    "        ]\n",
    "\n",
    "        self.model = nn.Sequential(*model)\n",
    "\n",
    "    def forward(self, x, layers = [], encode_only = False):\n",
    "        if -1 in layers:\n",
    "            layers.append(len(self.model))\n",
    "        if len(layers) > 0:\n",
    "            feat = x\n",
    "            feats = []\n",
    "            for layer_id, layer in enumerate(self.model):\n",
    "                feat = layer(feat)\n",
    "                if layer_id in layers:\n",
    "                    feats.append(feat)\n",
    "                else:\n",
    "                    pass\n",
    "                if layer_id == layers[-1] and encode_only:\n",
    "                    return feats\n",
    "            return feat, feats\n",
    "        else:\n",
    "            return self.model(x)\n",
    "    \n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, input_shape, kernel):\n",
    "        super(Discriminator, self).__init__()\n",
    "        channels, height, width = input_shape\n",
    "        self.output_shape = (1, height // 2 ** 4, width // 2 ** 4)\n",
    "\n",
    "        def discriminator_block(in_filters, out_filters, normalize = True):\n",
    "            \"\"\"Returns downsampling layers of each discriminator block\"\"\"\n",
    "            layers = [nn.Conv2d(in_filters, out_filters, 4, stride=2, padding=1)]\n",
    "            if normalize:\n",
    "                layers.append(nn.InstanceNorm2d(out_filters))\n",
    "            layers.append(nn.LeakyReLU(0.2, inplace=True))\n",
    "            return layers\n",
    "\n",
    "        self.model = nn.Sequential(\n",
    "            *discriminator_block(channels, 64, normalize=False),\n",
    "            *discriminator_block(64, 128),\n",
    "            *discriminator_block(128, 256),\n",
    "            *discriminator_block(256, 512),\n",
    "            nn.ZeroPad2d((1, 0, 1, 0)),\n",
    "            nn.Conv2d(512, 1, 4, padding=1)\n",
    "        )\n",
    "\n",
    "    def forward(self, img):\n",
    "        return self.model(img)\n",
    "    \n",
    "class PatchSampleF(nn.Module):\n",
    "    def __init__(self, use_mlp = True, init_type = 'normal', init_gain = 0.02, nc = 256):\n",
    "        # potential issues: currently, we use the same patch_ids for multiple images in the batch\n",
    "        super(PatchSampleF, self).__init__()\n",
    "        self.l2norm = Normalize(2)\n",
    "        self.use_mlp = use_mlp\n",
    "        self.nc = nc  # hard-coded\n",
    "        self.mlp_init = False\n",
    "        self.init_type = init_type\n",
    "        self.init_gain = init_gain\n",
    "\n",
    "    def create_mlp(self, feats):\n",
    "        for mlp_id, feat in enumerate(feats):\n",
    "            input_nc = feat.shape[1]\n",
    "            mlp = nn.Sequential(*[nn.Linear(input_nc, self.nc), nn.ReLU(), nn.Linear(self.nc, self.nc)])\n",
    "            if torch.cuda.is_available():\n",
    "                mlp.cuda()\n",
    "            setattr(self, 'mlp_%d' % mlp_id, mlp)\n",
    "        if torch.cuda.is_available():\n",
    "            self.to(device)\n",
    "        self.apply(weights_init_normal)\n",
    "        self.mlp_init = True\n",
    "\n",
    "    def forward(self, feats, num_patches = 64, patch_ids = None):\n",
    "        return_ids = []\n",
    "        return_feats = []\n",
    "        if self.use_mlp and not self.mlp_init:\n",
    "            self.create_mlp(feats)\n",
    "        for feat_id, feat in enumerate(feats):\n",
    "            B, H, W = feat.shape[0], feat.shape[2], feat.shape[3]\n",
    "            feat_reshape = feat.permute(0, 2, 3, 1).flatten(1, 2)\n",
    "            if num_patches > 0:\n",
    "                if patch_ids is not None:\n",
    "                    patch_id = patch_ids[feat_id]\n",
    "                else:\n",
    "                    patch_id = torch.randperm(feat_reshape.shape[1], device=feats[0].device)\n",
    "                    patch_id = patch_id[:int(min(num_patches, patch_id.shape[0]))]  # .to(patch_ids.device)\n",
    "                x_sample = feat_reshape[:, patch_id, :].flatten(0, 1)  # reshape(-1, x.shape[1])\n",
    "            else:\n",
    "                x_sample = feat_reshape\n",
    "                patch_id = []\n",
    "            if self.use_mlp:\n",
    "                mlp = getattr(self, 'mlp_%d' % feat_id)\n",
    "                x_sample = mlp(x_sample)\n",
    "            return_ids.append(patch_id)\n",
    "            x_sample = self.l2norm(x_sample)\n",
    "\n",
    "            if num_patches == 0:\n",
    "                x_sample = x_sample.permute(0, 2, 1).reshape([B, x_sample.shape[-1], H, W])\n",
    "            return_feats.append(x_sample)\n",
    "        return return_feats, return_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Class to create separate losses for each feature, from https://github.com/taesungp/contrastive-unpaired-translation\n",
    "class PatchNCELoss(nn.Module):\n",
    "    def __init__(self, batch_size, nce_T = 0.07):\n",
    "        super().__init__()\n",
    "        self.batch_size = batch_size\n",
    "        self.nce_T = nce_T\n",
    "        self.cross_entropy_loss = torch.nn.CrossEntropyLoss(reduction='none')\n",
    "        self.mask_dtype = torch.bool\n",
    "\n",
    "    def forward(self, feat_q, feat_k):\n",
    "        batchSize = feat_q.shape[0]\n",
    "        dim = feat_q.shape[1]\n",
    "        feat_k = feat_k.detach()\n",
    "\n",
    "        # pos logit\n",
    "        l_pos = torch.bmm(feat_q.view(batchSize, 1, -1), feat_k.view(batchSize, -1, 1))\n",
    "        l_pos = l_pos.view(batchSize, 1)\n",
    "\n",
    "        batch_dim_for_bmm = self.batch_size\n",
    "\n",
    "        # reshape features to batch size\n",
    "        feat_q = feat_q.view(batch_dim_for_bmm, -1, dim)\n",
    "        feat_k = feat_k.view(batch_dim_for_bmm, -1, dim)\n",
    "        npatches = feat_q.size(1)\n",
    "        l_neg_curbatch = torch.bmm(feat_q, feat_k.transpose(2, 1))\n",
    "\n",
    "        # diagonal entries are similarity between same features, and hence meaningless.\n",
    "        # just fill the diagonal with very small number, which is exp(-10) and almost zero\n",
    "        diagonal = torch.eye(npatches, device=feat_q.device, dtype=self.mask_dtype)[None, :, :]\n",
    "        l_neg_curbatch.masked_fill_(diagonal, -10.0)\n",
    "        l_neg = l_neg_curbatch.view(-1, npatches)\n",
    "\n",
    "        out = torch.cat((l_pos, l_neg), dim=1) / self.nce_T\n",
    "\n",
    "        loss = self.cross_entropy_loss(out, torch.zeros(out.size(0), dtype=torch.long,\n",
    "                                                        device=feat_q.device))\n",
    "\n",
    "        return loss\n",
    "    \n",
    "def nonsaturating_loss(prediction, is_real):\n",
    "    if is_real.mean() == 1:\n",
    "        loss = F.softplus(-prediction).view(prediction.size(0), -1).mean(dim = 1)\n",
    "    else:\n",
    "        loss = F.softplus(prediction).view(prediction.size(0), -1).mean(dim = 1)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the networks, load the most recent saved models, and set the loss functions and optimizers\n",
    "\n",
    "if faces:\n",
    "    input_size = (input_size[0], input_size[1], input_size[1])\n",
    "G = Generator(input_size, res_blocks, kernel_size).to(device)\n",
    "D = Discriminator(input_size, kernel_size).to(device)\n",
    "Sampler = PatchSampleF(batch_size).to(device)\n",
    "\n",
    "if nonsaturating:\n",
    "    criterion_GAN = nonsaturating_loss\n",
    "else:\n",
    "    criterion_GAN = torch.nn.MSELoss().to(device)\n",
    "criterion_NCE = []\n",
    "\n",
    "for nce_layer in nce_layers:\n",
    "    criterion_NCE.append(PatchNCELoss(batch_size).to(device))\n",
    "\n",
    "G.apply(weights_init_normal)\n",
    "D.apply(weights_init_normal)\n",
    "\n",
    "optimizer_G = torch.optim.Adam(G.parameters(), lr = learning_rate, betas = betas)\n",
    "optimizer_D = torch.optim.Adam(D.parameters(), lr = learning_rate, betas = betas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters for the sampler are not made until the first forward pass through the Sampler network\n",
    "# Hence, we do a 'trial' training pass before setting the optimizer for the Sampler\n",
    "for i, batch in enumerate(dataloader):\n",
    "    x, y = Variable(batch[\"a\"].type(Tensor)), Variable(batch[\"b\"].type(Tensor))\n",
    "    \n",
    "    real = Variable(Tensor(np.ones((x.size(0), *D.output_shape))), requires_grad = False)\n",
    "    fake = Variable(Tensor(np.zeros((x.size(0), *D.output_shape))), requires_grad = False)\n",
    "    \n",
    "    D.eval()\n",
    "    G.eval()\n",
    "    Sampler.eval()\n",
    "    # get the fake loss\n",
    "    fake_y = G(x)\n",
    "    D_fake = D(fake_y.detach())\n",
    "    loss_D_fake = criterion_GAN(D_fake, fake).mean()\n",
    "    # get the real loss\n",
    "    D_real = D(y)\n",
    "    loss_D_real = criterion_GAN(D_real, real).mean()\n",
    "    # combine loss and calculate gradients\n",
    "    loss_D = (loss_D_fake + loss_D_real) * 0.5\n",
    "    loss_D.backward()\n",
    "\n",
    "    # get the fake GAN loss\n",
    "    D_fake = D(fake_y)\n",
    "    loss_G_GAN = lambda_GAN * criterion_GAN(D_fake, real).mean()\n",
    "    total_nce_loss = 0\n",
    "    for fake, real in [(fake_y, x), (y, G(y))]:\n",
    "        # get the NCE loss\n",
    "        feat_q = G(fake, nce_layers, encode_only = True)\n",
    "        feat_k = G(real, nce_layers, encode_only = True)\n",
    "\n",
    "        feat_k_pool, sample_ids = Sampler(feat_k, 256, None)\n",
    "        feat_q_pool, _ = Sampler(feat_q, 256, sample_ids)\n",
    "\n",
    "        total_nce_loss = 0.0\n",
    "        for f_q, f_k, crit, nce_layer in zip(feat_q_pool, feat_k_pool, criterion_NCE, nce_layers):\n",
    "            loss = crit(f_q, f_k) * lambda_NCE\n",
    "            total_nce_loss += loss.mean()\n",
    "\n",
    "        nce_loss = total_nce_loss / len(nce_layers)\n",
    "        total_nce_loss += nce_loss\n",
    "\n",
    "    loss_G = loss_G_GAN + total_nce_loss\n",
    "    loss_G.backward()\n",
    "    \n",
    "    break\n",
    "\n",
    "optimizer_Sampler = torch.optim.Adam(Sampler.parameters(), lr = learning_rate, betas = betas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the most recently saved models if available\n",
    "if os.path.isdir('models/%s' % model_name) and load_weights:\n",
    "    # Get the most recent model and load them\n",
    "    epoch = max([int(fname[8:-4]) for fname in os.listdir('models/%s' % model_name) if 'Sampler' in fname])\n",
    "    G.load_state_dict(torch.load('models/%s/G_%d.pth' % (model_name, epoch)))\n",
    "    D.load_state_dict(torch.load('models/%s/D_%d.pth' % (model_name, epoch)))\n",
    "    Sampler.load_state_dict(torch.load('models/%s/Sampler_%d.pth' % (model_name, epoch)))\n",
    "    # Load the losses as well, for plotting\n",
    "    losses = np.load('models/%s/losses_%d.npy' % (model_name, epoch))\n",
    "    D_losses = list(losses[0])\n",
    "    GAN_losses = list(losses[1])\n",
    "    NCE_losses = list(losses[2])\n",
    "    G_total_losses = list(losses[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 39/200] [Batch 578/600] [D loss: 0.336752] [GAN loss: 0.331345, NCE loss: 1.311128, Total: 4.264729] ETA: 3:28:45.086472079739"
     ]
    }
   ],
   "source": [
    "# Training Loop, adapted from https://github.com/taesungp/contrastive-unpaired-translation\n",
    "\n",
    "prev_time = time.time()\n",
    "for epoch in range(epoch, epochs):\n",
    "    total_D_loss = 0\n",
    "    total_GAN_loss = 0\n",
    "    total_NCE_loss = 0\n",
    "    total_G_loss = 0\n",
    "    for i, batch in enumerate(dataloader):\n",
    "        x, y = Variable(batch[\"a\"].type(Tensor)), Variable(batch[\"b\"].type(Tensor))\n",
    "        \n",
    "        # Adversarial ground truths\n",
    "        real = Variable(Tensor(np.ones((x.size(0), *D.output_shape))), requires_grad = False)\n",
    "        fake = Variable(Tensor(np.zeros((x.size(0), *D.output_shape))), requires_grad = False)\n",
    "        \n",
    "        # train discriminator\n",
    "        D.train()\n",
    "        optimizer_D.zero_grad()\n",
    "        # get the fake loss\n",
    "        fake_y = G(x)\n",
    "        D_fake = D(fake_y.detach())\n",
    "        loss_D_fake = criterion_GAN(D_fake, fake).mean()\n",
    "        # get the real loss\n",
    "        D_real = D(y)\n",
    "        loss_D_real = criterion_GAN(D_real, real).mean()\n",
    "        # combine loss and calculate gradients\n",
    "        loss_D = (loss_D_fake + loss_D_real) * 0.5\n",
    "        loss_D.backward()\n",
    "        optimizer_D.step()\n",
    "        \n",
    "        # train generator\n",
    "        G.train()\n",
    "        optimizer_G.zero_grad()\n",
    "        optimizer_Sampler.zero_grad()\n",
    "        # get the fake GAN loss\n",
    "        D_fake = D(fake_y)\n",
    "        loss_G_GAN = criterion_GAN(D_fake, real).mean()\n",
    "        # get the NCE loss\n",
    "        Sampler.train()\n",
    "        total_nce_loss = 0\n",
    "        for fake, real in [(fake_y, x), (y, G(y))]:\n",
    "            feat_q = G(fake_y, nce_layers, encode_only = True)\n",
    "            feat_k = G(x, nce_layers, encode_only = True)\n",
    "\n",
    "            feat_k_pool, sample_ids = Sampler(feat_k, 256, None)\n",
    "            feat_q_pool, _ = Sampler(feat_q, 256, sample_ids)\n",
    "\n",
    "            total_nce_loss = 0.0\n",
    "            for f_q, f_k, crit, nce_layer in zip(feat_q_pool, feat_k_pool, criterion_NCE, nce_layers):\n",
    "                loss = crit(f_q, f_k) * lambda_NCE\n",
    "                total_nce_loss += loss.mean()\n",
    "\n",
    "            nce_loss = total_nce_loss / len(nce_layers)\n",
    "            total_nce_loss += nce_loss\n",
    "        total_nce_loss *= 0.5\n",
    "        \n",
    "        loss_G = loss_G_GAN + total_nce_loss\n",
    "        loss_G.backward()\n",
    "        optimizer_G.step()\n",
    "        optimizer_Sampler.step()\n",
    "        \n",
    "        # --------------\n",
    "        #  Log Progress\n",
    "        # --------------\n",
    "\n",
    "        # Determine approximate time left\n",
    "        batches_done = epoch * len(dataloader) + i\n",
    "        batches_left = 200 * len(dataloader) - batches_done\n",
    "        time_left = datetime.timedelta(seconds=batches_left * (time.time() - prev_time))\n",
    "        prev_time = time.time()\n",
    "\n",
    "        # Print log\n",
    "        sys.stdout.write(\n",
    "            \"\\r[Epoch %d/%d] [Batch %d/%d] [D loss: %f] [GAN loss: %f, NCE loss: %f, Total: %f] ETA: %s\"\n",
    "            % (\n",
    "                epoch,\n",
    "                200,\n",
    "                i,\n",
    "                len(dataloader),\n",
    "                loss_D.item(),\n",
    "                loss_G_GAN.item(),\n",
    "                nce_loss.item(),\n",
    "                loss_G.item(),\n",
    "                time_left,\n",
    "            )\n",
    "        )\n",
    "        \n",
    "        total_D_loss += loss_D.item()\n",
    "        total_GAN_loss += loss_G_GAN.item()\n",
    "        total_NCE_loss += nce_loss.item()\n",
    "        total_G_loss += loss_G.item()\n",
    "        \n",
    "        # If at sample interval save image\n",
    "        if batches_done % 300 == 0:\n",
    "            sample_images(batches_done)\n",
    "            \n",
    "    D_losses.append(total_D_loss / len(dataloader))\n",
    "    GAN_losses.append(total_GAN_loss / len(dataloader))\n",
    "    NCE_losses.append(total_NCE_loss / len(dataloader))\n",
    "    G_total_losses.append(total_G_loss / len(dataloader))\n",
    "    display.clear_output(wait = True)\n",
    "    time.sleep(1)\n",
    "    plt.clf()\n",
    "    plt.ylabel('Mean Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.plot(D_losses, color = 'r')\n",
    "    plt.plot(GAN_losses, color = 'g')\n",
    "    plt.plot(NCE_losses, color = 'b')\n",
    "    plt.savefig('models/%s/loss_plot.png' % model_name)\n",
    "    display.display(plt.gcf())\n",
    "    \n",
    "    if not os.path.isdir('models'):\n",
    "        os.makedirs('models')\n",
    "    if not os.path.isdir('models/%s' % model_name):\n",
    "        os.makedirs('models/%s' % model_name)\n",
    "    torch.save(G.state_dict(), \"models/%s/G_%d.pth\" % (model_name, epoch))\n",
    "    torch.save(D.state_dict(), \"models/%s/D_%d.pth\" % (model_name, epoch))\n",
    "    torch.save(Sampler.state_dict(), \"models/%s/Sampler_%d.pth\" % (model_name, epoch))\n",
    "    np.save('models/%s/losses_%d.npy' % (model_name, epoch), np.array([D_losses, GAN_losses, NCE_losses, G_total_losses]))\n",
    "    try:\n",
    "        os.remove('models/%s/G_%d.pth' % (model_name, epoch - 1))\n",
    "        os.remove('models/%s/D_%d.pth' % (model_name, epoch - 1))\n",
    "        os.remove('models/%s/Sampler_%d.pth' % (model_name, epoch - 1))\n",
    "        os.remove('models/%s/losses_%d.npy' % (model_name, epoch - 1))\n",
    "    except:\n",
    "        pass\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
