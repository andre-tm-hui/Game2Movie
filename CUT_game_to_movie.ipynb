{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: numpy in c:\\users\\andre\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (1.19.3)\n",
      "Requirement already satisfied: opencv-python in c:\\users\\andre\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (4.4.0.46)\n",
      "Requirement already satisfied: dlib in c:\\users\\andre\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (19.21.1)\n",
      "Requirement already satisfied: matplotlib in c:\\users\\andre\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (3.3.3)\n",
      "Requirement already satisfied: boto3 in c:\\users\\andre\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (1.17.4)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in c:\\users\\andre\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from matplotlib) (1.3.1)\n",
      "Requirement already satisfied: python-dateutil>=2.1 in c:\\users\\andre\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from matplotlib) (2.8.1)\n",
      "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.3 in c:\\users\\andre\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from matplotlib) (2.4.7)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\andre\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from matplotlib) (0.10.0)\n",
      "Requirement already satisfied: pillow>=6.2.0 in c:\\users\\andre\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from matplotlib) (8.0.1)\n",
      "Requirement already satisfied: s3transfer<0.4.0,>=0.3.0 in c:\\users\\andre\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from boto3) (0.3.4)\n",
      "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in c:\\users\\andre\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from boto3) (0.10.0)\n",
      "Requirement already satisfied: botocore<1.21.0,>=1.20.4 in c:\\users\\andre\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from boto3) (1.20.4)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\andre\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from python-dateutil>=2.1->matplotlib) (1.15.0)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.25.4 in c:\\users\\andre\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from botocore<1.21.0,>=1.20.4->boto3) (1.26.2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 20.2.1; however, version 21.0.1 is available.\n",
      "You should consider upgrading via the 'c:\\users\\andre\\appdata\\local\\programs\\python\\python38\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    }
   ],
   "source": [
    "!pip install numpy opencv-python dlib matplotlib boto3\n",
    "try:\n",
    "    import torch\n",
    "except:\n",
    "    !pip install pip install torch==1.7.1+cu101 torchvision==0.8.2+cu101 torchaudio===0.7.2 -f https://download.pytorch.org/whl/torch_stable.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import numpy as np\n",
    "import cv2\n",
    "import os\n",
    "import glob\n",
    "import random\n",
    "import time, datetime, sys\n",
    "import zipfile as zf\n",
    "import boto3\n",
    "from PIL import Image\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch.autograd import Variable\n",
    "from torchvision import transforms, datasets\n",
    "from torchvision.utils import save_image, make_grid\n",
    "\n",
    "import dlib\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython import display\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the model name - determines where the outputs are saved\n",
    "model_name = 'G2M-faces'\n",
    "\n",
    "# Variables/Hyperparameters\n",
    "dataset_size = 600\n",
    "generate_dataset = True\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "Tensor = torch.cuda.FloatTensor if torch.cuda.is_available() else torch.Tensor\n",
    "nce_layers = [0, 4, 8, 12, 16]\n",
    "lambda_NCE = 1.0\n",
    "lambda_GAN = 1.0\n",
    "batch_size = 1\n",
    "input_size = (3,200,200)\n",
    "res_blocks = 9\n",
    "load_weights = True\n",
    "epoch = 0\n",
    "epochs = 200\n",
    "nonsaturating = False\n",
    "\n",
    "learning_rate = 0.001\n",
    "betas = (0.9, 0.999)\n",
    "\n",
    "D_losses = []\n",
    "GAN_losses = []\n",
    "NCE_losses = []\n",
    "G_total_losses = []\n",
    "\n",
    "# Swap the game and movie datasets, so that translation goes from movie to game instead\n",
    "swap = False\n",
    "\n",
    "# Toggle whether translation is done only on faces\n",
    "faces = True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the original data (and the processed data)\n",
    "s3 = boto3.resource('s3', aws_access_key_id = 'AKIAIOPFTDXA3ZXLK5YA', aws_secret_access_key = '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')\n",
    "bucket = s3.Bucket('vision-dataset-vmrj42')\n",
    "\n",
    "for fname in ['Data.zip', 'datasets.zip']:\n",
    "    if not os.path.isfile(fname):\n",
    "        bucket.download_file(fname, fname)\n",
    "\n",
    "if not os.path.isdir('Data'):\n",
    "    files = zf.ZipFile('Data.zip', 'r')\n",
    "    files.extractall('')\n",
    "if not os.path.isdir('dataset') and not generate_dataset:\n",
    "    files = zf.ZipFile('datasets.zip', 'r')\n",
    "    files.extractall('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a basic dataset for full-frame translation\n",
    "\n",
    "for dname in ['dataset/train/game', \n",
    "              'dataset/train/movie', \n",
    "              'dataset/test/game', \n",
    "              'dataset/test/movie', \n",
    "              'face_dataset/train/game',\n",
    "              'face_dataset/train/movie',\n",
    "              'face_dataset/test/game',\n",
    "              'face_dataset/test/movie']:\n",
    "    if not os.path.isdir(dname):\n",
    "        os.makedirs(dname)\n",
    "\n",
    "if len(os.listdir('dataset/train/game')) < dataset_size:\n",
    "    # get some frames from the game footage\n",
    "    cap = cv2.VideoCapture('Data/game/MafiaVideogame.mp4')\n",
    "    length = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "    frame_count = 0\n",
    "    saved_frames = 0\n",
    "    \n",
    "    face_detector = dlib.get_frontal_face_detector()\n",
    "    faces = []\n",
    "\n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        frame_count += 1\n",
    "        # Save the entire frame as part of the dataset, alternating between the training and testing datasets\n",
    "        if frame_count % (length // (2 * dataset_size)) == 0 and ret:\n",
    "            if saved_frames < dataset_size:\n",
    "                fname = 'dataset/train/game/%d.png' % (saved_frames)\n",
    "            else:\n",
    "                fname = 'dataset/test/game/%d.png' % (saved_frames % dataset_size)\n",
    "            cv2.imwrite(fname, frame)\n",
    "            saved_frames += 1\n",
    "        \n",
    "        # Check if there is a face in every (length // (10 * dataset_size)) frame\n",
    "        if frame_count % (length // (6 * dataset_size)) == 0 and ret:\n",
    "            dets = face_detector(frame, 1)\n",
    "            for i, d in enumerate(dets):\n",
    "                left, top, right, bottom = d.left(), d.top(), d.right(), d.bottom()\n",
    "                if right - left > 60:\n",
    "                    face = frame[top:bottom, left:right]\n",
    "                    if len(face) > 0 and len(face[0]) > 0:\n",
    "                        faces.append(face)\n",
    "    cap.release()\n",
    "    \n",
    "    # Alternating between the training and testing datasets, save the extracted faces\n",
    "    saved_faces = 0\n",
    "    for i, face in enumerate(faces):\n",
    "        if i % (len(faces) // (2 * dataset_size)) == 0:\n",
    "            if saved_faces < dataset_size:\n",
    "                fname = 'face_dataset/train/game/%d.png' % (saved_faces)\n",
    "            else:\n",
    "                fname = 'face_dataset/test/game/%d.png' % (saved_faces % dataset_size)\n",
    "            cv2.imwrite(fname, cv2.resize(face, (input_size[1], input_size[1])))\n",
    "            saved_faces += 1\n",
    "\n",
    "    # get some frames from the movie footage\n",
    "    movie_dirs = ['Data/movie/TheGodfather.mp4', 'Data/movie/TheIrishman.mp4', 'Data/movie/TheSopranos.mp4']\n",
    "\n",
    "    saved_frames = 0\n",
    "    for movie in movie_dirs:\n",
    "        cap = cv2.VideoCapture(movie)\n",
    "        length = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "        frame_count = 0\n",
    "\n",
    "        while cap.isOpened():\n",
    "            ret, frame = cap.read()\n",
    "            if not ret:\n",
    "                break\n",
    "            frame_count += 1\n",
    "            if frame_count % (length // (2 * dataset_size / len(movie_dirs))) == 0 and ret:\n",
    "                if saved_frames < dataset_size:\n",
    "                    fname = 'dataset/train/movie/%d.png' % (saved_frames)\n",
    "                else:\n",
    "                    fname = 'dataset/test/movie/%d.png' % (saved_frames % dataset_size)\n",
    "                cv2.imwrite(fname, frame)\n",
    "                saved_frames += 1\n",
    "\n",
    "\n",
    "        cap.release()\n",
    "        \n",
    "    face_detector = dlib.get_frontal_face_detector()        \n",
    "    faces_dir = 'Data/faces/'\n",
    "    real_faces = os.listdir(faces_dir)\n",
    "    saved_faces = 0\n",
    "    current_face = 0\n",
    "    while saved_faces < dataset_size * 4:\n",
    "        real_face = cv2.imread(faces_dir + real_faces[current_face])\n",
    "        face = []\n",
    "        dets = face_detector(real_face, 1)\n",
    "        for d in dets:\n",
    "            left, top, right, bottom = d.left(), d.top(), d.right(), d.bottom()\n",
    "            if right - left > 60:\n",
    "                face = real_face[top : bottom, left : right]\n",
    "                if len(face) > 0 and len(face[0]) > 0:\n",
    "                    face = cv2.resize(face, (input_size[1], input_size[1]))\n",
    "                    if saved_faces < dataset_size * 2:\n",
    "                        fname = 'face_dataset/train/movie/%d.png' % (saved_faces)\n",
    "                    else:\n",
    "                        fname = 'face_dataset/test/movie/%d.png' % (saved_faces % (dataset_size * 2))\n",
    "                    cv2.imwrite(fname, face)\n",
    "                    saved_faces += 1\n",
    "                    break\n",
    "        current_face += 1\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helpful class for loading both game and movie samples as one dataset\n",
    "class ImageDataset(Dataset):\n",
    "    def __init__(self, root, swap = False, transforms_ = None, unaligned = False, mode = \"train\"):\n",
    "        self.transform = transforms.Compose(transforms_)\n",
    "        self.unaligned = unaligned\n",
    "        if swap:\n",
    "            self.files_game = sorted(glob.glob(os.path.join(root, \"%s/movie\" % mode) + \"/*.*\"))\n",
    "            self.files_movie = sorted(glob.glob(os.path.join(root, \"%s/game\" % mode) + \"/*.*\"))\n",
    "        else:\n",
    "            self.files_game = sorted(glob.glob(os.path.join(root, \"%s/game\" % mode) + \"/*.*\"))\n",
    "            self.files_movie = sorted(glob.glob(os.path.join(root, \"%s/movie\" % mode) + \"/*.*\"))\n",
    "        print(len(os.listdir(os.path.join(root, 'train/movie'))))\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        image_game = Image.open(self.files_game[index % len(self.files_game)])\n",
    "\n",
    "        if self.unaligned:\n",
    "            image_movie = Image.open(self.files_movie[random.randint(0, len(self.files_movie) - 1)])\n",
    "        else:\n",
    "            image_movie = Image.open(self.files_movie[index % len(self.files_movie)])\n",
    "\n",
    "        # Convert grayscale images to rgb\n",
    "        if image_game.mode != \"RGB\":\n",
    "            image_game = to_rgb(image_game)\n",
    "        if image_movie.mode != \"RGB\":\n",
    "            image_movie = to_rgb(image_movie)\n",
    "\n",
    "        item_game = self.transform(image_game)\n",
    "        item_movie = self.transform(image_movie)\n",
    "        \n",
    "        return {\"a\": item_game, \"b\": item_movie}\n",
    "\n",
    "    def __len__(self):\n",
    "        return max(len(self.files_game), len(self.files_movie))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1200\n",
      "1200\n"
     ]
    }
   ],
   "source": [
    "# Loading the dataset\n",
    "# Define the dataset-wide transformations\n",
    "\n",
    "transforms_ = [\n",
    "    transforms.Resize(int(input_size[1] * 1.4), Image.BICUBIC),\n",
    "    transforms.RandomCrop((input_size[1],input_size[2])),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
    "]\n",
    "\n",
    "\n",
    "if faces:\n",
    "    dataset_dir = 'face_dataset'\n",
    "else:\n",
    "    dataset_dir = 'dataset'\n",
    "# Training data loader\n",
    "dataloader = DataLoader(\n",
    "    ImageDataset(dataset_dir, swap = swap, transforms_ = transforms_, unaligned = True),\n",
    "    batch_size = batch_size,\n",
    "    shuffle = True,\n",
    "    num_workers = 0,\n",
    ")\n",
    "# Test data loader\n",
    "val_dataloader = DataLoader(\n",
    "    ImageDataset(dataset_dir, swap = swap, transforms_ = transforms_, unaligned = True, mode = 'test'),\n",
    "    batch_size = 5,\n",
    "    shuffle = True,\n",
    "    num_workers = 0,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Other Utils\n",
    "\n",
    "def weights_init_normal(m):\n",
    "    classname = m.__class__.__name__\n",
    "    if classname.find(\"Conv\") != -1:\n",
    "        torch.nn.init.normal_(m.weight.data, 0.0, 0.02)\n",
    "        if hasattr(m, \"bias\") and m.bias is not None:\n",
    "            torch.nn.init.constant_(m.bias.data, 0.0)\n",
    "    elif classname.find(\"BatchNorm2d\") != -1:\n",
    "        torch.nn.init.normal_(m.weight.data, 1.0, 0.02)\n",
    "        torch.nn.init.constant_(m.bias.data, 0.0)\n",
    "        \n",
    "def sample_images(batches_done):\n",
    "    \"\"\"Saves a generated sample from the test set\"\"\"\n",
    "    imgs = next(iter(val_dataloader))\n",
    "    G.eval()\n",
    "    real_A = Variable(imgs[\"a\"].type(Tensor))\n",
    "    fake_B = G(real_A)\n",
    "    # Arange images along x-axis\n",
    "    real_A = make_grid(real_A, nrow=5, normalize=True)\n",
    "    fake_B = make_grid(fake_B, nrow=5, normalize=True)\n",
    "    # Arange images along y-axis\n",
    "    image_grid = torch.cat((real_A, fake_B), 1)\n",
    "    if not os.path.isdir('models/%s/samples' % model_name):\n",
    "        os.makedirs('models/%s/samples' % model_name)\n",
    "    save_image(image_grid, \"models/%s/samples/%s.png\" % (model_name, batches_done), normalize=False)\n",
    "    \n",
    "class Normalize(nn.Module):\n",
    "    def __init__(self, power=2):\n",
    "        super(Normalize, self).__init__()\n",
    "        self.power = power\n",
    "\n",
    "    def forward(self, x):\n",
    "        norm = x.pow(self.power).sum(1, keepdim=True).pow(1. / self.power)\n",
    "        out = x.div(norm + 1e-7)\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generator and Discriminator models adapted from https://github.com/eriklindernoren/PyTorch-GAN\n",
    "# PatchSampleF adapted from https://github.com/taesungp/contrastive-unpaired-translation\n",
    "\n",
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, in_features):\n",
    "        super(ResidualBlock, self).__init__()\n",
    "\n",
    "        self.block = nn.Sequential(\n",
    "            nn.ReflectionPad2d(1),\n",
    "            nn.Conv2d(in_features, in_features, 3),\n",
    "            nn.InstanceNorm2d(in_features),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.ReflectionPad2d(1),\n",
    "            nn.Conv2d(in_features, in_features, 3),\n",
    "            nn.InstanceNorm2d(in_features),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x + self.block(x)\n",
    "\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self, input_shape, num_residual_blocks):\n",
    "        super(Generator, self).__init__()\n",
    "        channels = input_shape[0]\n",
    "\n",
    "        # Initial convolution block\n",
    "        out_features = 64\n",
    "        model = [\n",
    "            nn.ReflectionPad2d(channels),\n",
    "            nn.Conv2d(channels, out_features, 7),\n",
    "            nn.InstanceNorm2d(out_features),\n",
    "            nn.ReLU(inplace=True),\n",
    "        ]\n",
    "        in_features = out_features\n",
    "\n",
    "        # Downsampling\n",
    "        for _ in range(2):\n",
    "            out_features *= 2\n",
    "            model += [\n",
    "                nn.Conv2d(in_features, out_features, 3, stride=2, padding=1),\n",
    "                nn.InstanceNorm2d(out_features),\n",
    "                nn.ReLU(inplace=True),\n",
    "            ]\n",
    "            in_features = out_features\n",
    "\n",
    "        # Residual blocks\n",
    "        for _ in range(num_residual_blocks):\n",
    "            model += [ResidualBlock(out_features)]\n",
    "\n",
    "        # Upsampling\n",
    "        for _ in range(2):\n",
    "            out_features //= 2\n",
    "            model += [\n",
    "                nn.Upsample(scale_factor=2),\n",
    "                nn.Conv2d(in_features, out_features, 3, stride=1, padding=1),\n",
    "                nn.InstanceNorm2d(out_features),\n",
    "                nn.ReLU(inplace=True),\n",
    "            ]\n",
    "            in_features = out_features\n",
    "\n",
    "        # Output layer\n",
    "        model += [\n",
    "            nn.ReflectionPad2d(channels), \n",
    "            nn.Conv2d(out_features, channels, 7), \n",
    "            nn.Tanh()\n",
    "        ]\n",
    "\n",
    "        self.model = nn.Sequential(*model)\n",
    "\n",
    "    def forward(self, x, layers = [], encode_only = False):\n",
    "        if -1 in layers:\n",
    "            layers.append(len(self.model))\n",
    "        if len(layers) > 0:\n",
    "            feat = x\n",
    "            feats = []\n",
    "            for layer_id, layer in enumerate(self.model):\n",
    "                feat = layer(feat)\n",
    "                if layer_id in layers:\n",
    "                    feats.append(feat)\n",
    "                else:\n",
    "                    pass\n",
    "                if layer_id == layers[-1] and encode_only:\n",
    "                    return feats\n",
    "            return feat, feats\n",
    "        else:\n",
    "            return self.model(x)\n",
    "    \n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, input_shape):\n",
    "        super(Discriminator, self).__init__()\n",
    "        channels, height, width = input_shape\n",
    "        self.output_shape = (1, height // 2 ** 4, width // 2 ** 4)\n",
    "\n",
    "        def discriminator_block(in_filters, out_filters, normalize = True):\n",
    "            \"\"\"Returns downsampling layers of each discriminator block\"\"\"\n",
    "            layers = [nn.Conv2d(in_filters, out_filters, 4, stride=2, padding=1)]\n",
    "            if normalize:\n",
    "                layers.append(nn.InstanceNorm2d(out_filters))\n",
    "            layers.append(nn.LeakyReLU(0.2, inplace=True))\n",
    "            return layers\n",
    "\n",
    "        self.model = nn.Sequential(\n",
    "            *discriminator_block(channels, 64, normalize=False),\n",
    "            *discriminator_block(64, 128),\n",
    "            *discriminator_block(128, 256),\n",
    "            *discriminator_block(256, 512),\n",
    "            nn.ZeroPad2d((1, 0, 1, 0)),\n",
    "            nn.Conv2d(512, 1, 4, padding=1)\n",
    "        )\n",
    "\n",
    "    def forward(self, img):\n",
    "        return self.model(img)\n",
    "    \n",
    "class PatchSampleF(nn.Module):\n",
    "    def __init__(self, use_mlp = True, init_type = 'normal', init_gain = 0.02, nc = 256):\n",
    "        # potential issues: currently, we use the same patch_ids for multiple images in the batch\n",
    "        super(PatchSampleF, self).__init__()\n",
    "        self.l2norm = Normalize(2)\n",
    "        self.use_mlp = use_mlp\n",
    "        self.nc = nc  # hard-coded\n",
    "        self.mlp_init = False\n",
    "        self.init_type = init_type\n",
    "        self.init_gain = init_gain\n",
    "\n",
    "    def create_mlp(self, feats):\n",
    "        for mlp_id, feat in enumerate(feats):\n",
    "            input_nc = feat.shape[1]\n",
    "            mlp = nn.Sequential(*[nn.Linear(input_nc, self.nc), nn.ReLU(), nn.Linear(self.nc, self.nc)])\n",
    "            if torch.cuda.is_available():\n",
    "                mlp.cuda()\n",
    "            setattr(self, 'mlp_%d' % mlp_id, mlp)\n",
    "        if torch.cuda.is_available():\n",
    "            self.to(device)\n",
    "        self.apply(weights_init_normal)\n",
    "        self.mlp_init = True\n",
    "\n",
    "    def forward(self, feats, num_patches = 64, patch_ids = None):\n",
    "        return_ids = []\n",
    "        return_feats = []\n",
    "        if self.use_mlp and not self.mlp_init:\n",
    "            self.create_mlp(feats)\n",
    "        for feat_id, feat in enumerate(feats):\n",
    "            B, H, W = feat.shape[0], feat.shape[2], feat.shape[3]\n",
    "            feat_reshape = feat.permute(0, 2, 3, 1).flatten(1, 2)\n",
    "            if num_patches > 0:\n",
    "                if patch_ids is not None:\n",
    "                    patch_id = patch_ids[feat_id]\n",
    "                else:\n",
    "                    patch_id = torch.randperm(feat_reshape.shape[1], device=feats[0].device)\n",
    "                    patch_id = patch_id[:int(min(num_patches, patch_id.shape[0]))]  # .to(patch_ids.device)\n",
    "                x_sample = feat_reshape[:, patch_id, :].flatten(0, 1)  # reshape(-1, x.shape[1])\n",
    "            else:\n",
    "                x_sample = feat_reshape\n",
    "                patch_id = []\n",
    "            if self.use_mlp:\n",
    "                mlp = getattr(self, 'mlp_%d' % feat_id)\n",
    "                x_sample = mlp(x_sample)\n",
    "            return_ids.append(patch_id)\n",
    "            x_sample = self.l2norm(x_sample)\n",
    "\n",
    "            if num_patches == 0:\n",
    "                x_sample = x_sample.permute(0, 2, 1).reshape([B, x_sample.shape[-1], H, W])\n",
    "            return_feats.append(x_sample)\n",
    "        return return_feats, return_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Class to create separate losses for each feature, from https://github.com/taesungp/contrastive-unpaired-translation\n",
    "class PatchNCELoss(nn.Module):\n",
    "    def __init__(self, batch_size, nce_T = 0.07):\n",
    "        super().__init__()\n",
    "        self.batch_size = batch_size\n",
    "        self.nce_T = nce_T\n",
    "        self.cross_entropy_loss = torch.nn.CrossEntropyLoss(reduction='none')\n",
    "        self.mask_dtype = torch.bool\n",
    "\n",
    "    def forward(self, feat_q, feat_k):\n",
    "        batchSize = feat_q.shape[0]\n",
    "        dim = feat_q.shape[1]\n",
    "        feat_k = feat_k.detach()\n",
    "\n",
    "        # pos logit\n",
    "        l_pos = torch.bmm(feat_q.view(batchSize, 1, -1), feat_k.view(batchSize, -1, 1))\n",
    "        l_pos = l_pos.view(batchSize, 1)\n",
    "\n",
    "        batch_dim_for_bmm = self.batch_size\n",
    "\n",
    "        # reshape features to batch size\n",
    "        feat_q = feat_q.view(batch_dim_for_bmm, -1, dim)\n",
    "        feat_k = feat_k.view(batch_dim_for_bmm, -1, dim)\n",
    "        npatches = feat_q.size(1)\n",
    "        l_neg_curbatch = torch.bmm(feat_q, feat_k.transpose(2, 1))\n",
    "\n",
    "        # diagonal entries are similarity between same features, and hence meaningless.\n",
    "        # just fill the diagonal with very small number, which is exp(-10) and almost zero\n",
    "        diagonal = torch.eye(npatches, device=feat_q.device, dtype=self.mask_dtype)[None, :, :]\n",
    "        l_neg_curbatch.masked_fill_(diagonal, -10.0)\n",
    "        l_neg = l_neg_curbatch.view(-1, npatches)\n",
    "\n",
    "        out = torch.cat((l_pos, l_neg), dim=1) / self.nce_T\n",
    "\n",
    "        loss = self.cross_entropy_loss(out, torch.zeros(out.size(0), dtype=torch.long,\n",
    "                                                        device=feat_q.device))\n",
    "\n",
    "        return loss\n",
    "    \n",
    "def nonsaturating_loss(prediction, is_real):\n",
    "    if is_real.mean() == 1:\n",
    "        loss = F.softplus(-prediction).view(prediction.size(0), -1).mean(dim = 1)\n",
    "    else:\n",
    "        loss = F.softplus(prediction).view(prediction.size(0), -1).mean(dim = 1)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the networks, load the most recent saved models, and set the loss functions and optimizers\n",
    "\n",
    "if faces:\n",
    "    input_size = (input_size[0], input_size[1], input_size[1])\n",
    "G = Generator(input_size, res_blocks).to(device)\n",
    "D = Discriminator(input_size).to(device)\n",
    "Sampler = PatchSampleF(batch_size).to(device)\n",
    "\n",
    "if nonsaturating:\n",
    "    criterion_GAN = nonsaturating_loss\n",
    "else:\n",
    "    criterion_GAN = torch.nn.MSELoss().to(device)\n",
    "criterion_NCE = []\n",
    "\n",
    "for nce_layer in nce_layers:\n",
    "    criterion_NCE.append(PatchNCELoss(batch_size).to(device))\n",
    "\n",
    "G.apply(weights_init_normal)\n",
    "D.apply(weights_init_normal)\n",
    "\n",
    "optimizer_G = torch.optim.Adam(G.parameters(), lr = learning_rate, betas = betas)\n",
    "optimizer_D = torch.optim.Adam(D.parameters(), lr = learning_rate, betas = betas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters for the sampler are not made until the first forward pass through the Sampler network\n",
    "# Hence, we do a 'trial' training pass before setting the optimizer for the Sampler\n",
    "for i, batch in enumerate(dataloader):\n",
    "    x, y = Variable(batch[\"a\"].type(Tensor)), Variable(batch[\"b\"].type(Tensor))\n",
    "    \n",
    "    real = Variable(Tensor(np.ones((x.size(0), *D.output_shape))), requires_grad = False)\n",
    "    fake = Variable(Tensor(np.zeros((x.size(0), *D.output_shape))), requires_grad = False)\n",
    "    \n",
    "    D.eval()\n",
    "    G.eval()\n",
    "    Sampler.eval()\n",
    "    # get the fake loss\n",
    "    fake_y = G(x)\n",
    "    D_fake = D(fake_y.detach())\n",
    "    loss_D_fake = criterion_GAN(D_fake, fake).mean()\n",
    "    # get the real loss\n",
    "    D_real = D(y)\n",
    "    loss_D_real = criterion_GAN(D_real, real).mean()\n",
    "    # combine loss and calculate gradients\n",
    "    loss_D = (loss_D_fake + loss_D_real) * 0.5\n",
    "    loss_D.backward()\n",
    "\n",
    "    # get the fake GAN loss\n",
    "    D_fake = D(fake_y)\n",
    "    loss_G_GAN = lambda_GAN * criterion_GAN(D_fake, real).mean()\n",
    "    total_nce_loss = 0\n",
    "    for fake, real in [(fake_y, x), (y, G(y))]:\n",
    "        # get the NCE loss\n",
    "        feat_q = G(fake, nce_layers, encode_only = True)\n",
    "        feat_k = G(real, nce_layers, encode_only = True)\n",
    "\n",
    "        feat_k_pool, sample_ids = Sampler(feat_k, 256, None)\n",
    "        feat_q_pool, _ = Sampler(feat_q, 256, sample_ids)\n",
    "\n",
    "        total_nce_loss = 0.0\n",
    "        for f_q, f_k, crit, nce_layer in zip(feat_q_pool, feat_k_pool, criterion_NCE, nce_layers):\n",
    "            loss = crit(f_q, f_k) * lambda_NCE\n",
    "            total_nce_loss += loss.mean()\n",
    "\n",
    "        nce_loss = total_nce_loss / len(nce_layers)\n",
    "        total_nce_loss += nce_loss\n",
    "\n",
    "    loss_G = loss_G_GAN + total_nce_loss\n",
    "    loss_G.backward()\n",
    "    \n",
    "    break\n",
    "\n",
    "optimizer_Sampler = torch.optim.Adam(Sampler.parameters(), lr = learning_rate, betas = betas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the most recently saved models if available\n",
    "if os.path.isdir('models/%s' % model_name) and load_weights:\n",
    "    # Get the most recent model and load them\n",
    "    epoch = max([int(fname[8:-4]) for fname in os.listdir('models/%s' % model_name) if 'Sampler' in fname])\n",
    "    G.load_state_dict(torch.load('models/%s/G_%d.pth' % (model_name, epoch)))\n",
    "    D.load_state_dict(torch.load('models/%s/D_%d.pth' % (model_name, epoch)))\n",
    "    Sampler.load_state_dict(torch.load('models/%s/Sampler_%d.pth' % (model_name, epoch)))\n",
    "    # Load the losses as well, for plotting\n",
    "    losses = np.load('models/%s/losses_%d.npy' % (model_name, epoch))\n",
    "    D_losses = list(losses[0])\n",
    "    GAN_losses = list(losses[1])\n",
    "    NCE_losses = list(losses[2])\n",
    "    G_total_losses = list(losses[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 81/200] [Batch 154/1200] [D loss: 0.180118] [GAN loss: 0.693916, NCE loss: 1.364447, Total: 4.787255] ETA: 5:52:10.789606510509"
     ]
    }
   ],
   "source": [
    "# Training Loop, adapted from https://github.com/taesungp/contrastive-unpaired-translation\n",
    "\n",
    "prev_time = time.time()\n",
    "for epoch in range(epoch, epochs):\n",
    "    total_D_loss = 0\n",
    "    total_GAN_loss = 0\n",
    "    total_NCE_loss = 0\n",
    "    total_G_loss = 0\n",
    "    for i, batch in enumerate(dataloader):\n",
    "        x, y = Variable(batch[\"a\"].type(Tensor)), Variable(batch[\"b\"].type(Tensor))\n",
    "        \n",
    "        # Adversarial ground truths\n",
    "        real = Variable(Tensor(np.ones((x.size(0), *D.output_shape))), requires_grad = False)\n",
    "        fake = Variable(Tensor(np.zeros((x.size(0), *D.output_shape))), requires_grad = False)\n",
    "        \n",
    "        # train discriminator\n",
    "        D.train()\n",
    "        optimizer_D.zero_grad()\n",
    "        # get the fake loss\n",
    "        fake_y = G(x)\n",
    "        D_fake = D(fake_y.detach())\n",
    "        loss_D_fake = criterion_GAN(D_fake, fake).mean()\n",
    "        # get the real loss\n",
    "        D_real = D(y)\n",
    "        loss_D_real = criterion_GAN(D_real, real).mean()\n",
    "        # combine loss and calculate gradients\n",
    "        loss_D = (loss_D_fake + loss_D_real) * 0.5\n",
    "        loss_D.backward()\n",
    "        optimizer_D.step()\n",
    "        \n",
    "        # train generator\n",
    "        G.train()\n",
    "        optimizer_G.zero_grad()\n",
    "        optimizer_Sampler.zero_grad()\n",
    "        # get the fake GAN loss\n",
    "        D_fake = D(fake_y)\n",
    "        loss_G_GAN = criterion_GAN(D_fake, real).mean()\n",
    "        # get the NCE loss\n",
    "        Sampler.train()\n",
    "        total_nce_loss = 0\n",
    "        for fake, real in [(fake_y, x), (y, G(y))]:\n",
    "            feat_q = G(fake_y, nce_layers, encode_only = True)\n",
    "            feat_k = G(x, nce_layers, encode_only = True)\n",
    "\n",
    "            feat_k_pool, sample_ids = Sampler(feat_k, 256, None)\n",
    "            feat_q_pool, _ = Sampler(feat_q, 256, sample_ids)\n",
    "\n",
    "            total_nce_loss = 0.0\n",
    "            for f_q, f_k, crit, nce_layer in zip(feat_q_pool, feat_k_pool, criterion_NCE, nce_layers):\n",
    "                loss = crit(f_q, f_k) * lambda_NCE\n",
    "                total_nce_loss += loss.mean()\n",
    "\n",
    "            nce_loss = total_nce_loss / len(nce_layers)\n",
    "            total_nce_loss += nce_loss\n",
    "        total_nce_loss *= 0.5\n",
    "        \n",
    "        loss_G = loss_G_GAN + total_nce_loss\n",
    "        loss_G.backward()\n",
    "        optimizer_G.step()\n",
    "        optimizer_Sampler.step()\n",
    "        \n",
    "        # --------------\n",
    "        #  Log Progress\n",
    "        # --------------\n",
    "\n",
    "        # Determine approximate time left\n",
    "        batches_done = epoch * len(dataloader) + i\n",
    "        batches_left = 200 * len(dataloader) - batches_done\n",
    "        time_left = datetime.timedelta(seconds=batches_left * (time.time() - prev_time))\n",
    "        prev_time = time.time()\n",
    "\n",
    "        # Print log\n",
    "        sys.stdout.write(\n",
    "            \"\\r[Epoch %d/%d] [Batch %d/%d] [D loss: %f] [GAN loss: %f, NCE loss: %f, Total: %f] ETA: %s\"\n",
    "            % (\n",
    "                epoch,\n",
    "                200,\n",
    "                i,\n",
    "                len(dataloader),\n",
    "                loss_D.item(),\n",
    "                loss_G_GAN.item(),\n",
    "                nce_loss.item(),\n",
    "                loss_G.item(),\n",
    "                time_left,\n",
    "            )\n",
    "        )\n",
    "        \n",
    "        total_D_loss += loss_D.item()\n",
    "        total_GAN_loss += loss_G_GAN.item()\n",
    "        total_NCE_loss += nce_loss.item()\n",
    "        total_G_loss += loss_G.item()\n",
    "        \n",
    "        # If at sample interval save image\n",
    "        if batches_done % 300 == 0:\n",
    "            sample_images(batches_done)\n",
    "            \n",
    "    D_losses.append(total_D_loss / len(dataloader))\n",
    "    GAN_losses.append(total_GAN_loss / len(dataloader))\n",
    "    NCE_losses.append(total_NCE_loss / len(dataloader))\n",
    "    G_total_losses.append(total_G_loss / len(dataloader))\n",
    "    display.clear_output(wait = True)\n",
    "    time.sleep(1)\n",
    "    plt.clf()\n",
    "    plt.ylabel('Mean Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.plot(D_losses, color = 'r')\n",
    "    plt.plot(GAN_losses, color = 'g')\n",
    "    plt.plot(NCE_losses, color = 'b')\n",
    "    plt.savefig('models/%s/loss_plot.png' % model_name)\n",
    "    display.display(plt.gcf())\n",
    "    \n",
    "    if not os.path.isdir('models'):\n",
    "        os.makedirs('models')\n",
    "    if not os.path.isdir('models/%s' % model_name):\n",
    "        os.makedirs('models/%s' % model_name)\n",
    "    torch.save(G.state_dict(), \"models/%s/G_%d.pth\" % (model_name, epoch))\n",
    "    torch.save(D.state_dict(), \"models/%s/D_%d.pth\" % (model_name, epoch))\n",
    "    torch.save(Sampler.state_dict(), \"models/%s/Sampler_%d.pth\" % (model_name, epoch))\n",
    "    np.save('models/%s/losses_%d.npy' % (model_name, epoch), np.array([D_losses, GAN_losses, NCE_losses, G_total_losses]))\n",
    "    try:\n",
    "        os.remove('models/%s/G_%d.pth' % (model_name, epoch - 1))\n",
    "        os.remove('models/%s/D_%d.pth' % (model_name, epoch - 1))\n",
    "        os.remove('models/%s/Sampler_%d.pth' % (model_name, epoch - 1))\n",
    "        os.remove('models/%s/losses_%d.npy' % (model_name, epoch - 1))\n",
    "    except:\n",
    "        pass\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
