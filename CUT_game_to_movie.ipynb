{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 20.2.1; however, version 21.0.1 is available.\n",
      "You should consider upgrading via the 'c:\\users\\andre\\appdata\\local\\programs\\python\\python38\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: numpy in c:\\users\\andre\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (1.19.3)\n",
      "Requirement already satisfied: opencv-python in c:\\users\\andre\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (4.4.0.46)\n",
      "Requirement already satisfied: dlib in c:\\users\\andre\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (19.21.1)\n",
      "Requirement already satisfied: matplotlib in c:\\users\\andre\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (3.3.3)\n",
      "Requirement already satisfied: boto3 in c:\\users\\andre\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (1.17.4)\n",
      "Requirement already satisfied: pillow>=6.2.0 in c:\\users\\andre\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from matplotlib) (8.0.1)\n",
      "Requirement already satisfied: python-dateutil>=2.1 in c:\\users\\andre\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from matplotlib) (2.8.1)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in c:\\users\\andre\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from matplotlib) (1.3.1)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\andre\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from matplotlib) (0.10.0)\n",
      "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.3 in c:\\users\\andre\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from matplotlib) (2.4.7)\n",
      "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in c:\\users\\andre\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from boto3) (0.10.0)\n",
      "Requirement already satisfied: botocore<1.21.0,>=1.20.4 in c:\\users\\andre\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from boto3) (1.20.4)\n",
      "Requirement already satisfied: s3transfer<0.4.0,>=0.3.0 in c:\\users\\andre\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from boto3) (0.3.4)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\andre\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from python-dateutil>=2.1->matplotlib) (1.15.0)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.25.4 in c:\\users\\andre\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from botocore<1.21.0,>=1.20.4->boto3) (1.26.2)\n"
     ]
    }
   ],
   "source": [
    "!pip install numpy opencv-python dlib matplotlib boto3\n",
    "try:\n",
    "    import torch\n",
    "except:\n",
    "    !pip install pip install torch==1.7.1+cu101 torchvision==0.8.2+cu101 torchaudio===0.7.2 -f https://download.pytorch.org/whl/torch_stable.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import numpy as np\n",
    "import cv2\n",
    "import os\n",
    "import glob\n",
    "import random\n",
    "import time, datetime, sys\n",
    "import zipfile as zf\n",
    "import boto3\n",
    "from PIL import Image\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch.autograd import Variable\n",
    "from torchvision import transforms, datasets\n",
    "from torchvision.utils import save_image, make_grid\n",
    "\n",
    "import dlib\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython import display\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the model name - determines where the outputs are saved\n",
    "model_name = 'G2M-frames'\n",
    "\n",
    "# Variables/Hyperparameters\n",
    "dataset_size = 600\n",
    "generate_dataset = True\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "Tensor = torch.cuda.FloatTensor if torch.cuda.is_available() else torch.Tensor\n",
    "nce_layers = [0, 4, 8, 12, 16]\n",
    "lambda_NCE = 1.0\n",
    "lambda_GAN = 1.0\n",
    "batch_size = 1\n",
    "input_size = (3,216,384)\n",
    "res_blocks = 9\n",
    "load_weights = True\n",
    "epoch = 0\n",
    "epochs = 200\n",
    "nonsaturating = False\n",
    "\n",
    "learning_rate = 0.001\n",
    "betas = (0.9, 0.999)\n",
    "\n",
    "D_losses = []\n",
    "GAN_losses = []\n",
    "NCE_losses = []\n",
    "G_total_losses = []\n",
    "\n",
    "# Swap the game and movie datasets, so that translation goes from movie to game instead\n",
    "swap = False\n",
    "\n",
    "# Toggle whether translation is done only on faces\n",
    "faces = False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the original data (and the processed data)\n",
    "s3 = boto3.resource('s3', aws_access_key_id = 'AKIAIOPFTDXA3ZXLK5YA', aws_secret_access_key = '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')\n",
    "bucket = s3.Bucket('vision-dataset-vmrj42')\n",
    "\n",
    "for fname in ['Data.zip', 'datasets.zip']:\n",
    "    if not os.path.isfile(fname):\n",
    "        bucket.download_file(fname, fname)\n",
    "\n",
    "if not os.path.isdir('Data'):\n",
    "    files = zf.ZipFile('Data.zip', 'r')\n",
    "    files.extractall('')\n",
    "if not os.path.isdir('dataset') and not generate_dataset:\n",
    "    files = zf.ZipFile('datasets.zip', 'r')\n",
    "    files.extractall('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a basic dataset for full-frame translation\n",
    "\n",
    "for dname in ['dataset/train/game', \n",
    "              'dataset/train/movie', \n",
    "              'dataset/test/game', \n",
    "              'dataset/test/movie', \n",
    "              'face_dataset/train/game',\n",
    "              'face_dataset/train/movie',\n",
    "              'face_dataset/test/game',\n",
    "              'face_dataset/test/movie']:\n",
    "    if not os.path.isdir(dname):\n",
    "        os.makedirs(dname)\n",
    "\n",
    "if len(os.listdir('dataset/train/game')) < dataset_size:\n",
    "    # get some frames from the game footage\n",
    "    cap = cv2.VideoCapture('Data/game/MafiaVideogame.mp4')\n",
    "    length = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "    frame_count = 0\n",
    "    saved_frames = 0\n",
    "    \n",
    "    face_detector = dlib.get_frontal_face_detector()\n",
    "    faces = []\n",
    "\n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        frame_count += 1\n",
    "        # Save the entire frame as part of the dataset, alternating between the training and testing datasets\n",
    "        if frame_count % (length // (2 * dataset_size)) == 0 and ret:\n",
    "            if saved_frames < dataset_size:\n",
    "                fname = 'dataset/train/game/%d.png' % (saved_frames)\n",
    "            else:\n",
    "                fname = 'dataset/test/game/%d.png' % (saved_frames % dataset_size)\n",
    "            cv2.imwrite(fname, frame)\n",
    "            saved_frames += 1\n",
    "        \n",
    "        # Check if there is a face in every (length // (10 * dataset_size)) frame\n",
    "        if frame_count % (length // (6 * dataset_size)) == 0 and ret:\n",
    "            dets = face_detector(frame, 1)\n",
    "            for i, d in enumerate(dets):\n",
    "                left, top, right, bottom = d.left(), d.top(), d.right(), d.bottom()\n",
    "                if right - left > 60:\n",
    "                    face = frame[top:bottom, left:right]\n",
    "                    if len(face) > 0 and len(face[0]) > 0:\n",
    "                        faces.append(face)\n",
    "    cap.release()\n",
    "    \n",
    "    # Alternating between the training and testing datasets, save the extracted faces\n",
    "    saved_faces = 0\n",
    "    for i, face in enumerate(faces):\n",
    "        if i % (len(faces) // (2 * dataset_size)) == 0:\n",
    "            if saved_faces < dataset_size:\n",
    "                fname = 'face_dataset/train/game/%d.png' % (saved_faces)\n",
    "            else:\n",
    "                fname = 'face_dataset/test/game/%d.png' % (saved_faces % dataset_size)\n",
    "            cv2.imwrite(fname, cv2.resize(face, (input_size[1], input_size[1])))\n",
    "            saved_faces += 1\n",
    "\n",
    "    # get some frames from the movie footage\n",
    "    movie_dirs = ['Data/movie/TheGodfather.mp4', 'Data/movie/TheIrishman.mp4', 'Data/movie/TheSopranos.mp4']\n",
    "\n",
    "    saved_frames = 0\n",
    "    for movie in movie_dirs:\n",
    "        cap = cv2.VideoCapture(movie)\n",
    "        length = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "        frame_count = 0\n",
    "\n",
    "        while cap.isOpened():\n",
    "            ret, frame = cap.read()\n",
    "            if not ret:\n",
    "                break\n",
    "            frame_count += 1\n",
    "            if frame_count % (length // (2 * dataset_size / len(movie_dirs))) == 0 and ret:\n",
    "                if saved_frames < dataset_size:\n",
    "                    fname = 'dataset/train/movie/%d.png' % (saved_frames)\n",
    "                else:\n",
    "                    fname = 'dataset/test/movie/%d.png' % (saved_frames % dataset_size)\n",
    "                cv2.imwrite(fname, frame)\n",
    "                saved_frames += 1\n",
    "\n",
    "\n",
    "        cap.release()\n",
    "        \n",
    "    face_detector = dlib.get_frontal_face_detector()        \n",
    "    faces_dir = 'Data/faces/'\n",
    "    real_faces = os.listdir(faces_dir)\n",
    "    saved_faces = 0\n",
    "    current_face = 0\n",
    "    while saved_faces < dataset_size * 4:\n",
    "        real_face = cv2.imread(faces_dir + real_faces[current_face])\n",
    "        face = []\n",
    "        dets = face_detector(real_face, 1)\n",
    "        for d in dets:\n",
    "            left, top, right, bottom = d.left(), d.top(), d.right(), d.bottom()\n",
    "            if right - left > 60:\n",
    "                face = real_face[top : bottom, left : right]\n",
    "                if len(face) > 0 and len(face[0]) > 0:\n",
    "                    face = cv2.resize(face, (input_size[1], input_size[1]))\n",
    "                    if saved_faces < dataset_size * 2:\n",
    "                        fname = 'face_dataset/train/movie/%d.png' % (saved_faces)\n",
    "                    else:\n",
    "                        fname = 'face_dataset/test/movie/%d.png' % (saved_faces % (dataset_size * 2))\n",
    "                    cv2.imwrite(fname, face)\n",
    "                    saved_faces += 1\n",
    "                    break\n",
    "        current_face += 1\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helpful class for loading both game and movie samples as one dataset\n",
    "class ImageDataset(Dataset):\n",
    "    def __init__(self, root, swap = False, transforms_ = None, unaligned = False, mode = \"train\"):\n",
    "        self.transform = transforms.Compose(transforms_)\n",
    "        self.unaligned = unaligned\n",
    "        if swap:\n",
    "            self.files_game = sorted(glob.glob(os.path.join(root, \"%s/movie\" % mode) + \"/*.*\"))\n",
    "            self.files_movie = sorted(glob.glob(os.path.join(root, \"%s/game\" % mode) + \"/*.*\"))\n",
    "        else:\n",
    "            self.files_game = sorted(glob.glob(os.path.join(root, \"%s/game\" % mode) + \"/*.*\"))\n",
    "            self.files_movie = sorted(glob.glob(os.path.join(root, \"%s/movie\" % mode) + \"/*.*\"))\n",
    "        print(len(os.listdir(os.path.join(root, 'train/movie'))))\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        image_game = Image.open(self.files_game[index % len(self.files_game)])\n",
    "\n",
    "        if self.unaligned:\n",
    "            image_movie = Image.open(self.files_movie[random.randint(0, len(self.files_movie) - 1)])\n",
    "        else:\n",
    "            image_movie = Image.open(self.files_movie[index % len(self.files_movie)])\n",
    "\n",
    "        # Convert grayscale images to rgb\n",
    "        if image_game.mode != \"RGB\":\n",
    "            image_game = to_rgb(image_game)\n",
    "        if image_movie.mode != \"RGB\":\n",
    "            image_movie = to_rgb(image_movie)\n",
    "\n",
    "        item_game = self.transform(image_game)\n",
    "        item_movie = self.transform(image_movie)\n",
    "        \n",
    "        return {\"a\": item_game, \"b\": item_movie}\n",
    "\n",
    "    def __len__(self):\n",
    "        return max(len(self.files_game), len(self.files_movie))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "600\n",
      "600\n"
     ]
    }
   ],
   "source": [
    "# Loading the dataset\n",
    "# Define the dataset-wide transformations\n",
    "\n",
    "transforms_ = [\n",
    "    transforms.Resize(int(input_size[1] * 1.4), Image.BICUBIC),\n",
    "    transforms.RandomCrop((input_size[1],input_size[2])),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
    "]\n",
    "\n",
    "\n",
    "if faces:\n",
    "    dataset_dir = 'face_dataset'\n",
    "else:\n",
    "    dataset_dir = 'dataset'\n",
    "# Training data loader\n",
    "dataloader = DataLoader(\n",
    "    ImageDataset(dataset_dir, swap = swap, transforms_ = transforms_, unaligned = True),\n",
    "    batch_size = batch_size,\n",
    "    shuffle = True,\n",
    "    num_workers = 0,\n",
    ")\n",
    "# Test data loader\n",
    "val_dataloader = DataLoader(\n",
    "    ImageDataset(dataset_dir, swap = swap, transforms_ = transforms_, unaligned = True, mode = 'test'),\n",
    "    batch_size = 5,\n",
    "    shuffle = True,\n",
    "    num_workers = 0,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Other Utils\n",
    "\n",
    "def weights_init_normal(m):\n",
    "    classname = m.__class__.__name__\n",
    "    if classname.find(\"Conv\") != -1:\n",
    "        torch.nn.init.normal_(m.weight.data, 0.0, 0.02)\n",
    "        if hasattr(m, \"bias\") and m.bias is not None:\n",
    "            torch.nn.init.constant_(m.bias.data, 0.0)\n",
    "    elif classname.find(\"BatchNorm2d\") != -1:\n",
    "        torch.nn.init.normal_(m.weight.data, 1.0, 0.02)\n",
    "        torch.nn.init.constant_(m.bias.data, 0.0)\n",
    "        \n",
    "def sample_images(batches_done):\n",
    "    \"\"\"Saves a generated sample from the test set\"\"\"\n",
    "    imgs = next(iter(val_dataloader))\n",
    "    G.eval()\n",
    "    real_A = Variable(imgs[\"a\"].type(Tensor))\n",
    "    fake_B = G(real_A)\n",
    "    # Arange images along x-axis\n",
    "    real_A = make_grid(real_A, nrow=5, normalize=True)\n",
    "    fake_B = make_grid(fake_B, nrow=5, normalize=True)\n",
    "    # Arange images along y-axis\n",
    "    image_grid = torch.cat((real_A, fake_B), 1)\n",
    "    if not os.path.isdir('models/%s/samples' % model_name):\n",
    "        os.makedirs('models/%s/samples' % model_name)\n",
    "    save_image(image_grid, \"models/%s/samples/%s.png\" % (model_name, batches_done), normalize=False)\n",
    "    \n",
    "class Normalize(nn.Module):\n",
    "    def __init__(self, power=2):\n",
    "        super(Normalize, self).__init__()\n",
    "        self.power = power\n",
    "\n",
    "    def forward(self, x):\n",
    "        norm = x.pow(self.power).sum(1, keepdim=True).pow(1. / self.power)\n",
    "        out = x.div(norm + 1e-7)\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generator and Discriminator models adapted from https://github.com/eriklindernoren/PyTorch-GAN\n",
    "# PatchSampleF adapted from https://github.com/taesungp/contrastive-unpaired-translation\n",
    "\n",
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, in_features):\n",
    "        super(ResidualBlock, self).__init__()\n",
    "\n",
    "        self.block = nn.Sequential(\n",
    "            nn.ReflectionPad2d(1),\n",
    "            nn.Conv2d(in_features, in_features, 3),\n",
    "            nn.InstanceNorm2d(in_features),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.ReflectionPad2d(1),\n",
    "            nn.Conv2d(in_features, in_features, 3),\n",
    "            nn.InstanceNorm2d(in_features),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x + self.block(x)\n",
    "\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self, input_shape, num_residual_blocks):\n",
    "        super(Generator, self).__init__()\n",
    "        channels = input_shape[0]\n",
    "\n",
    "        # Initial convolution block\n",
    "        out_features = 64\n",
    "        model = [\n",
    "            nn.ReflectionPad2d(channels),\n",
    "            nn.Conv2d(channels, out_features, 7),\n",
    "            nn.InstanceNorm2d(out_features),\n",
    "            nn.ReLU(inplace=True),\n",
    "        ]\n",
    "        in_features = out_features\n",
    "\n",
    "        # Downsampling\n",
    "        for _ in range(2):\n",
    "            out_features *= 2\n",
    "            model += [\n",
    "                nn.Conv2d(in_features, out_features, 3, stride=2, padding=1),\n",
    "                nn.InstanceNorm2d(out_features),\n",
    "                nn.ReLU(inplace=True),\n",
    "            ]\n",
    "            in_features = out_features\n",
    "\n",
    "        # Residual blocks\n",
    "        for _ in range(num_residual_blocks):\n",
    "            model += [ResidualBlock(out_features)]\n",
    "\n",
    "        # Upsampling\n",
    "        for _ in range(2):\n",
    "            out_features //= 2\n",
    "            model += [\n",
    "                nn.Upsample(scale_factor=2),\n",
    "                nn.Conv2d(in_features, out_features, 3, stride=1, padding=1),\n",
    "                nn.InstanceNorm2d(out_features),\n",
    "                nn.ReLU(inplace=True),\n",
    "            ]\n",
    "            in_features = out_features\n",
    "\n",
    "        # Output layer\n",
    "        model += [\n",
    "            nn.ReflectionPad2d(channels), \n",
    "            nn.Conv2d(out_features, channels, 7), \n",
    "            nn.Tanh()\n",
    "        ]\n",
    "\n",
    "        self.model = nn.Sequential(*model)\n",
    "\n",
    "    def forward(self, x, layers = [], encode_only = False):\n",
    "        if -1 in layers:\n",
    "            layers.append(len(self.model))\n",
    "        if len(layers) > 0:\n",
    "            feat = x\n",
    "            feats = []\n",
    "            for layer_id, layer in enumerate(self.model):\n",
    "                feat = layer(feat)\n",
    "                if layer_id in layers:\n",
    "                    feats.append(feat)\n",
    "                else:\n",
    "                    pass\n",
    "                if layer_id == layers[-1] and encode_only:\n",
    "                    return feats\n",
    "            return feat, feats\n",
    "        else:\n",
    "            return self.model(x)\n",
    "    \n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, input_shape):\n",
    "        super(Discriminator, self).__init__()\n",
    "        channels, height, width = input_shape\n",
    "        self.output_shape = (1, height // 2 ** 4, width // 2 ** 4)\n",
    "\n",
    "        def discriminator_block(in_filters, out_filters, normalize = True):\n",
    "            \"\"\"Returns downsampling layers of each discriminator block\"\"\"\n",
    "            layers = [nn.Conv2d(in_filters, out_filters, 4, stride=2, padding=1)]\n",
    "            if normalize:\n",
    "                layers.append(nn.InstanceNorm2d(out_filters))\n",
    "            layers.append(nn.LeakyReLU(0.2, inplace=True))\n",
    "            return layers\n",
    "\n",
    "        self.model = nn.Sequential(\n",
    "            *discriminator_block(channels, 64, normalize=False),\n",
    "            *discriminator_block(64, 128),\n",
    "            *discriminator_block(128, 256),\n",
    "            *discriminator_block(256, 512),\n",
    "            nn.ZeroPad2d((1, 0, 1, 0)),\n",
    "            nn.Conv2d(512, 1, 4, padding=1)\n",
    "        )\n",
    "\n",
    "    def forward(self, img):\n",
    "        return self.model(img)\n",
    "    \n",
    "class PatchSampleF(nn.Module):\n",
    "    def __init__(self, use_mlp = True, init_type = 'normal', init_gain = 0.02, nc = 256):\n",
    "        # potential issues: currently, we use the same patch_ids for multiple images in the batch\n",
    "        super(PatchSampleF, self).__init__()\n",
    "        self.l2norm = Normalize(2)\n",
    "        self.use_mlp = use_mlp\n",
    "        self.nc = nc  # hard-coded\n",
    "        self.mlp_init = False\n",
    "        self.init_type = init_type\n",
    "        self.init_gain = init_gain\n",
    "\n",
    "    def create_mlp(self, feats):\n",
    "        for mlp_id, feat in enumerate(feats):\n",
    "            input_nc = feat.shape[1]\n",
    "            mlp = nn.Sequential(*[nn.Linear(input_nc, self.nc), nn.ReLU(), nn.Linear(self.nc, self.nc)])\n",
    "            if torch.cuda.is_available():\n",
    "                mlp.cuda()\n",
    "            setattr(self, 'mlp_%d' % mlp_id, mlp)\n",
    "        if torch.cuda.is_available():\n",
    "            self.to(device)\n",
    "        self.apply(weights_init_normal)\n",
    "        self.mlp_init = True\n",
    "\n",
    "    def forward(self, feats, num_patches = 64, patch_ids = None):\n",
    "        return_ids = []\n",
    "        return_feats = []\n",
    "        if self.use_mlp and not self.mlp_init:\n",
    "            self.create_mlp(feats)\n",
    "        for feat_id, feat in enumerate(feats):\n",
    "            B, H, W = feat.shape[0], feat.shape[2], feat.shape[3]\n",
    "            feat_reshape = feat.permute(0, 2, 3, 1).flatten(1, 2)\n",
    "            if num_patches > 0:\n",
    "                if patch_ids is not None:\n",
    "                    patch_id = patch_ids[feat_id]\n",
    "                else:\n",
    "                    patch_id = torch.randperm(feat_reshape.shape[1], device=feats[0].device)\n",
    "                    patch_id = patch_id[:int(min(num_patches, patch_id.shape[0]))]  # .to(patch_ids.device)\n",
    "                x_sample = feat_reshape[:, patch_id, :].flatten(0, 1)  # reshape(-1, x.shape[1])\n",
    "            else:\n",
    "                x_sample = feat_reshape\n",
    "                patch_id = []\n",
    "            if self.use_mlp:\n",
    "                mlp = getattr(self, 'mlp_%d' % feat_id)\n",
    "                x_sample = mlp(x_sample)\n",
    "            return_ids.append(patch_id)\n",
    "            x_sample = self.l2norm(x_sample)\n",
    "\n",
    "            if num_patches == 0:\n",
    "                x_sample = x_sample.permute(0, 2, 1).reshape([B, x_sample.shape[-1], H, W])\n",
    "            return_feats.append(x_sample)\n",
    "        return return_feats, return_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Class to create separate losses for each feature, from https://github.com/taesungp/contrastive-unpaired-translation\n",
    "class PatchNCELoss(nn.Module):\n",
    "    def __init__(self, batch_size, nce_T = 0.07):\n",
    "        super().__init__()\n",
    "        self.batch_size = batch_size\n",
    "        self.nce_T = nce_T\n",
    "        self.cross_entropy_loss = torch.nn.CrossEntropyLoss(reduction='none')\n",
    "        self.mask_dtype = torch.bool\n",
    "\n",
    "    def forward(self, feat_q, feat_k):\n",
    "        batchSize = feat_q.shape[0]\n",
    "        dim = feat_q.shape[1]\n",
    "        feat_k = feat_k.detach()\n",
    "\n",
    "        # pos logit\n",
    "        l_pos = torch.bmm(feat_q.view(batchSize, 1, -1), feat_k.view(batchSize, -1, 1))\n",
    "        l_pos = l_pos.view(batchSize, 1)\n",
    "\n",
    "        batch_dim_for_bmm = self.batch_size\n",
    "\n",
    "        # reshape features to batch size\n",
    "        feat_q = feat_q.view(batch_dim_for_bmm, -1, dim)\n",
    "        feat_k = feat_k.view(batch_dim_for_bmm, -1, dim)\n",
    "        npatches = feat_q.size(1)\n",
    "        l_neg_curbatch = torch.bmm(feat_q, feat_k.transpose(2, 1))\n",
    "\n",
    "        # diagonal entries are similarity between same features, and hence meaningless.\n",
    "        # just fill the diagonal with very small number, which is exp(-10) and almost zero\n",
    "        diagonal = torch.eye(npatches, device=feat_q.device, dtype=self.mask_dtype)[None, :, :]\n",
    "        l_neg_curbatch.masked_fill_(diagonal, -10.0)\n",
    "        l_neg = l_neg_curbatch.view(-1, npatches)\n",
    "\n",
    "        out = torch.cat((l_pos, l_neg), dim=1) / self.nce_T\n",
    "\n",
    "        loss = self.cross_entropy_loss(out, torch.zeros(out.size(0), dtype=torch.long,\n",
    "                                                        device=feat_q.device))\n",
    "\n",
    "        return loss\n",
    "    \n",
    "def nonsaturating_loss(prediction, is_real):\n",
    "    if is_real.mean() == 1:\n",
    "        loss = F.softplus(-prediction).view(prediction.size(0), -1).mean(dim = 1)\n",
    "    else:\n",
    "        loss = F.softplus(prediction).view(prediction.size(0), -1).mean(dim = 1)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the networks, load the most recent saved models, and set the loss functions and optimizers\n",
    "\n",
    "if faces:\n",
    "    input_size = (input_size[0], input_size[1], input_size[1])\n",
    "G = Generator(input_size, res_blocks).to(device)\n",
    "D = Discriminator(input_size).to(device)\n",
    "Sampler = PatchSampleF(batch_size).to(device)\n",
    "\n",
    "if nonsaturating:\n",
    "    criterion_GAN = nonsaturating_loss\n",
    "else:\n",
    "    criterion_GAN = torch.nn.MSELoss().to(device)\n",
    "criterion_NCE = []\n",
    "\n",
    "for nce_layer in nce_layers:\n",
    "    criterion_NCE.append(PatchNCELoss(batch_size).to(device))\n",
    "\n",
    "G.apply(weights_init_normal)\n",
    "D.apply(weights_init_normal)\n",
    "\n",
    "optimizer_G = torch.optim.Adam(G.parameters(), lr = learning_rate, betas = betas)\n",
    "optimizer_D = torch.optim.Adam(D.parameters(), lr = learning_rate, betas = betas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters for the sampler are not made until the first forward pass through the Sampler network\n",
    "# Hence, we do a 'trial' training pass before setting the optimizer for the Sampler\n",
    "for i, batch in enumerate(dataloader):\n",
    "    x, y = Variable(batch[\"a\"].type(Tensor)), Variable(batch[\"b\"].type(Tensor))\n",
    "    \n",
    "    real = Variable(Tensor(np.ones((x.size(0), *D.output_shape))), requires_grad = False)\n",
    "    fake = Variable(Tensor(np.zeros((x.size(0), *D.output_shape))), requires_grad = False)\n",
    "    \n",
    "    D.eval()\n",
    "    G.eval()\n",
    "    Sampler.eval()\n",
    "    # get the fake loss\n",
    "    fake_y = G(x)\n",
    "    D_fake = D(fake_y.detach())\n",
    "    loss_D_fake = criterion_GAN(D_fake, fake).mean()\n",
    "    # get the real loss\n",
    "    D_real = D(y)\n",
    "    loss_D_real = criterion_GAN(D_real, real).mean()\n",
    "    # combine loss and calculate gradients\n",
    "    loss_D = (loss_D_fake + loss_D_real) * 0.5\n",
    "    loss_D.backward()\n",
    "\n",
    "    # get the fake GAN loss\n",
    "    D_fake = D(fake_y)\n",
    "    loss_G_GAN = lambda_GAN * criterion_GAN(D_fake, real).mean()\n",
    "    total_nce_loss = 0\n",
    "    for fake, real in [(fake_y, x), (y, G(y))]:\n",
    "        # get the NCE loss\n",
    "        feat_q = G(fake, nce_layers, encode_only = True)\n",
    "        feat_k = G(real, nce_layers, encode_only = True)\n",
    "\n",
    "        feat_k_pool, sample_ids = Sampler(feat_k, 256, None)\n",
    "        feat_q_pool, _ = Sampler(feat_q, 256, sample_ids)\n",
    "\n",
    "        total_nce_loss = 0.0\n",
    "        for f_q, f_k, crit, nce_layer in zip(feat_q_pool, feat_k_pool, criterion_NCE, nce_layers):\n",
    "            loss = crit(f_q, f_k) * lambda_NCE\n",
    "            total_nce_loss += loss.mean()\n",
    "\n",
    "        nce_loss = total_nce_loss / len(nce_layers)\n",
    "        total_nce_loss += nce_loss\n",
    "\n",
    "    loss_G = loss_G_GAN + total_nce_loss\n",
    "    loss_G.backward()\n",
    "    \n",
    "    break\n",
    "\n",
    "optimizer_Sampler = torch.optim.Adam(Sampler.parameters(), lr = learning_rate, betas = betas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the most recently saved models if available\n",
    "if os.path.isdir('models/%s' % model_name) and load_weights:\n",
    "    # Get the most recent model and load them\n",
    "    epoch = max([int(fname[8:-4]) for fname in os.listdir('models/%s' % model_name) if 'Sampler' in fname])\n",
    "    G.load_state_dict(torch.load('models/%s/G_%d.pth' % (model_name, epoch)))\n",
    "    D.load_state_dict(torch.load('models/%s/D_%d.pth' % (model_name, epoch)))\n",
    "    Sampler.load_state_dict(torch.load('models/%s/Sampler_%d.pth' % (model_name, epoch)))\n",
    "    # Load the losses as well, for plotting\n",
    "    losses = np.load('models/%s/losses_%d.npy' % (model_name, epoch))\n",
    "    D_losses = list(losses[0])\n",
    "    GAN_losses = list(losses[1])\n",
    "    NCE_losses = list(losses[2])\n",
    "    G_total_losses = list(losses[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAEGCAYAAABo25JHAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/Il7ecAAAACXBIWXMAAAsTAAALEwEAmpwYAAA5DklEQVR4nO3dd3hUZfbA8e9LEkINCSH0EqoQEEGq2CuIgoqigmt3bSDq6rpg+S2rropiBRfFrmsXCyLiUiwUKaF3hEjoEEoSCKTO+f1xJqSHBDKZkDmf55knmTv33jkzk7xn3nqdiGCMMSZwVfF3AMYYY/zLEoExxgQ4SwTGGBPgLBEYY0yAs0RgjDEBLtjfAZRWvXr1JDo62t9hGGPMSWXx4sV7RSSqsMdOukQQHR1NbGysv8MwxpiTinMuvqjHrGnIGGMCnCUCY4wJcJYIjDEmwFkiMMaYAGeJwBhjApwlAmOMCXCWCIwxJsAFTCJYtQoefxz27vV3JMYYU7EETCJYvx7+/W/Yvt3fkRhjTMUSMImgTh39mZTk3ziMMaaiCbhEkJzs3ziMMaaiCZhEEBamP61GYIwxeQVMIrCmIWOMKVzAJILsGoE1DRljTF4BkwiqV4fgYKsRGGNMfgGTCJzT5iGrERhjTF4BkwhAm4esRmCMMXkFVCKoU8cSgTHG5BdwicCahowxJq+ASgTWNGSMMQUFVCKwpiFjjCkooBJBWJg1DRljTH4BlQiyawQi/o7EGGMqDp8lAudcM+fcz865Nc651c65+wvZ5zznXJJzbpn39n++igc0EWRlwZEjvnwWY4w5uQT78NyZwEMissQ5VxtY7JybLiJr8u03W0Qu92EcR+VeeK5GjfJ4RmOMqfh8ViMQkZ0issT7+0FgLdDEV89XErbwnDHGFFQufQTOuWigK7CgkIfPcM4td8796JzrWMTxdzrnYp1zsQkJCccdh12TwBhjCvJ5InDO1QImAQ+ISP4ieAnQQkROA8YB3xZ2DhGZKCLdRaR7VFTUccdi1yQwxpiCfJoInHMhaBL4WES+zv+4iCSLyCHv71OBEOdcPV/FY01DxhhTkC9HDTngHWCtiLxUxD4NvfvhnOvpjWefr2KypiFjjCnIl6OGzgRuBFY655Z5tz0KNAcQkTeAa4B7nHOZwBHgehHfjfK3piFjjCnIZ4lAROYA7hj7jAfG+yqG/OwqZcYYU1BAzSwOCoKaNa1GYIwxuQVUIgBbeM4YY/ILyERgTUPGGJMj4BKBXZPAGGPyCrhEYE1DxhiTV0AmAmsaMsaYHAGXCKxpyBhj8gq4RGA1AmOMySvgEkFYGKSkQGamvyMxxpiKIeASga03ZIwxeVkiMMaYABdwicAWnjPGmLwCLhHYNQmMMSavgE0E1jRkjDEq4BKBNQ0ZY0xeAZcIrEZgjDF5BVwisBqBMcbkFXCJoHp1CA62RGCMMdkCLhE4Z8tMGGNMbgGXCMAWnjPGmNwCMhHYNQmMMSZHwCYCaxoyxhgVkInAmoaMMSZHQCYCqxEYY0yOgE0EViMwxhgVkIkgu2lIxN+RGGOM/wVkIqhTB7Ky4MgRf0dijDH+F5CJwJaZMMaYHAGZCOyaBMYYk8NnicA518w597Nzbo1zbrVz7v5C9nHOudeccxudcyucc6f7Kp7cGjfWn/Hx5fFsxhhTsfmyRpAJPCQiMUBvYJhzLibfPpcCbb23O4EJPoznqI4d9eeqVeXxbMYYU7H5LBGIyE4RWeL9/SCwFmiSb7crgA9FzQfCnXONfBVTtqgoaNAAVq/29TMZY0zFVy59BM65aKArsCDfQ02Arbnub6NgsvCJTp2sRmCMMVAOicA5VwuYBDwgIsc1n9c5d6dzLtY5F5uQkFAmcXXqpDUCj6dMTmeMMSctnyYC51wImgQ+FpGvC9llO9As1/2m3m15iMhEEekuIt2joqLKJLZOneDwYdi8uUxOZ4wxJy1fjhpywDvAWhF5qYjdJgM3eUcP9QaSRGSnr2LKrVMn/WnNQ8aYQBfsw3OfCdwIrHTOLfNuexRoDiAibwBTgf7ARuAwcKsP48kjxjt+afVqGDiwvJ7VGGMqHp8lAhGZA7hj7CPAMF/FUJywMGjRwmoExhgTkDOLs3XsaInAGGMCOhF06gTr1kFGhr8jMcYY/wn4RJCeDhs3+jsSY4zxn4BPBGDNQ8aYwBbQiaB9e6hSxZaaMMYEtoBOBNWrQ5s2ViMwxgS2gE4EYGsOGWOMJYJO8McfkJrq70iMMcY/LBF00oXnVqzwdyTGGOMfAZ8IzjsPnIOpU/0diTHG+EfAJ4KoKOjTB77/3t+RGGOMfwR8IgBddG7JEti2rfDHN26Enj2LftwYY05mlgiAAQP0Z1G1grffhkWLYNq08ovJGGPKyzETgXNusHOutvf3x51zXzvnTvd9aOWnfXudT1BYIhCBzz7T3xfkv9CmMcZUAiWpETwhIgedc2cBF6EXm5ng27DKl3PaPDRzJhw6lPexBQsgPh5CQy0RGGMqp5Ikgizvz8uAiSLyA1DVdyH5x4ABugDd//6Xd/vnn0PVqnDPPboURf5EYYwxJ7uSJILtzrk3geuAqc650BIed1I580yIiMjbPJSVpYmgf3+45BKdbxAb678YjTHGF0pSoF8L/AT0FZFEoC7wd18G5Q8hIVrgT5miCQBgzhzYuROuv15HDYE1DxljKp+SJIJGwA8i8odz7jxgMLDQl0H5y5VXwt69cMMNsH+/dhLXqAGXXw6RkdC6tSUCY0zlU5JEMAnIcs61ASYCzYBPfBqVnwwaBE89BZMmwamnaiIYOBBq1tTHe/WyRGCMqXxKkgg8IpIJDALGicjf0VpCpVOlCjz+OMyfD3XqQGIiDBmS83ivXrBjB2zf7rcQjTGmzJUkEWQ454YANwFTvNtCfBeS/3XrBosXw/TpOZPNQBMBWK3AGFO5lCQR3AqcAfxbRP50zrUEPvJtWP5XvTpcdJHOMcjWpYsOJbVEYIypTI6ZCERkDfAwsNI51wnYJiJjfB5ZBRQaqsnAEoExpjIpyRIT5wF/AK8D/wE2OOfO8W1YFVevXjqXIHuIqTHGnOxK0jT0InCJiJwrIucAfYGXfRtWxdWzJ6SkwHPPweHD/o7GGGNOXEkSQYiIrM++IyIbqOSdxcW54grtO3j8cWjVCl58sehlJ9LS4OmnoVEj+PXX8o3TGGNKqiSJINY597Zz7jzv7S0gYBdaqF1bRxP99pte5vLhh6F5c3jsMdi1S699vGcP/PQTnHYaPPEEJCXBAw/oEhXZsrcdb3+DSFm8GmOMKVkiuAdYA4zw3tYAd/syqJPB2WfDjBnw++9w/vnw7LP6zb96dWjQAPr1g4wM+PFHeOcdWLYMPvFOwxPRRexefVWvjvboo1p7KKk//4S2beGLL3zy0owxAcbJcXy1dM7NFZEzfRDPMXXv3l1iK+DKb3/8oTORg4MhLAzq1dNZydWra02gZ09ISID163Uhu1tugZEjtfbw7rs6k3niROjdu/jnEYFLL9UaR/36sGGDTn4zxpjiOOcWi0j3Qh8UkVLfgK0l2OddYA+wqojHzwOSgGXe2/+V5Lm7desmJ6NZs0RA5J57RGrWFDnnHJHMTH1syhSRxo318ZtuEtmxo+jzfPqp7nfHHSLOiTz4YPnEb4w5uQGxUkS5erzLSZekGvE+0O8Y+8wWkS7e25PHGctJ4fzzdXXTCRN0Utp//wtBQfrYZZdpTWHUKK1VtGsHkycXPMeBA9qv0L07vPEG/PWvMG4crFmjj+/bB2PH6nUTjDGmpIpMBM65QUXcrgaqH+vEIvIbsL8sgz3ZPf88tGgB770HzZrlfaxWLXjmGS3EO3SAa66B777Lu8/Ikbo66sSJmkT+/W/tvL7vPhgzRldH/fvf4fTT9VyZmb55HRs2wEMPQVQUjBhhHdfGnPSKqioA7xV3K+q4fOeIpvimoX3AcuBHoGMx57kTHakU27x5c59Wn3zN4zn2PomJIr16iQQHi3z5pd7OOEObhB56KO++48frdhC57DKRX38VufZavd+tm8js2Xn3P3xYz7dnT+nizsgQmTRJ5MIL9dzBwSLdu+vvTz9dunMVJzOzZO+RMaZ0KKZp6Lj6CEp6O0YiCANqeX/vD/xRknOerH0EpZWUJNK7d04h36qVyKuviqSn590vI0PkmWdEfv457/YvvxRp0ECPPesska++Ehk5UqRuXd3Wvr3Irl3HjiM9XeTZZ0WaNNHjmjUTeeop7cfweET+8hfd/v77J/6aExNFYmJEhgyxZGBMWauQiaCQfTcD9Y61X6AkAhFNBg8/LPLNNzkdy6WRkqLJo1kz/aSrVBG56iqRN94QqV5dpFMnkYSEoo/fsCHnW3/fviLffVcwjrQ0kYsu0hrCM8+IrF9f+jhFtOC/7rqcxFcWicUYk6O4RHBcw0dLyjkXDUwRkU6FPNYQ2C0i4pzrCXwFtJBjBFRRh49WZOnpOuchJgaio3XbzJl65bX27eHee3UYa0KCXrIzPFznQLzwAlSrBm+9pRftKUpyss64/uUXvd+uHfzznzB0aMljfPtt7fx+6imdsLd0KaxcqX0qxpgTV9zwUZ8lAufcp2g/QD1gN/BPvEtTiMgbzrnh6GS1TOAI8DcRmXes81oiKDvTpmkBnp6u92vX1g7mI0f0/oUXwgcfQJMmJTtffLxe8/n993Vhvuef15nXuZfyLszq1dCjB5x5ps6PiI+Hzp11dNTMmXrBIGPMiTnhROCc64M28wRnbxORD8sqwNKwRFC29u7Vgj8qSr/9gyaGQ4cgIuLYhXhh0tLg5pt14tzf/qY1iypVtNFn9WqdET1pkl7pLT1dl+WIioLly6FhQz3Hu+/C7bfrRLzQUD3nKafo9aQvvFAn7hljSu6EEoFz7iOgNTrpK3vxZRGREWUZZElZIjg5eDw652HcOJ03ERqqQ14TEzUpnHOOfuuvWlVv11+vs6uziehw2dmz9diQEFi4UNdoatBAm7QOHtSE1aqVJp4rrtCZ3Mcbb3p6TjI8lkOH9HXUqHF8z+dL27bBokVw1VX+jsRUJCeaCNYCMcdquy8vlghOHiK6vtLKlVrIpqdrP8XVV2thXlqpqTB1Knz6qfZp1K4NNWvqek9bt+pSG0OHwvDh+jwltWgRXHcd7N6t8zduvVUTVf4mqdmzdd2oRYtg7Vqd+/G3v8GDD1acZT4yM/WaGUuW6OVWTz/d3xGZiuJEE8GXwAgR2emL4ErLEoHJz+PRjur339dmp7Q0bT4aPlxnbYcUsWi6CLzyCvzjH7pg4EUXwZdfak2jRw/45puc/pHs/pRatXQ9qB49YNUqbeKKiNDO9KpVtSmtc2e4887SN6ulpelz16t3/O/Fc8/pDPWQEBg8GD7++PjPZSqXE00EPwNdgIXA0TUyRWRgGcZYYpYITHESEnQE0n/+o00kDRvCbbdBx46wcSNs2gQ7d+pyHXv2wJYtWsC/+y7UrasXG/r0U23WCguD77/XpTsGDNAZ3zNn6n7Zli6F0aO1ViKiV647cEBrFu+9p4mjJKZPh7vv1tj+8x9dlLC01q6Frl011ubNdXXbP/8sOIs9v6wsfd6mTUv/nObkcaKJ4NzCtouIXy61YonAlERmpi4BPnGiNidlXwuiaVP9lh8Robfzz4c77ij47X3FCi1Q9+7VY9u1g1mzIDKy+OcV0fWeRo7U5DNunCaX7OST3URWpYqeKzJSk83HH+vS4g0bahPUTTfB669rIsn+Fy2uhpGVBWedpavgrl6ttYtWrTShjR1b/HGDB+vaVnPmHHv1W3Py8svwUV+xRGBKa8cO7aRu2bJ0ncm7dmkhmZKiw1qjokp+7E8/aQd4YuKx9w0J0eac7Cadp56CJ5/UTnIRLdRr1tTmqN69dfTUkSMaV0KCFv5r18K6dZpQsudvDB2qw3mz+0/yE+91Md58U/tbGjfWGs7xdribiu1EawS9gXFAB6AqEASkiEhYWQdaEpYITHnKnut8PHMZskfvNGyot8jInBFQmZmwf782O0VEaCGc22+/aR9FSIiOZDpwAObP1wsc5V5MMCREv/m3bQsXXKA1gOyaw+LFOhdj7FhdHHDLFj1P48baWf/009qsNXKk9qlcfLEuJpi/BpGWpjWaHTs0Oe7apeepVk1HTUVEaG0ku2kpMxPmztUENWSIJjHjfyeaCGKB64Evge7ATUA7ERlV1oGWhCUCE8iOHNECuWbNnFtxSer887VQzsrKe6nU4GAtsG+5RftHnNM+iokTNQllZOgIqWnTNFmVRPv2evv1V00UoAnqo490JBNozWXJEm12CzvBr5Iix+6QF9HkOWWKvq6ICJ2t3rq19uOcSMf8yeaEE4GIdHfOrRCRzt5tS0Wkqw9iPSZLBMaU3OLFMH68dhi3aqUF4Y4dWlupVUtnfmePqjp4UOdybN2qSaNOHZ2L0KaN1iIaNtTRVQ0b6nnS07X/Y8cO7T+ZMUObp849Vwv6GjXgrrv08Rtv1FhWrtTnqlsXHnkEhg3Tx+fN076NqCitWURFaS0kPl5/BgVpbQp0GfRVq7RmMmyY1moKq3X89pvOL9m8OWc015Ejes7s5rZhw/Q9KGmz365d2mwXHKzJpE0b7aDPX6OriE40EfwGXAS8DewCdgK3iMhpZR1oSVgiMMZ3fv9dm4YGDdLbifYXJCVps9RHH+n1ua+7Djp10ueYOlUL+CzvNNXsWkp+kZGamFJT9WebNnoO0NnrLVpox3r//jk1hA8/1EEALVtq01f//jlzVzwevZjTs8/qCLHq1bVZ7MIL4bzzNKYDB7QPpnv3nFFi8+frHJh9+3So8MGDOTE2baqz4CMitIZWtSpceaWe83iGEQcFlf3s+RNNBC3QtYKqAg8CdYD/iMjGsg2zZCwRGHPyycwsWLD9/rvO24iJ0STRvr3O2N62TUdrNWyow2CLm+09e7bO2Vi3Tr+VX3yx1kQmTNA+k6++0sK5KGvXwmuvwf/+B3FxBR8PDtb5JaedBi+9pAX+N99o7SIhQWsnsbE6633xYo3f49Gfhw5p38m//qVNdCVJCD/+qMnSOa1ZXXCBLqtSmoEKRSmLtYaqA81FZP2Jh3NiLBEYY3JLS9MZ7D/9pPMx9u/XlWxff73oyYSFiYvT5BQcrMkjJETP+cUXOh+jXz8dlZV7HklxMb39tl4pcMcOrZn06wd9+2rNxOPR/ov27XOGJL/1lo7i6txZR4jNmqVzX6Ki9LErrji+9yfbidYIBgBjgaoi0tI51wV40iaUGWMqmqwsnSjYsOHxLZhYGBFdILFx49KPHktN1eQxebJORkxJyft4dt9Fy5bw7bdw6aWaeLInIq5YoR36S5fqxMiXXz7+TvYTTQSLgQuAX7I7iJ1zK0Xk1GIP9BFLBMaYk1Famg4nzl6wMCtLR1D98os2Lw0Zos1U+ZvQ0tO1eem557Sm88Ybx/f8J5oI5otI79wjhXKPICpvlgiMMYFo3jztKK9f//iOLy4RlKRferVzbigQ5JxrC4wAjnkBGWOMMWWnTx/fnbskLV73AR3RBec+BZKBB3wXkjHGmPJ0zBqBiBwGHvPejDHGVDJFJgLn3OTiDvTXqCFjjDFlq7gawRnAVrQ5aAFQRoOxjDHGVCTFJYKGwMXAEGAo8APwqYisLo/AjDHGlI8iO4tFJEtEponIzUBvYCPwi3NueLlFZ4wxxueK7Sx2zoUCl6G1gmjgNeAb34dljDGmvBTXWfwh0AmYCvxLRFaVW1TGGGPKTXE1gr8AKcD9wAiXs3CHA8RfVygzxhhTtopMBCJyHBfnM8YYc7Kxwt4YYwKcJQJjjAlwlgiMMSbAWSIwxpgA57NE4Jx71zm3xzlX6LBTp15zzm10zq1wzp3uq1iMMcYUzZc1gveBfsU8finQ1nu7E5jgw1iMMcYUwWeJQER+A/YXs8sVwIei5gPhzrlGvorHGGNM4fzZR9AEXd002zbvtgKcc3c652Kdc7EJCQnlEpwxxgSKk6KzWEQmikh3EekeFRXl73CMMaZS8Wci2A40y3W/qXebMcaYcuTPRDAZuMk7eqg3kCQiO/0YjzHGBKRjXrP4eDnnPgXOA+o557YB/wRCAETkDXRV0/7odQ4OA7f6KhZjjDFF81kiEJEhx3hcgGG+en5jjDElc1J0FhtjjPEdSwTGGBPgLBEYY0yAs0RgjDEBzhKBMcYEuIBJBBv3b+TFeS9yKP2Qv0MxxpgKJWASwcrdK3l4+sOsTVjr71CMMaZCCZhE0L5eewDW7V3n50iMMaZiCZhE0KZuG4KrBLN2r9UIjDEmt4BJBCFBIbSp28YSgTHG5BMwiQC0eciahowxJq+ASgQd6nVg4/6NZGRl+DsUY4ypMAIuEWR6Mtm4f6O/QzHGmAojoBKBjRwyxpiCAjIRWIexMcbkCKhEUDu0Nk3DmlqNwBhjcgmoRADaT2A1AmOMyRFwiSB7CKleIM0YY0zAJYIO9TpwKP0Q2w9u93coxhhTIQReIojqAGCLzxljjFfAJQIbOWSMMXkFXCJoULMB4dXCbeSQMcZ4BVwicM7ZyCFjjMkl4BIBaPOQ9REYY3xtc+JmYl6P4c7v72Tf4X3+DqdIAZkIOtTrwO6U3Rw4csDfoRhT6Rw4coD/+/n/6PFWD2bGzfR3OH5zMO0gAz4dwJakLby79F1OGX8K7y59F494jnns1qStPDrzUeq/UJ+qT1Wl9rO1qfd8PZ7+7WmfxBrsk7NWcJ3qdwJgxe4VnBt9rp+jMSejyesnsy15G/f2uLfEx3jEQxVXvt+90jLT2LBvA/O2zmPu1rkkpiYyrMcwLml9Cc65Ep9n8Y7FzN4ym/t63kdQlaBC90lJT+HF31/kpd9fIiktiQY1G3DxRxfzyJmP8NT5TxESFFLq+LM8WaRnpVM9pHqpjktMTeTD5R/y48YfueKUK7it621UDapa4uMnrZnEiGkjGBwzmKcveJpaVWvleXxz4ma+WfsN0+Om0y6yHQPaDeCcFuccfY1ZniyGfj2UtQlrmfaXaTSo2YB7friH2yffzv/9/H8MjhnM4I6DiagWwb4j+9h7eC9bkrawOXEza/euZfqm6QjCgHYDiImKITUzlbTMtKNlV1lzJ9vEqu7du0tsbOwJnWPf4X3Ue6Eez134HP846x9lFJkJFOv3rqfLm11IzUzlwys/5MbTbix2/6TUJIZ+PZT1e9czechkYqJijj62bNcyZsbNpE61OtStXpe2ddtyaoNTSxVPpieTr9d+zfRN00lOTyY5LZn9R/azJWkLuw7tOrpf/Zr1Ca4SzI6DO+jVpBc3dr6R1Qmrmb9tPvuP7OfB3g9yd/e7CQ0OPXrMtuRtPDbrMT5c/iEA4y8dz7CewwrEMG3jNO6ecjfxSfFc2f5KRp87mraRbXlw2oNMXDKRmKgYWtRpQYYng9CgUG7tcitXtr+yQFLJyMrgjdg3eH3R6+xO2U1SahIAF7e+mDu63sHAUwaybu86psdNJ3ZHLM3CmtGxfkdaR7Qm4XAC8YnxLN+9nC/XfMnhjMM0rt2YHQd3EB0ezRPnPEHf1n1pXLsxzjlEhLgDcWzYt4E2ddvQpm4bMj2ZjJo5ihd/f5GW4S35M/FPosOjGX/peKq4Ksz6cxbT46azfPdyANpFtiM+MZ60rDTCQsPo1qgbnep3Yt+RfXyy8hNe7//60S8LHvEwac0k/rvyv0zbOI30rPQC72ONkBq0DG9J/7b9ubfHvUSHR5fqb6E4zrnFItK90McCMREAtBvXjo71O/LNdd+UQVTGH0SE0b+Mpm71utzf+/5yec5MTyZnv3c26/eup0NUB5bsXMK82+bRtVHXQvffnLiZyz+5nPX71hNeLZxMTybfXvctfZr14ZnZz/DUb0+RJVl5junRuAd3d7+b6ztdT42QGgXO6REPew/vZXvydmbEzWDcwnFsTd5K3ep1iaoRRe3Q2kRUi6BZWDOa12lO67qtOaPpGbSKaEV6VjrvL3ufZ+Y8w5akLdSqWoteTXqR4cngt/jfiA6P5q5ud7Hz4E5WJ6xm3tZ5eMTDg70fZNGORSzasYh1w9bRqHYjQL95D586nI9Xfkz7eu15a8BbnNX8rDzxfrXmK8bOG4tHPIQEhbDj4A42J26mXWQ7RvQcQdvItoRXC2fHwR2MmjmKdXvXcXbzs+nSsAsR1SJIy0rjk5WfsDV5K0Eu6Oj71bxOc3Yf2k1aVlqe56sTWofBMYO5p8c9dG3YlWkbp/H4z4+zZOcSAMJCw44W8slpyUePq1ejHnWr12XDvg0M6zGMFy95kYXbF3LH93ewYd8GAKoGVaV3094MaDeAQR0G0SqiFSnpKcyIm8HUP6aybPcyVu9ZTUpGCiN6juDVS18t9O8iKTWJnzb9hEc8RFaPJLJGJE3DmhJVI6pUNbXSsERQiJu/vZmfNv7Ezod2+uyNN2UnPjGeWlVrEVkj8ui2iYsncteUuwB4Z+A73Nb1tuM+//4j+0lMTaRVRKti93tuznOMmjmKT6/+lAtaXsDpb55OSFAIi+9cTN3qdQH9Vrt271pid8QyauYo0rPS+frar2kZ0ZL+H/dn04FNtItsx6o9q/hL578w5qIxZHmy2H9kP7O3zGZC7ATWJKwhLDSMIZ2GcFvX22hbty1fr/2aT1Z9wpwtc/J8mzwv+jwe7P0gl7W9rMhmm/zSs9KJT4ynVUQrgqoEISJMj5vOyBkjWbprKTVDahITFUP3xt155MxHiA6P5o99f3DqhFO5qsNVfHr1p+w4uIO+/+3L+r3refTsRxl11qg8tYmiZHmy+Hrt1zw397mjhXO2tnXb8uIlL3J5u8vz/F9mebKYHjednzb+RJeGXbio1UU0CWtClieLuANxxB2Io37N+rQIb0FEtYgC/9Miwtytc1m+azlrEtYQlxhHy/CWdG3YlVPqncL6veuZt20e6/auY3iP4dzQ+Yajx6ZmpvL5qs9pEtaEPs36FJqcc/OIh/1H9lOvRr2SfBTlxhJBISYsmsC9U+8lbkQcLSNalkFkJlumJ5PgKsV3P3299muW7lyKRzx4xENE9QhahrckOjyajvU7Hv1ny/Rk8sLcFxj9q37zn3TtJPo068OSnUvo806fo308s/6cxY83/MhFrS7K8zwiwvLdy5kdP5sF2xewcPtCWoS34JW+r9CxfkdAv7H+9fu/kpiaSJu6bejfpj+XtbuMc1ucm6dgi90RS593+nBF+yv44povcM6xYNsCznn/HOqE1qFm1ZpkZGWQcDjhaEHdtm5bJg+ZfHQi44EjBxj0xSBW7l7JG5e/wTUx1xR4b0SEOVvm8NaSt/hqzVccyTxCFVcFj3hoU7cNA9oNoGV4S5qENSEmKuboucuCRzzsSdlD/Zr1C+3P+Ncv/2L0r6P5T///MGbuGPYd2ce3133Lha0uLPVziQjr961n3+F9JKYm4hEPfdv0LVVbvik5vyUC51w/4FUgCHhbRJ7L9/gtwAtA9sI/40Xk7eLOWVaJYNmuZXR9syufDPqEIacOOeHzVVYiwqS1k2hbty2nNTytyP2yPFnMiJvBe8ve49t139KtcTc+HvRxgTZOEeGp357in7/8E4AgF0QVV4UMT87lQ6sFV+PClhfSt3VfPlrxEYt2LOKq9lexYvcKtiRtYewlY3l1waukZ6Wz5M4lVA2qylnvncWWpC1MuGwCoCM2YnfEMnXjVHYc3AFAo1qN6NGkB3O2zCE5LZmHz3iYvYf38vbSt+nZpCdDOw3lf3H/Y9afs0jNTKVW1Vr0a9OP8NBwfon/hY37N1K/Zn1W3bOKqJpRR+OdsmEKH6/8mKpBVQmpEkJk9Ui6NOxCl4ZdaBfZrsC3dBEhw5NRogIvKTWJz1Z9xpakLVzV4Sq6Nerm1xpsamYqnSd05o/9fxBVI4ofb/iRbo27+S0eU3LFJQJExCc3tPDfBLQCqgLLgZh8+9yCFv4lPm+3bt2kLGRkZUjNf9eU+6beVybnq6z+9cu/hNEIo5EbJt0gcfvj8jyenJosL817SZq/3FwYjdQdU1du/fZWCXs2TOo8W0e+WPXF0X09Ho88/NPDwmjkpm9ukoysjKOPJaUmyfJdy+XrNV/LiKkjpOUrLYXRSOSYSPl81eciIrLv8D65+MOLhdFI8JPBMm/LvKPHxyfGS6OxjY7Gymgk7NkwGfzFYHlv6XuyJXGLeDweERFJSEmQW769RRiNuNFORs0YJemZ6UfPdTj9sExZP0Xu+v4uafxiY6nzbB0Z+OlAefn3lyU+Md4n7/PJZO6WudLvv/1k/d71/g7FlAIQK0WUqz6rETjnzgBGi0hf7/1R3sTzbK59bgG6i8jwkp63rGoEAOd/cD4p6Sks/OvCMjmfP4kIU/+YyoTYCUTWiKR3k970btqb0xqelqeKv2L3Ckb/MpoGNRtwVYerOC/6vCK/mY6ZM4aRM0dyY+cbaRrWlFfmv0KmJ5PTGp5G/Zr1CQsN48c/fiQpLYlzW5zL8J7DGdBuAKHBofx54E+GTBrCgu0LaB3Rmoa1GgIwd+tc7u1+L+P6jyt2KKWIsHH/RqJqRhFeLfzo9uymolYRrbiu03V5jklMTWTT/k3UqlqL2qG1iaoRVeyQxfnb5gPQu2nvYuMArB/JnPT80jTknLsG6Ccid3jv3wj0yl3oexPBs0ACsAF4UES2FnKuO4E7AZo3b94tPj6+TGIcNWMUY38fS/LI5FKPU/anGXEzuH3y7URUi+DMZmcSExXDe8veY/HOxTQNa0p6Vjp7UvYAOrxteI/hDDl1CK8teI1n5zxLWGgYaZlppGSkEBYaRlSNKLIkC494aBbWjE71OxFcJZjXF73OkE5D+OiqjwiqEsSOgzt4cd6LrN27lj0pe0g4nECvJr14uM/D9GzSs0CcGVkZvDL/FRbvXMzulN0kpCRwbcdreeKcJ6xgNaacVeREEAkcEpE059xdwHUickFx5y3LGsHk9ZO54rMrmHPrHM5sfmaZnNPX3ox9k2FTh9Eush1Nwpowf9t8DqUfolVEKx47+zFu7HwjwVWCiU+K55fNv/BG7Bss2L7g6PE3dr6Rl/u+TI2QGkyPm87UP6ZyMP0gQU7bsTcnbmblnpUkpiYyOGYwn1z9yTE7fo0xFV9xicCX/+HbgWa57jclp1MYABHJvfjG28DzPoyngOwmgd+3/V7hE0F8Yjwv/v4i4xaO49I2l/LZNZ8RFhpGpieTuAM6FC53M0h0eDS3dLmFW7rcwsLtC/li9Rdc1Ooi+rXpd3SfgacMZOApAws8l4hwIPXA0eGQxpjKzZeJYBHQ1jnXEk0A1wNDc+/gnGskIju9dwcC5boSXP2a9WkV0Yrft/1eov0zsjIYM3cM4xaO476e9/HImY8U2r5+MO0gExdPJLxaODFRMcRExVCnWp08+3jEw6Lti4isEUnL8JYEVQkiLTONZbuWsWTnEg6mHyQtM42ktCSmx01nxe4VANzX8z5e6vvS0W/pwVWCaRfZrti4ezbpWWjTTVGcc5YEjAkgPksEIpLpnBsO/ISOIHpXRFY7555Ee68nAyOccwOBTGA/OoqoXPVu2puf//wZEeFwxmEOpB6gUa1GBYb8Ld25lNsm38ayXcs4tf6pPPHzE3y++nPeHvA2vZr2OrpfUmoSl358aZ7k4nBc3u5y7u91P+e3PJ8pG6bwxM9PHC3cqwVXIzo8mj8P/FlglmRwlWD6NOvD2IvHMuCUAccs9I0xprQCdkJZtvELx3Pfj/cRFhp2dLp5jZAadIzqSOu6rdl5cCebDmxiW/I2GtRswITLJnBVh6uYsmEK9/xwD9uTtzO442AeP/txmoY1pd/H/ViycwmfDPqEbo27sSZhDfO2zuPtJW+TcDiByOqR7DuyjzZ12zDyzJE451i9ZzWbDmyibd229G7amx5NehBZPZLQ4FCCXJB1rBpjTpjNLC7GrkO7+MeMfxBWNYwmYU2oE1qH9fvWs3LPSuIOxNG4dmNaR7Smfb323N397jxNJslpyTw35znGLxzPwfSDNKjZgP1H9vPVtV8VaHtPzUzls1Wf8d367xjQbgA3nXaTdcIaY8qNJQIf239kP+MWjOPz1Z8z9pKx9G/b398hGWNMHpYIAHbvhh9+gJtvhqCSLcxljDGVRXGJIHCuUPbLL3D77bBokb8jMcaYCiVwEsFFF4FzMG2avyMxxpgKJXASQWQk9OwJP/3k70iMMaZCCZxEANCvHyxcCPv2HXtfY4wJEIGVCPr2BY8HZszIuz0uDg4f9k9MpmSWLYM///R3FMZUSoGVCHr0gIiIvM1DH30ErVtDrVrQsiX07w9PPAGTJ8OuXUWfy5SfhAQ491y49lo4yUa5GXMyCKwZTcHBcPHF2mEsAmlp8Nhj0LkzXHMNrFsHq1bB//4HWVnauXzDDfDUUxAdnXOehAT44w/YtEmTxamnwhlnQJ06+tisWXqeW2+FVkVcA/fIET2+QwcbznosTz4JyckQGwsLFkDvoq8fYIw5DkVdsaai3k74CmXvvCMCIsuXi7z4ov4+Y0befVJSRObOFfn730WqVROpWlXk1ltFrrhCpEkTPSb/zTmRFi3ybqtdW+SDD0Q8Hr3Nni0yfLjI6aeLBAfrPu3aiXz4oUhGztW6JC1NZOdOkTVrRBYs0N+9V9fymeRkkTFjRBYt8u3zlNa6dfpe3XijSJ06IkOG+DsiY05K+OMKZb5ywjOLt2+Hpk21JjBhAnTvXvxIom3bYPRo+PBD/XbfvTucfjqccoo2KdWvD0uXwty5Wgvo0kWHqtarB7fcArNnwyWXaD/Exo1Qo4bWHnr2hObN4Y03YPlyaNFCm6d27oT9+wvGUbcutG+vx4PWIq68Up+jWrXjfz9E4NtvYcQIfa3BwTBmDDz4oNaIEhO1BtW1q77m8nbllVrD2rgRnnsOxo2D+Hho3Nh3z7liBWRkQDe7Fq+pPGxmcX6dO8Pq1dpxvGSJFnLHIqIFY2lkZWmh+swz2j9x660waJAW+Nk8Hvj+e5g4EUJDoVEjaNhQh7tGREDNmrB5s8a7fj2kp+txBw5oU1ajRlqIBwVpAbZqFezdq00phw5B7drQoAFERelzpaRox3hoqD6WkQGLF+t78sILmpi++QYuvVQf/+47bUILCYGHHoLHH9eYipOSosmpuCavAwe0mWfxYo27b1+47ba8+/z6K5x3Hvz73/Doo9qU1ratxvDkk6X7LEoiLU2T/vPP62f92mtw771l/zwejz5XdT9eFW/3bv0C07dv6f+uzUnJEkF+jzyihd7118Onn5ZNYMU5niRSknPOmqWF5M8/67amTbW/omFDCAvTAvvgQdizR29BQbqtRg0tiJKTNSlcfbUmk+BgPe/rr2uhX7s2DB2qyeuDD+D996FZM7jzTujTR2s1Bw9qgbJ0qY7sWbZMv72Hh8P558MFF0BMjCa28HCtOX3yidbCMjM17qgo7VvJLvBBk8M112hfyvr1OTWhgQNh/nzYulWTWWkcPqzPHRysr6tqrmtJzJ4Nd98Na9ZoQspekmTYMHjlFT2mKHv36nsbEZF3e1aWvsfZ20U0sf7jH5rUzjhDC+KePfW1BAfre9G2beF/LyL63m7frsm9cWP9nEvzt5WVpTXhxx+HpCTtM3vrLa2R+prHA88+C+PH64CMu++GKt7xKvv3699x797QpInvYzmW5GSN8y9/0Zp7JWCJIL9ly7QTePJkbd452W3apIVsZGTZnfPgQS2ccheWc+Zogli4sPBjWrfWprHOnWHLFpg5U2sz+TVrBkOG6LyO00/X5HTrrfDf/8LIkdoM9vjjWoB+/rmOGMo2Y4YWXsOHawd+crLWapzTW61aWpjWr6+FZLVqWpv54Qf9hr93r54nOloLo/BwGDsWfv9dE+lbb2lcWVkay9ixWuhGRGgyql9fX9+pp2pB+tVX8Ntv+th772nyAli7Vn9fswbatYMzz9RCfPZsbeK7/HIt+BYvLvj+NGyoNaFOnbR2lZys7+P8+QXnwDRsqAXqPfdobBkZWss9fFjPkTtJLF6sSXzJEm2+vOSSnJrVAw/oZ75hA6Sm6n6DB+fU6tLS9LXs36/NhYcP6/sbEaFJ5FiJZM8eLVSnT9cm1rg4jeG55+DLL/XLx6FDum/v3vre3XVX3tpzbpmZWiOOji56n5I4eFC/hLRokfNa16/XJsl16/SzmzNH/6ZKSkQ/rxYtchJdaRT3xfEEvlRaIjBlK7tZZ+FCHSnVtSucdpr+nt/mzXrbt08LkVNOgbPOKvgP4vFoM8ybb+r9QYO0mSr/P6CIPt/y5TnbsmsyInqeolx2mdYGDx/WJJD9d9SqFfztb9rfkr/Z64svYOpUPSYlRb+Nr1mjBS7oqK9BgzTpzZ8PDz+sSeKee7SAuvtu/eIxZ44mpNGj4Y47cmoYe/ZowZOZqbf4eE0QP/+s/UXBwVrgNmwIvXppLaJVKz1uxw593h9/1KSd/b4cOaLnvuACLWBbtdKRb88+q8nilVe0kHdOP5s77tDz1KihBd+hQ5q02rTR/RYv1gSWfd78spvRhg/P2ZaZqet7rVqlSXHyZE0gr72mz/fWW/qep6To38J11+laYAsWwKRJmqyaNtVYBw3S59izR5Pud99pYj9wQI/t0EHfm3vvzduvk5CgSWb3bk3aiYl6zIED+re4bZtuB21ivfpq6NhRa2xVq2o/4qhRmvhnzdK/jd27Nfmnp+v7Vb267hsUpF8efvtNY9uyRRPfBx8UngwOHNDPffNmrd3Gx+f0I27ZojXf11/P+fuPi9N+u+uv1y9Rx6G4ROD3UUClvZ3wqCFTcXk8Ii+/LPLJJ8WPkkpJEdm2TUc6ZWUVfGzzZpGFC0VmzhT54QeRSZNEVq8u+Fw//ijy3XcimZmlizM9XWTlSh3RlC01VeTee3NGjJ1zjsj27XmfrzQjvzwekcOHS3bM2rUid98t0ru3yIgRIl98ITJ+vI6yCgkRad1aY7r5ZpH9+wt/rr17c54rK0vfs27d9LiOHXPO+8svOuJuwwaR2FgdcTdwoO43erSeY+5ckc6dc96LunVFLrxQj8stLk7k+edF1q8vGFPuc/TqJdKmTd7z3XSTyHvvifzznyL9+4uEheljAwboZ37XXTriL/uYsDCRZs30nOeeK3LllSLDhulIuTfeEBk0KGf/008XiY/XOL79VqRKFZGLLhK57jp9PwsbNZh9q1lTRxfedpve//vfc17TqlUil10mUr9+wePq1BHp2lXk2mtF7rxTRyrWry/y5Zcijz8uEhqq537zzWP/PRSBYkYN+b1gL+3NEoGp0D77TAu33MOB/WXXLi0wW7cW+f770h/v8YgcPHjs/TIyNMlkF9og0rSpJvTdu49/6HNGhsirr4rExGjhOmaMJojC3tukJJGnnxaJiNDnDw0V+etftfDN/2WhKMnJmugOH867/c039Zzh4SIPPKDDuhMTRXbsENm4URPxypWa6FJT9RiPRxMN6N/D6NGaRCIjRW6/XeSFF0QmT9b4kpIKxrJ8ed5kOnSofvk5AcUlAmsaMsacOI9Hm1ReeQXuv1+bwE6k7f54JSVpP8TZZ2vfTllZuVL7wLIHLZREVpY2eU2apPeHDtX3p6T9Denp2lTatas2p54g6yMwxpSP1NQTm9dS2aSman/UeedpH5UfFZcIAmuJCWOMb1kSyKtaNR2qXsEF1qJzxhhjCrBEYIwxAc4SgTHGBDhLBMYYE+AsERhjTICzRGCMMQHOEoExxgQ4SwTGGBPgTrqZxc65BCD+OA+vB+wtw3BOBoH2mu31Vm6B9nqh7F5zCxEpdH2Lky4RnAjnXGxRU6wrq0B7zfZ6K7dAe71QPq/ZmoaMMSbAWSIwxpgAF2iJYKK/A/CDQHvN9nort0B7vVAOrzmg+giMMcYUFGg1AmOMMflYIjDGmAAXMInAOdfPObfeObfROTfS3/GUNedcM+fcz865Nc651c65+73b6zrnpjvn/vD+jPB3rGXJORfknFvqnJvivd/SObfA+zl/7pyr6u8Yy5JzLtw595Vzbp1zbq1z7ozK/Bk75x70/j2vcs596pyrVpk+Y+fcu865Pc65Vbm2Ffp5OvWa93WvcM6dXlZxBEQicM4FAa8DlwIxwBDnXIx/oypzmcBDIhID9AaGeV/jSGCmiLQFZnrvVyb3A2tz3R8DvCwibYADwO1+icp3XgWmiUh74DT0tVfKz9g51wQYAXQXkU5AEHA9leszfh/ol29bUZ/npUBb7+1OYEJZBREQiQDoCWwUkTgRSQc+A67wc0xlSkR2isgS7+8H0QKiCfo6P/Du9gFwpV8C9AHnXFPgMuBt730HXAB85d2lsr3eOsA5wDsAIpIuIolU4s8YvZxudedcMFAD2Ekl+oxF5Ddgf77NRX2eVwAfipoPhDvnGpVFHIGSCJoAW3Pd3+bdVik556KBrsACoIGI7PQ+tAto4K+4fOAV4BHA470fCSSKSKb3fmX7nFsCCcB73uawt51zNamkn7GIbAfGAlvQBJAELKZyf8ZQ9Ofps3IsUBJBwHDO1QImAQ+ISHLux0THCleK8cLOucuBPSKy2N+xlKNg4HRggoh0BVLI1wxUyT7jCPRbcEugMVCTgs0olVp5fZ6Bkgi2A81y3W/q3VapOOdC0CTwsYh87d28O7v66P25x1/xlbEzgYHOuc1oU98FaPt5uLcZASrf57wN2CYiC7z3v0ITQ2X9jC8C/hSRBBHJAL5GP/fK/BlD0Z+nz8qxQEkEi4C23tEGVdEOp8l+jqlMedvH3wHWishLuR6aDNzs/f1m4Lvyjs0XRGSUiDQVkWj085wlIjcAPwPXeHerNK8XQER2AVudc6d4N10IrKGSfsZok1Bv51wN79939uuttJ+xV1Gf52TgJu/ood5AUq4mpBMjIgFxA/oDG4BNwGP+jscHr+8stAq5AljmvfVH281nAn8AM4C6/o7VB6/9PGCK9/dWwEJgI/AlEOrv+Mr4tXYBYr2f87dARGX+jIF/AeuAVcBHQGhl+oyBT9H+jwy0xnd7UZ8n4NDRj5uAlehoqjKJw5aYMMaYABcoTUPGGGOKYInAGGMCnCUCY4wJcJYIjDEmwFkiMMaYAGeJwJh8nHNZzrlluW5ltoibcy4690qTxlQEwcfexZiAc0REuvg7CGPKi9UIjCkh59xm59zzzrmVzrmFzrk23u3RzrlZ3jXiZzrnmnu3N3DOfeOcW+699fGeKsg595Z3nf3/Oeeq++1FGYMlAmMKUz1f09B1uR5LEpFTgfHo6qcA44APRKQz8DHwmnf7a8CvInIauibQau/2tsDrItIRSASu9umrMeYYbGaxMfk45w6JSK1Ctm8GLhCROO8Cf7tEJNI5txdoJCIZ3u07RaSecy4BaCoiabnOEQ1MF73oCM65fwAhIvJ0Obw0YwplNQJjSkeK+L000nL9noX11Rk/s0RgTOlcl+vn797f56EroALcAMz2/j4TuAeOXlu5TnkFaUxp2DcRYwqq7pxbluv+NBHJHkIa4ZxbgX6rH+Lddh961bC/o1cQu9W7/X5gonPudvSb/z3oSpPGVCjWR2BMCXn7CLqLyF5/x2JMWbKmIWOMCXBWIzDGmABnNQJjjAlwlgiMMSbAWSIwxpgAZ4nAGGMCnCUCY4wJcP8PGa47joTGBT0AAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 99/200] [Batch 18/600] [D loss: 0.215817] [GAN loss: 0.305189, NCE loss: 4.712877, Total: 14.443819] ETA: 5:47:39.16448708241"
     ]
    }
   ],
   "source": [
    "# Training Loop, adapted from https://github.com/taesungp/contrastive-unpaired-translation\n",
    "\n",
    "prev_time = time.time()\n",
    "for epoch in range(epoch, epochs):\n",
    "    total_D_loss = 0\n",
    "    total_GAN_loss = 0\n",
    "    total_NCE_loss = 0\n",
    "    total_G_loss = 0\n",
    "    for i, batch in enumerate(dataloader):\n",
    "        x, y = Variable(batch[\"a\"].type(Tensor)), Variable(batch[\"b\"].type(Tensor))\n",
    "        \n",
    "        # Adversarial ground truths\n",
    "        real = Variable(Tensor(np.ones((x.size(0), *D.output_shape))), requires_grad = False)\n",
    "        fake = Variable(Tensor(np.zeros((x.size(0), *D.output_shape))), requires_grad = False)\n",
    "        \n",
    "        # train discriminator\n",
    "        D.train()\n",
    "        optimizer_D.zero_grad()\n",
    "        # get the fake loss\n",
    "        fake_y = G(x)\n",
    "        D_fake = D(fake_y.detach())\n",
    "        loss_D_fake = criterion_GAN(D_fake, fake).mean()\n",
    "        # get the real loss\n",
    "        D_real = D(y)\n",
    "        loss_D_real = criterion_GAN(D_real, real).mean()\n",
    "        # combine loss and calculate gradients\n",
    "        loss_D = (loss_D_fake + loss_D_real) * 0.5\n",
    "        loss_D.backward()\n",
    "        optimizer_D.step()\n",
    "        \n",
    "        # train generator\n",
    "        G.train()\n",
    "        optimizer_G.zero_grad()\n",
    "        optimizer_Sampler.zero_grad()\n",
    "        # get the fake GAN loss\n",
    "        D_fake = D(fake_y)\n",
    "        loss_G_GAN = criterion_GAN(D_fake, real).mean()\n",
    "        # get the NCE loss\n",
    "        Sampler.train()\n",
    "        total_nce_loss = 0\n",
    "        for fake, real in [(fake_y, x), (y, G(y))]:\n",
    "            feat_q = G(fake_y, nce_layers, encode_only = True)\n",
    "            feat_k = G(x, nce_layers, encode_only = True)\n",
    "\n",
    "            feat_k_pool, sample_ids = Sampler(feat_k, 256, None)\n",
    "            feat_q_pool, _ = Sampler(feat_q, 256, sample_ids)\n",
    "\n",
    "            total_nce_loss = 0.0\n",
    "            for f_q, f_k, crit, nce_layer in zip(feat_q_pool, feat_k_pool, criterion_NCE, nce_layers):\n",
    "                loss = crit(f_q, f_k) * lambda_NCE\n",
    "                total_nce_loss += loss.mean()\n",
    "\n",
    "            nce_loss = total_nce_loss / len(nce_layers)\n",
    "            total_nce_loss += nce_loss\n",
    "        total_nce_loss *= 0.5\n",
    "        \n",
    "        loss_G = loss_G_GAN + total_nce_loss\n",
    "        loss_G.backward()\n",
    "        optimizer_G.step()\n",
    "        optimizer_Sampler.step()\n",
    "        \n",
    "        # --------------\n",
    "        #  Log Progress\n",
    "        # --------------\n",
    "\n",
    "        # Determine approximate time left\n",
    "        batches_done = epoch * len(dataloader) + i\n",
    "        batches_left = 200 * len(dataloader) - batches_done\n",
    "        time_left = datetime.timedelta(seconds=batches_left * (time.time() - prev_time))\n",
    "        prev_time = time.time()\n",
    "\n",
    "        # Print log\n",
    "        sys.stdout.write(\n",
    "            \"\\r[Epoch %d/%d] [Batch %d/%d] [D loss: %f] [GAN loss: %f, NCE loss: %f, Total: %f] ETA: %s\"\n",
    "            % (\n",
    "                epoch,\n",
    "                200,\n",
    "                i,\n",
    "                len(dataloader),\n",
    "                loss_D.item(),\n",
    "                loss_G_GAN.item(),\n",
    "                nce_loss.item(),\n",
    "                loss_G.item(),\n",
    "                time_left,\n",
    "            )\n",
    "        )\n",
    "        \n",
    "        total_D_loss += loss_D.item()\n",
    "        total_GAN_loss += loss_G_GAN.item()\n",
    "        total_NCE_loss += nce_loss.item()\n",
    "        total_G_loss += loss_G.item()\n",
    "        \n",
    "        # If at sample interval save image\n",
    "        if batches_done % 300 == 0:\n",
    "            sample_images(batches_done)\n",
    "            \n",
    "    D_losses.append(total_D_loss / len(dataloader))\n",
    "    GAN_losses.append(total_GAN_loss / len(dataloader))\n",
    "    NCE_losses.append(total_NCE_loss / len(dataloader))\n",
    "    G_total_losses.append(total_G_loss / len(dataloader))\n",
    "    display.clear_output(wait = True)\n",
    "    time.sleep(1)\n",
    "    plt.clf()\n",
    "    plt.ylabel('Mean Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.plot(D_losses, color = 'r')\n",
    "    plt.plot(GAN_losses, color = 'g')\n",
    "    plt.plot(NCE_losses, color = 'b')\n",
    "    plt.savefig('models/%s/loss_plot.png' % model_name)\n",
    "    display.display(plt.gcf())\n",
    "    \n",
    "    if not os.path.isdir('models'):\n",
    "        os.makedirs('models')\n",
    "    if not os.path.isdir('models/%s' % model_name):\n",
    "        os.makedirs('models/%s' % model_name)\n",
    "    torch.save(G.state_dict(), \"models/%s/G_%d.pth\" % (model_name, epoch))\n",
    "    torch.save(D.state_dict(), \"models/%s/D_%d.pth\" % (model_name, epoch))\n",
    "    torch.save(Sampler.state_dict(), \"models/%s/Sampler_%d.pth\" % (model_name, epoch))\n",
    "    np.save('models/%s/losses_%d.npy' % (model_name, epoch), np.array([D_losses, GAN_losses, NCE_losses, G_total_losses]))\n",
    "    try:\n",
    "        os.remove('models/%s/G_%d.pth' % (model_name, epoch - 1))\n",
    "        os.remove('models/%s/D_%d.pth' % (model_name, epoch - 1))\n",
    "        os.remove('models/%s/Sampler_%d.pth' % (model_name, epoch - 1))\n",
    "        os.remove('models/%s/losses_%d.pth' % (model_name, epoch - 1))\n",
    "    except:\n",
    "        pass\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
