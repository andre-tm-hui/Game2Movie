{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import numpy as np\n",
    "import cv2\n",
    "import os\n",
    "import glob\n",
    "import random\n",
    "import time, datetime, sys\n",
    "from PIL import Image\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch.autograd import Variable\n",
    "from torchvision import transforms, datasets\n",
    "from torchvision.utils import save_image, make_grid\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython import display\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Variables\n",
    "dataset_size = 600\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "Tensor = torch.cuda.FloatTensor if torch.cuda.is_available() else torch.Tensor\n",
    "nce_layers = [0, 4, 8, 12, 16]\n",
    "lambda_NCE = 1.0\n",
    "batch_size = 1\n",
    "input_size = (3,216,384)\n",
    "res_blocks = 9\n",
    "load_weights = True\n",
    "epoch = 0\n",
    "epochs = 200\n",
    "\n",
    "D_losses = []\n",
    "GAN_losses = []\n",
    "NCE_losses = []\n",
    "G_total_losses = []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a basic dataset for full-frame translation\n",
    "\n",
    "if not os.path.isdir('./dataset'):\n",
    "    os.mkdir('./dataset')\n",
    "    os.mkdir('./dataset/train')\n",
    "    os.mkdir('./dataset/train/game')\n",
    "    os.mkdir('./dataset/train/movie')\n",
    "    os.mkdir('./dataset/test')\n",
    "    os.mkdir('./dataset/test/game')\n",
    "    os.mkdir('./dataset/test/movie')\n",
    "\n",
    "    # get some frames from the game footage\n",
    "    cap = cv2.VideoCapture('./Data/game/MafiaVideogame.mp4')\n",
    "    length = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "    frame_count = 0\n",
    "    saved_frames = 0\n",
    "\n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        frame_count += 1\n",
    "        if frame_count % (length // (2 * dataset_size)) == 0 and ret:\n",
    "            #print(ret)\n",
    "            if saved_frames % 2 == 0:\n",
    "                fname = './dataset/train/game/' + str(saved_frames // 2) + '.png'\n",
    "            else:\n",
    "                fname = './dataset/test/game/' + str(saved_frames // 2) + '.png'\n",
    "            cv2.imwrite(fname, frame)\n",
    "            saved_frames += 1\n",
    "\n",
    "    cap.release()\n",
    "\n",
    "    # get some frames from the movie footage\n",
    "    movie_dirs = ['./Data/movie/TheGodfather.mp4', './Data/movie/TheIrishman.mp4', './Data/movie/TheSopranos.mp4']\n",
    "\n",
    "    saved_frames = 0\n",
    "    for movie in movie_dirs:\n",
    "        cap = cv2.VideoCapture(movie)\n",
    "        length = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "        frame_count = 0\n",
    "\n",
    "        while cap.isOpened():\n",
    "            ret, frame = cap.read()\n",
    "            if not ret:\n",
    "                break\n",
    "            frame_count += 1\n",
    "            if frame_count % (length // (2 * dataset_size / len(movie_dirs))) == 0 and ret:\n",
    "                if saved_frames % 2 == 0:\n",
    "                    fname = './dataset/train/movie/' + str(saved_frames // 2) + '.png'\n",
    "                else:\n",
    "                    fname = './dataset/test/movie/' + str(saved_frames // 2) + '.png'\n",
    "                cv2.imwrite(fname, frame)\n",
    "                saved_frames += 1\n",
    "\n",
    "\n",
    "        cap.release()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helpful class for loading both game and movie samples as one dataset\n",
    "class ImageDataset(Dataset):\n",
    "    def __init__(self, root, transforms_ = None, unaligned = False, mode = \"train\"):\n",
    "        self.transform = transforms.Compose(transforms_)\n",
    "        self.unaligned = unaligned\n",
    "\n",
    "        self.files_game = sorted(glob.glob(os.path.join(root, \"%s/game\" % mode) + \"/*.*\"))\n",
    "        self.files_movie = sorted(glob.glob(os.path.join(root, \"%s/movie\" % mode) + \"/*.*\"))\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        image_game = Image.open(self.files_game[index % len(self.files_game)])\n",
    "\n",
    "        if self.unaligned:\n",
    "            image_movie = Image.open(self.files_movie[random.randint(0, len(self.files_movie) - 1)])\n",
    "        else:\n",
    "            image_movie = Image.open(self.files_movie[index % len(self.files_movie)])\n",
    "\n",
    "        # Convert grayscale images to rgb\n",
    "        if image_game.mode != \"RGB\":\n",
    "            image_game = to_rgb(image_game)\n",
    "        if image_movie.mode != \"RGB\":\n",
    "            image_movie = to_rgb(image_movie)\n",
    "\n",
    "        item_game = self.transform(image_game)\n",
    "        item_movie = self.transform(image_movie)\n",
    "        return {\"game\": item_game, \"movie\": item_movie}\n",
    "\n",
    "    def __len__(self):\n",
    "        return max(len(self.files_game), len(self.files_movie))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the dataset\n",
    "# Define the dataset-wide transformations\n",
    "transforms_ = [\n",
    "    transforms.Resize(input_size[1], Image.BICUBIC),\n",
    "    #transforms.RandomCrop((256,256)),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
    "]\n",
    "\n",
    "# Training data loader\n",
    "dataloader = DataLoader(\n",
    "    ImageDataset('./dataset', transforms_ = transforms_, unaligned = True),\n",
    "    batch_size = batch_size,\n",
    "    shuffle = True,\n",
    "    num_workers = 0,\n",
    ")\n",
    "# Test data loader\n",
    "val_dataloader = DataLoader(\n",
    "    ImageDataset('./dataset', transforms_ = transforms_, unaligned = True, mode = 'test'),\n",
    "    batch_size = 5,\n",
    "    shuffle = True,\n",
    "    num_workers = 0,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Other Utils\n",
    "\n",
    "def weights_init_normal(m):\n",
    "    classname = m.__class__.__name__\n",
    "    if classname.find(\"Conv\") != -1:\n",
    "        torch.nn.init.normal_(m.weight.data, 0.0, 0.02)\n",
    "        if hasattr(m, \"bias\") and m.bias is not None:\n",
    "            torch.nn.init.constant_(m.bias.data, 0.0)\n",
    "    elif classname.find(\"BatchNorm2d\") != -1:\n",
    "        torch.nn.init.normal_(m.weight.data, 1.0, 0.02)\n",
    "        torch.nn.init.constant_(m.bias.data, 0.0)\n",
    "        \n",
    "def sample_images(batches_done):\n",
    "    \"\"\"Saves a generated sample from the test set\"\"\"\n",
    "    imgs = next(iter(val_dataloader))\n",
    "    G.eval()\n",
    "    real_A = Variable(imgs[\"game\"].type(Tensor))\n",
    "    fake_B = G(real_A)\n",
    "    # Arange images along x-axis\n",
    "    real_A = make_grid(real_A, nrow=5, normalize=True)\n",
    "    fake_B = make_grid(fake_B, nrow=5, normalize=True)\n",
    "    # Arange images along y-axis\n",
    "    image_grid = torch.cat((real_A, fake_B), 1)\n",
    "    if not os.path.isdir('images'):\n",
    "        os.mkdir('images')\n",
    "    save_image(image_grid, \"images/%s.png\" % (batches_done), normalize=False)\n",
    "    \n",
    "class Normalize(nn.Module):\n",
    "    def __init__(self, power=2):\n",
    "        super(Normalize, self).__init__()\n",
    "        self.power = power\n",
    "\n",
    "    def forward(self, x):\n",
    "        norm = x.pow(self.power).sum(1, keepdim=True).pow(1. / self.power)\n",
    "        out = x.div(norm + 1e-7)\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generator and Discriminator models adapted from https://github.com/eriklindernoren/PyTorch-GAN\n",
    "# PatchSampleF adapted from https://github.com/taesungp/contrastive-unpaired-translation\n",
    "\n",
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, in_features):\n",
    "        super(ResidualBlock, self).__init__()\n",
    "\n",
    "        self.block = nn.Sequential(\n",
    "            nn.ReflectionPad2d(1),\n",
    "            nn.Conv2d(in_features, in_features, 3),\n",
    "            nn.InstanceNorm2d(in_features),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.ReflectionPad2d(1),\n",
    "            nn.Conv2d(in_features, in_features, 3),\n",
    "            nn.InstanceNorm2d(in_features),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x + self.block(x)\n",
    "\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self, input_shape, num_residual_blocks):\n",
    "        super(Generator, self).__init__()\n",
    "        channels = input_shape[0]\n",
    "\n",
    "        # Initial convolution block\n",
    "        out_features = 64\n",
    "        model = [\n",
    "            nn.ReflectionPad2d(channels),\n",
    "            nn.Conv2d(channels, out_features, 7),\n",
    "            nn.InstanceNorm2d(out_features),\n",
    "            nn.ReLU(inplace=True),\n",
    "        ]\n",
    "        in_features = out_features\n",
    "\n",
    "        # Downsampling\n",
    "        for _ in range(2):\n",
    "            out_features *= 2\n",
    "            model += [\n",
    "                nn.Conv2d(in_features, out_features, 3, stride=2, padding=1),\n",
    "                nn.InstanceNorm2d(out_features),\n",
    "                nn.ReLU(inplace=True),\n",
    "            ]\n",
    "            in_features = out_features\n",
    "\n",
    "        # Residual blocks\n",
    "        for _ in range(num_residual_blocks):\n",
    "            model += [ResidualBlock(out_features)]\n",
    "\n",
    "        # Upsampling\n",
    "        for _ in range(2):\n",
    "            out_features //= 2\n",
    "            model += [\n",
    "                nn.Upsample(scale_factor=2),\n",
    "                nn.Conv2d(in_features, out_features, 3, stride=1, padding=1),\n",
    "                nn.InstanceNorm2d(out_features),\n",
    "                nn.ReLU(inplace=True),\n",
    "            ]\n",
    "            in_features = out_features\n",
    "\n",
    "        # Output layer\n",
    "        model += [\n",
    "            nn.ReflectionPad2d(channels), \n",
    "            nn.Conv2d(out_features, channels, 7), \n",
    "            nn.Tanh()\n",
    "        ]\n",
    "\n",
    "        self.model = nn.Sequential(*model)\n",
    "\n",
    "    def forward(self, x, layers = [], encode_only = False):\n",
    "        if -1 in layers:\n",
    "            layers.append(len(self.model))\n",
    "        if len(layers) > 0:\n",
    "            feat = x\n",
    "            feats = []\n",
    "            for layer_id, layer in enumerate(self.model):\n",
    "                feat = layer(feat)\n",
    "                if layer_id in layers:\n",
    "                    feats.append(feat)\n",
    "                else:\n",
    "                    pass\n",
    "                if layer_id == layers[-1] and encode_only:\n",
    "                    return feats\n",
    "            return feat, feats\n",
    "        else:\n",
    "            return self.model(x)\n",
    "    \n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, input_shape):\n",
    "        super(Discriminator, self).__init__()\n",
    "        channels, height, width = input_shape\n",
    "        self.output_shape = (1, height // 2 ** 4, width // 2 ** 4)\n",
    "\n",
    "        def discriminator_block(in_filters, out_filters, normalize = True):\n",
    "            \"\"\"Returns downsampling layers of each discriminator block\"\"\"\n",
    "            layers = [nn.Conv2d(in_filters, out_filters, 4, stride=2, padding=1)]\n",
    "            if normalize:\n",
    "                layers.append(nn.InstanceNorm2d(out_filters))\n",
    "            layers.append(nn.LeakyReLU(0.2, inplace=True))\n",
    "            return layers\n",
    "\n",
    "        self.model = nn.Sequential(\n",
    "            *discriminator_block(channels, 64, normalize=False),\n",
    "            *discriminator_block(64, 128),\n",
    "            *discriminator_block(128, 256),\n",
    "            *discriminator_block(256, 512),\n",
    "            nn.ZeroPad2d((1, 0, 1, 0)),\n",
    "            nn.Conv2d(512, 1, 4, padding=1)\n",
    "        )\n",
    "\n",
    "    def forward(self, img):\n",
    "        return self.model(img)\n",
    "    \n",
    "class PatchSampleF(nn.Module):\n",
    "    def __init__(self, use_mlp = True, init_type = 'normal', init_gain = 0.02, nc = 256):\n",
    "        # potential issues: currently, we use the same patch_ids for multiple images in the batch\n",
    "        super(PatchSampleF, self).__init__()\n",
    "        self.l2norm = Normalize(2)\n",
    "        self.use_mlp = use_mlp\n",
    "        self.nc = nc  # hard-coded\n",
    "        self.mlp_init = False\n",
    "        self.init_type = init_type\n",
    "        self.init_gain = init_gain\n",
    "\n",
    "    def create_mlp(self, feats):\n",
    "        for mlp_id, feat in enumerate(feats):\n",
    "            input_nc = feat.shape[1]\n",
    "            mlp = nn.Sequential(*[nn.Linear(input_nc, self.nc), nn.ReLU(), nn.Linear(self.nc, self.nc)])\n",
    "            if torch.cuda.is_available():\n",
    "                mlp.cuda()\n",
    "            setattr(self, 'mlp_%d' % mlp_id, mlp)\n",
    "        if torch.cuda.is_available():\n",
    "            self.to(device)\n",
    "        self.apply(weights_init_normal)\n",
    "        self.mlp_init = True\n",
    "\n",
    "    def forward(self, feats, num_patches = 64, patch_ids = None):\n",
    "        return_ids = []\n",
    "        return_feats = []\n",
    "        if self.use_mlp and not self.mlp_init:\n",
    "            self.create_mlp(feats)\n",
    "        for feat_id, feat in enumerate(feats):\n",
    "            B, H, W = feat.shape[0], feat.shape[2], feat.shape[3]\n",
    "            feat_reshape = feat.permute(0, 2, 3, 1).flatten(1, 2)\n",
    "            if num_patches > 0:\n",
    "                if patch_ids is not None:\n",
    "                    patch_id = patch_ids[feat_id]\n",
    "                else:\n",
    "                    patch_id = torch.randperm(feat_reshape.shape[1], device=feats[0].device)\n",
    "                    patch_id = patch_id[:int(min(num_patches, patch_id.shape[0]))]  # .to(patch_ids.device)\n",
    "                x_sample = feat_reshape[:, patch_id, :].flatten(0, 1)  # reshape(-1, x.shape[1])\n",
    "            else:\n",
    "                x_sample = feat_reshape\n",
    "                patch_id = []\n",
    "            if self.use_mlp:\n",
    "                mlp = getattr(self, 'mlp_%d' % feat_id)\n",
    "                x_sample = mlp(x_sample)\n",
    "            return_ids.append(patch_id)\n",
    "            x_sample = self.l2norm(x_sample)\n",
    "\n",
    "            if num_patches == 0:\n",
    "                x_sample = x_sample.permute(0, 2, 1).reshape([B, x_sample.shape[-1], H, W])\n",
    "            return_feats.append(x_sample)\n",
    "        return return_feats, return_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Class to create separate losses for each feature, from https://github.com/taesungp/contrastive-unpaired-translation\n",
    "class PatchNCELoss(nn.Module):\n",
    "    def __init__(self, batch_size, nce_T = 0.07):\n",
    "        super().__init__()\n",
    "        self.batch_size = batch_size\n",
    "        self.nce_T = nce_T\n",
    "        self.cross_entropy_loss = torch.nn.CrossEntropyLoss(reduction='none')\n",
    "        self.mask_dtype = torch.bool\n",
    "\n",
    "    def forward(self, feat_q, feat_k):\n",
    "        batchSize = feat_q.shape[0]\n",
    "        dim = feat_q.shape[1]\n",
    "        feat_k = feat_k.detach()\n",
    "\n",
    "        # pos logit\n",
    "        l_pos = torch.bmm(feat_q.view(batchSize, 1, -1), feat_k.view(batchSize, -1, 1))\n",
    "        l_pos = l_pos.view(batchSize, 1)\n",
    "\n",
    "        # neg logit\n",
    "\n",
    "        # Should the negatives from the other samples of a minibatch be utilized?\n",
    "        # In CUT and FastCUT, we found that it's best to only include negatives\n",
    "        # from the same image. Therefore, we set\n",
    "        # --nce_includes_all_negatives_from_minibatch as False\n",
    "        # However, for single-image translation, the minibatch consists of\n",
    "        # crops from the \"same\" high-resolution image.\n",
    "        # Therefore, we will include the negatives from the entire minibatch.\n",
    "\n",
    "        batch_dim_for_bmm = self.batch_size\n",
    "\n",
    "        # reshape features to batch size\n",
    "        feat_q = feat_q.view(batch_dim_for_bmm, -1, dim)\n",
    "        feat_k = feat_k.view(batch_dim_for_bmm, -1, dim)\n",
    "        npatches = feat_q.size(1)\n",
    "        l_neg_curbatch = torch.bmm(feat_q, feat_k.transpose(2, 1))\n",
    "\n",
    "        # diagonal entries are similarity between same features, and hence meaningless.\n",
    "        # just fill the diagonal with very small number, which is exp(-10) and almost zero\n",
    "        diagonal = torch.eye(npatches, device=feat_q.device, dtype=self.mask_dtype)[None, :, :]\n",
    "        l_neg_curbatch.masked_fill_(diagonal, -10.0)\n",
    "        l_neg = l_neg_curbatch.view(-1, npatches)\n",
    "\n",
    "        out = torch.cat((l_pos, l_neg), dim=1) / self.nce_T\n",
    "\n",
    "        loss = self.cross_entropy_loss(out, torch.zeros(out.size(0), dtype=torch.long,\n",
    "                                                        device=feat_q.device))\n",
    "\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the networks, load the most recent saved models, and set the loss functions and optimizers\n",
    "\n",
    "G = Generator(input_size, res_blocks).to(device)\n",
    "D = Discriminator(input_size).to(device)\n",
    "Sampler = PatchSampleF(batch_size).to(device)\n",
    "\n",
    "criterion_GAN = torch.nn.MSELoss().to(device)\n",
    "criterion_NCE = []\n",
    "\n",
    "for nce_layer in nce_layers:\n",
    "    criterion_NCE.append(PatchNCELoss(batch_size).to(device))\n",
    "\n",
    "G.apply(weights_init_normal)\n",
    "D.apply(weights_init_normal)\n",
    "\n",
    "optimizer_G = torch.optim.Adam(G.parameters(), lr = 0.0002, betas = (0.5, 0.999))\n",
    "optimizer_D = torch.optim.Adam(D.parameters(), lr = 0.0002, betas = (0.5, 0.999))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters for the sampler are not made until the first forward pass through the Sampler network\n",
    "# Hence, we do a 'trial' training pass before setting the optimizer for the Sampler\n",
    "for i, batch in enumerate(dataloader):\n",
    "    x, y = Variable(batch[\"game\"].type(Tensor)), Variable(batch[\"movie\"].type(Tensor))\n",
    "    \n",
    "    real = Variable(Tensor(np.ones((x.size(0), *D.output_shape))), requires_grad = False)\n",
    "    fake = Variable(Tensor(np.zeros((x.size(0), *D.output_shape))), requires_grad = False)\n",
    "    \n",
    "    # get the fake loss\n",
    "    fake_y = G(x)\n",
    "    D_fake = D(fake_y.detach())\n",
    "    loss_D_fake = criterion_GAN(D_fake, fake).mean()\n",
    "    # get the real loss\n",
    "    D_real = D(y)\n",
    "    loss_D_real = criterion_GAN(D_real, real).mean()\n",
    "    # combine loss and calculate gradients\n",
    "    loss_D = (loss_D_fake + loss_D_real) * 0.5\n",
    "    loss_D.backward()\n",
    "\n",
    "    # get the fake GAN loss\n",
    "    D_fake = D(fake_y)\n",
    "    loss_G_GAN = criterion_GAN(D_fake, real).mean()\n",
    "    total_nce_loss = 0\n",
    "    for fake, real in [(fake_y, x), (y, G(y))]:\n",
    "        # get the NCE loss\n",
    "        feat_q = G(fake, nce_layers, encode_only = True)\n",
    "        feat_k = G(real, nce_layers, encode_only = True)\n",
    "\n",
    "        feat_k_pool, sample_ids = Sampler(feat_k, 256, None)\n",
    "        feat_q_pool, _ = Sampler(feat_q, 256, sample_ids)\n",
    "\n",
    "        total_nce_loss = 0.0\n",
    "        for f_q, f_k, crit, nce_layer in zip(feat_q_pool, feat_k_pool, criterion_NCE, nce_layers):\n",
    "            loss = crit(f_q, f_k) * lambda_NCE\n",
    "            total_nce_loss += loss.mean()\n",
    "\n",
    "        nce_loss = total_nce_loss / len(nce_layers)\n",
    "        total_nce_loss += nce_loss\n",
    "\n",
    "    loss_G = loss_G_GAN + total_nce_loss\n",
    "    loss_G.backward()\n",
    "    \n",
    "    break\n",
    "\n",
    "optimizer_Sampler = torch.optim.Adam(Sampler.parameters(), lr = 0.0002, betas = (0.5, 0.999))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the most recently saved models if available\n",
    "if os.path.isdir('saved_models') and load_weights:\n",
    "    # Get the most recent model and load them\n",
    "    epoch = max([int(fname[8:-4]) for fname in os.listdir('saved_models') if 'Sampler' in fname])\n",
    "    G.load_state_dict(torch.load('saved_models/G_%d.pth' % (epoch)))\n",
    "    D.load_state_dict(torch.load('saved_models/D_%d.pth' % (epoch)))\n",
    "    Sampler.load_state_dict(torch.load('saved_models/Sampler_%d.pth' % (epoch)))\n",
    "    # Load the losses as well, for plotting\n",
    "    losses = np.load('saved_models/losses_%d.npy' % (epoch))\n",
    "    D_losses = list(losses[0])\n",
    "    GAN_losses = list(losses[1])\n",
    "    NCE_losses = list(losses[2])\n",
    "    G_total_losses = list(losses[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 32/200] [Batch 485/602] [D loss: 0.014287] [GAN loss: 1.146085, NCE loss: 0.720480, Total: 5.468964] ETA: 1 day, 14:21:57.9348632"
     ]
    }
   ],
   "source": [
    "# Training Loop, adapted from https://github.com/taesungp/contrastive-unpaired-translation\n",
    "\n",
    "prev_time = time.time()\n",
    "for epoch in range(epoch, epochs):\n",
    "    total_D_loss = 0\n",
    "    total_GAN_loss = 0\n",
    "    total_NCE_loss = 0\n",
    "    total_G_loss = 0\n",
    "    for i, batch in enumerate(dataloader):\n",
    "        x, y = Variable(batch[\"game\"].type(Tensor)), Variable(batch[\"movie\"].type(Tensor))\n",
    "        \n",
    "        # Adversarial ground truths\n",
    "        real = Variable(Tensor(np.ones((x.size(0), *D.output_shape))), requires_grad = False)\n",
    "        fake = Variable(Tensor(np.zeros((x.size(0), *D.output_shape))), requires_grad = False)\n",
    "        \n",
    "        # train discriminator\n",
    "        D.train()\n",
    "        optimizer_D.zero_grad()\n",
    "        # get the fake loss\n",
    "        fake_y = G(x)\n",
    "        D_fake = D(fake_y.detach())\n",
    "        loss_D_fake = criterion_GAN(D_fake, fake).mean()\n",
    "        # get the real loss\n",
    "        D_real = D(y)\n",
    "        loss_D_real = criterion_GAN(D_real, real).mean()\n",
    "        # combine loss and calculate gradients\n",
    "        loss_D = (loss_D_fake + loss_D_real) * 0.5\n",
    "        loss_D.backward()\n",
    "        optimizer_D.step()\n",
    "        \n",
    "        # train generator\n",
    "        G.train()\n",
    "        optimizer_G.zero_grad()\n",
    "        optimizer_Sampler.zero_grad()\n",
    "        # get the fake GAN loss\n",
    "        D_fake = D(fake_y)\n",
    "        loss_G_GAN = criterion_GAN(D_fake, real).mean()\n",
    "        # get the NCE loss\n",
    "        total_nce_loss = 0\n",
    "        for fake, real in [(fake_y, x), (y, G(y))]:\n",
    "            feat_q = G(fake_y, nce_layers, encode_only = True)\n",
    "            feat_k = G(x, nce_layers, encode_only = True)\n",
    "\n",
    "            feat_k_pool, sample_ids = Sampler(feat_k, 256, None)\n",
    "            feat_q_pool, _ = Sampler(feat_q, 256, sample_ids)\n",
    "\n",
    "            total_nce_loss = 0.0\n",
    "            for f_q, f_k, crit, nce_layer in zip(feat_q_pool, feat_k_pool, criterion_NCE, nce_layers):\n",
    "                loss = crit(f_q, f_k) * lambda_NCE\n",
    "                total_nce_loss += loss.mean()\n",
    "\n",
    "            nce_loss = total_nce_loss / len(nce_layers)\n",
    "            total_nce_loss += nce_loss\n",
    "        \n",
    "        loss_G = loss_G_GAN + total_nce_loss\n",
    "        loss_G.backward()\n",
    "        optimizer_G.step()\n",
    "        optimizer_Sampler.step()\n",
    "        \n",
    "        # --------------\n",
    "        #  Log Progress\n",
    "        # --------------\n",
    "\n",
    "        # Determine approximate time left\n",
    "        batches_done = epoch * len(dataloader) + i\n",
    "        batches_left = 200 * len(dataloader) - batches_done\n",
    "        time_left = datetime.timedelta(seconds=batches_left * (time.time() - prev_time))\n",
    "        prev_time = time.time()\n",
    "\n",
    "        # Print log\n",
    "        sys.stdout.write(\n",
    "            \"\\r[Epoch %d/%d] [Batch %d/%d] [D loss: %f] [GAN loss: %f, NCE loss: %f, Total: %f] ETA: %s\"\n",
    "            % (\n",
    "                epoch,\n",
    "                200,\n",
    "                i,\n",
    "                len(dataloader),\n",
    "                loss_D.item(),\n",
    "                loss_G_GAN.item(),\n",
    "                nce_loss.item(),\n",
    "                loss_G.item(),\n",
    "                time_left,\n",
    "            )\n",
    "        )\n",
    "        \n",
    "        total_D_loss += loss_D.item()\n",
    "        total_GAN_loss += loss_G_GAN.item()\n",
    "        total_NCE_loss += nce_loss.item()\n",
    "        total_G_loss += loss_G.item()\n",
    "        \n",
    "        # If at sample interval save image\n",
    "        if batches_done % 300 == 0:\n",
    "            sample_images(batches_done)\n",
    "            \n",
    "    D_losses.append(total_D_loss / len(dataloader))\n",
    "    GAN_losses.append(total_GAN_loss / len(dataloader))\n",
    "    NCE_losses.append(total_NCE_loss / len(dataloader))\n",
    "    G_total_losses.append(total_G_loss / len(dataloader))\n",
    "    display.clear_output(wait = True)\n",
    "    time.sleep(1)\n",
    "    plt.clf()\n",
    "    plt.ylabel('Mean Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.plot(D_losses, color = 'r')\n",
    "    plt.plot(GAN_losses, color = 'g')\n",
    "    plt.plot(NCE_losses, color = 'b')\n",
    "    display.display(plt.gcf())\n",
    "    \n",
    "    if epoch % 5 == 0:\n",
    "        if not os.path.isdir('saved_models'):\n",
    "            os.mkdir('saved_models')\n",
    "        torch.save(G.state_dict(), \"saved_models/G_%d.pth\" % (epoch))\n",
    "        torch.save(D.state_dict(), \"saved_models/D_%d.pth\" % (epoch))\n",
    "        torch.save(Sampler.state_dict(), \"saved_models/Sampler_%d.pth\" % (epoch))\n",
    "        np.save('saved_models/losses_%d.npy' % (epoch), np.array([D_losses, GAN_losses, NCE_losses, G_total_losses]))\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
