{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "import os\n",
    "\n",
    "if not os.path.isdir('./dataset'):\n",
    "    os.mkdir('./dataset')\n",
    "    os.mkdir('./dataset/train')\n",
    "    os.mkdir('./dataset/train/game')\n",
    "    os.mkdir('./dataset/train/movie')\n",
    "    os.mkdir('./dataset/test')\n",
    "    os.mkdir('./dataset/test/game')\n",
    "    os.mkdir('./dataset/test/movie')\n",
    "\n",
    "    dataset_size = 600\n",
    "\n",
    "    # get some frames from the game footage\n",
    "    cap = cv2.VideoCapture('./Data/game/MafiaVideogame.mp4')\n",
    "    length = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "    frame_count = 0\n",
    "    saved_frames = 0\n",
    "\n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        frame_count += 1\n",
    "        if frame_count % (length // (2 * dataset_size)) == 0 and ret:\n",
    "            #print(ret)\n",
    "            if saved_frames % 2 == 0:\n",
    "                fname = './dataset/train/game/' + str(saved_frames // 2) + '.png'\n",
    "            else:\n",
    "                fname = './dataset/test/game/' + str(saved_frames // 2) + '.png'\n",
    "            cv2.imwrite(fname, frame)\n",
    "            saved_frames += 1\n",
    "\n",
    "    cap.release()\n",
    "\n",
    "    # get some frames from the movie footage\n",
    "    movie_dirs = ['./Data/movie/TheGodfather.mp4', './Data/movie/TheIrishman.mp4', './Data/movie/TheSopranos.mp4']\n",
    "\n",
    "    saved_frames = 0\n",
    "    for movie in movie_dirs:\n",
    "        cap = cv2.VideoCapture(movie)\n",
    "        length = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "        frame_count = 0\n",
    "\n",
    "        while cap.isOpened():\n",
    "            ret, frame = cap.read()\n",
    "            if not ret:\n",
    "                break\n",
    "            frame_count += 1\n",
    "            if frame_count % (length // (2 * dataset_size / len(movie_dirs))) == 0 and ret:\n",
    "                if saved_frames % 2 == 0:\n",
    "                    fname = './dataset/train/movie/' + str(saved_frames // 2) + '.png'\n",
    "                else:\n",
    "                    fname = './dataset/test/movie/' + str(saved_frames // 2) + '.png'\n",
    "                cv2.imwrite(fname, frame)\n",
    "                saved_frames += 1\n",
    "\n",
    "\n",
    "        cap.release()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "import glob\n",
    "import random\n",
    "\n",
    "class ImageDataset(Dataset):\n",
    "    def __init__(self, root, transforms_ = None, unaligned = False, mode = \"train\"):\n",
    "        self.transform = transforms.Compose(transforms_)\n",
    "        self.unaligned = unaligned\n",
    "\n",
    "        self.files_game = sorted(glob.glob(os.path.join(root, \"%s/game\" % mode) + \"/*.*\"))\n",
    "        self.files_movie = sorted(glob.glob(os.path.join(root, \"%s/movie\" % mode) + \"/*.*\"))\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        image_game = Image.open(self.files_game[index % len(self.files_game)])\n",
    "\n",
    "        if self.unaligned:\n",
    "            image_movie = Image.open(self.files_movie[random.randint(0, len(self.files_movie) - 1)])\n",
    "        else:\n",
    "            image_movie = Image.open(self.files_movie[index % len(self.files_movie)])\n",
    "\n",
    "        # Convert grayscale images to rgb\n",
    "        if image_game.mode != \"RGB\":\n",
    "            image_game = to_rgb(image_game)\n",
    "        if image_movie.mode != \"RGB\":\n",
    "            image_movie = to_rgb(image_movie)\n",
    "\n",
    "        item_game = self.transform(image_game)\n",
    "        item_movie = self.transform(image_movie)\n",
    "        return {\"game\": item_game, \"movie\": item_movie}\n",
    "\n",
    "    def __len__(self):\n",
    "        return max(len(self.files_game), len(self.files_movie))\n",
    "    \n",
    "class LambdaLR:\n",
    "    def __init__(self, n_epochs, offset, decay_start_epoch):\n",
    "        assert (n_epochs - decay_start_epoch) > 0, \"Decay must start before the training session ends!\"\n",
    "        self.n_epochs = n_epochs\n",
    "        self.offset = offset\n",
    "        self.decay_start_epoch = decay_start_epoch\n",
    "\n",
    "    def step(self, epoch):\n",
    "        return 1.0 - max(0, epoch + self.offset - self.decay_start_epoch) / (self.n_epochs - self.decay_start_epoch)\n",
    "    \n",
    "\n",
    "class ReplayBuffer:\n",
    "    def __init__(self, max_size=50):\n",
    "        assert max_size > 0, \"Empty buffer or trying to create a black hole. Be careful.\"\n",
    "        self.max_size = max_size\n",
    "        self.data = []\n",
    "\n",
    "    def push_and_pop(self, data):\n",
    "        to_return = []\n",
    "        for element in data.data:\n",
    "            element = torch.unsqueeze(element, 0)\n",
    "            if len(self.data) < self.max_size:\n",
    "                self.data.append(element)\n",
    "                to_return.append(element)\n",
    "            else:\n",
    "                if random.uniform(0, 1) > 0.5:\n",
    "                    i = random.randint(0, self.max_size - 1)\n",
    "                    to_return.append(self.data[i].clone())\n",
    "                    self.data[i] = element\n",
    "                else:\n",
    "                    to_return.append(element)\n",
    "        return Variable(torch.cat(to_return))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.autograd import Variable\n",
    "from torchvision import transforms, datasets\n",
    "\n",
    "cuda = torch.cuda.is_available()\n",
    "device = torch.device('cuda:0' if cuda else 'cpu')\n",
    "Tensor = torch.cuda.FloatTensor if cuda else torch.Tensor\n",
    "\n",
    "# Buffers of previously generated samples\n",
    "fake_A_buffer = ReplayBuffer()\n",
    "fake_B_buffer = ReplayBuffer()\n",
    "\n",
    "\n",
    "transforms_ = [\n",
    "    transforms.Resize(216, Image.BICUBIC),\n",
    "    #transforms.RandomCrop((256,256)),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
    "]\n",
    "\n",
    "# Training data loader\n",
    "dataloader = DataLoader(\n",
    "    ImageDataset('./dataset', transforms_ = transforms_, unaligned = True),\n",
    "    batch_size = 1,\n",
    "    shuffle = True,\n",
    "    num_workers = 0,\n",
    ")\n",
    "# Test data loader\n",
    "val_dataloader = DataLoader(\n",
    "    ImageDataset('./dataset', transforms_ = transforms_, unaligned = True, mode = 'test'),\n",
    "    batch_size = 5,\n",
    "    shuffle = True,\n",
    "    num_workers = 0,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Models adapted from https://github.com/eriklindernoren/PyTorch-GAN/blob/36d3c77e5ff20ebe0aeefd322326a134a279b93e/implementations/stargan/models.py#L35\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "def weights_init_normal(m):\n",
    "    classname = m.__class__.__name__\n",
    "    if classname.find(\"Conv\") != -1:\n",
    "        torch.nn.init.normal_(m.weight.data, 0.0, 0.02)\n",
    "        if hasattr(m, \"bias\") and m.bias is not None:\n",
    "            torch.nn.init.constant_(m.bias.data, 0.0)\n",
    "    elif classname.find(\"BatchNorm2d\") != -1:\n",
    "        torch.nn.init.normal_(m.weight.data, 1.0, 0.02)\n",
    "        torch.nn.init.constant_(m.bias.data, 0.0)\n",
    "\n",
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, in_features):\n",
    "        super(ResidualBlock, self).__init__()\n",
    "\n",
    "        self.block = nn.Sequential(\n",
    "            nn.ReflectionPad2d(1),\n",
    "            nn.Conv2d(in_features, in_features, 3),\n",
    "            nn.InstanceNorm2d(in_features),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.ReflectionPad2d(1),\n",
    "            nn.Conv2d(in_features, in_features, 3),\n",
    "            nn.InstanceNorm2d(in_features),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x + self.block(x)\n",
    "\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self, input_shape, num_residual_blocks):\n",
    "        super(Generator, self).__init__()\n",
    "        channels = input_shape[0]\n",
    "\n",
    "        # Initial convolution block\n",
    "        out_features = 64\n",
    "        model = [\n",
    "            nn.ReflectionPad2d(channels),\n",
    "            nn.Conv2d(channels, out_features, 7),\n",
    "            nn.InstanceNorm2d(out_features),\n",
    "            nn.ReLU(inplace=True),\n",
    "        ]\n",
    "        in_features = out_features\n",
    "\n",
    "        # Downsampling\n",
    "        for _ in range(2):\n",
    "            out_features *= 2\n",
    "            model += [\n",
    "                nn.Conv2d(in_features, out_features, 3, stride=2, padding=1),\n",
    "                nn.InstanceNorm2d(out_features),\n",
    "                nn.ReLU(inplace=True),\n",
    "            ]\n",
    "            in_features = out_features\n",
    "\n",
    "        # Residual blocks\n",
    "        for _ in range(num_residual_blocks):\n",
    "            model += [ResidualBlock(out_features)]\n",
    "\n",
    "        # Upsampling\n",
    "        for _ in range(2):\n",
    "            out_features //= 2\n",
    "            model += [\n",
    "                nn.Upsample(scale_factor=2),\n",
    "                nn.Conv2d(in_features, out_features, 3, stride=1, padding=1),\n",
    "                nn.InstanceNorm2d(out_features),\n",
    "                nn.ReLU(inplace=True),\n",
    "            ]\n",
    "            in_features = out_features\n",
    "\n",
    "        # Output layer\n",
    "        model += [\n",
    "            nn.ReflectionPad2d(channels), \n",
    "            nn.Conv2d(out_features, channels, 7), \n",
    "            nn.Tanh()\n",
    "        ]\n",
    "\n",
    "        self.model = nn.Sequential(*model)\n",
    "\n",
    "    def forward(self, x, layers = [], encode_only = False):\n",
    "        if -1 in layers:\n",
    "            layers.append(len(self.model))\n",
    "        if len(layers) > 0:\n",
    "            feat = x\n",
    "            feats = []\n",
    "            for layer_id, layer in enumerate(self.model):\n",
    "                feat = layer(feat)\n",
    "                if layer_id in layers:\n",
    "                    feats.append(feat)\n",
    "                else:\n",
    "                    pass\n",
    "                if layer_id == layers[-1] and encode_only:\n",
    "                    return feats\n",
    "            return feat, feats\n",
    "        else:\n",
    "            return self.model(x)\n",
    "    \n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, input_shape):\n",
    "        super(Discriminator, self).__init__()\n",
    "        channels, height, width = input_shape\n",
    "        self.output_shape = (1, height // 2 ** 4, width // 2 ** 4)\n",
    "\n",
    "        def discriminator_block(in_filters, out_filters, normalize = True):\n",
    "            \"\"\"Returns downsampling layers of each discriminator block\"\"\"\n",
    "            layers = [nn.Conv2d(in_filters, out_filters, 4, stride=2, padding=1)]\n",
    "            if normalize:\n",
    "                layers.append(nn.InstanceNorm2d(out_filters))\n",
    "            layers.append(nn.LeakyReLU(0.2, inplace=True))\n",
    "            return layers\n",
    "\n",
    "        self.model = nn.Sequential(\n",
    "            *discriminator_block(channels, 64, normalize=False),\n",
    "            *discriminator_block(64, 128),\n",
    "            *discriminator_block(128, 256),\n",
    "            *discriminator_block(256, 512),\n",
    "            nn.ZeroPad2d((1, 0, 1, 0)),\n",
    "            nn.Conv2d(512, 1, 4, padding=1)\n",
    "        )\n",
    "\n",
    "    def forward(self, img):\n",
    "        return self.model(img)\n",
    "    \n",
    "class PatchSampleF(nn.Module):\n",
    "    def __init__(self, use_mlp = True, init_type = 'normal', init_gain = 0.02, nc = 256):\n",
    "        # potential issues: currently, we use the same patch_ids for multiple images in the batch\n",
    "        super(PatchSampleF, self).__init__()\n",
    "        self.l2norm = Normalize(2)\n",
    "        self.use_mlp = use_mlp\n",
    "        self.nc = nc  # hard-coded\n",
    "        self.mlp_init = False\n",
    "        self.init_type = init_type\n",
    "        self.init_gain = init_gain\n",
    "\n",
    "    def create_mlp(self, feats):\n",
    "        for mlp_id, feat in enumerate(feats):\n",
    "            input_nc = feat.shape[1]\n",
    "            mlp = nn.Sequential(*[nn.Linear(input_nc, self.nc), nn.ReLU(), nn.Linear(self.nc, self.nc)])\n",
    "            if torch.cuda.is_available():\n",
    "                mlp.cuda()\n",
    "            setattr(self, 'mlp_%d' % mlp_id, mlp)\n",
    "        if torch.cuda.is_available():\n",
    "            self.to(device)\n",
    "        self.apply(weights_init_normal)\n",
    "        self.mlp_init = True\n",
    "\n",
    "    def forward(self, feats, num_patches = 64, patch_ids = None):\n",
    "        return_ids = []\n",
    "        return_feats = []\n",
    "        if self.use_mlp and not self.mlp_init:\n",
    "            self.create_mlp(feats)\n",
    "        for feat_id, feat in enumerate(feats):\n",
    "            B, H, W = feat.shape[0], feat.shape[2], feat.shape[3]\n",
    "            feat_reshape = feat.permute(0, 2, 3, 1).flatten(1, 2)\n",
    "            if num_patches > 0:\n",
    "                if patch_ids is not None:\n",
    "                    patch_id = patch_ids[feat_id]\n",
    "                else:\n",
    "                    patch_id = torch.randperm(feat_reshape.shape[1], device=feats[0].device)\n",
    "                    patch_id = patch_id[:int(min(num_patches, patch_id.shape[0]))]  # .to(patch_ids.device)\n",
    "                x_sample = feat_reshape[:, patch_id, :].flatten(0, 1)  # reshape(-1, x.shape[1])\n",
    "            else:\n",
    "                x_sample = feat_reshape\n",
    "                patch_id = []\n",
    "            if self.use_mlp:\n",
    "                mlp = getattr(self, 'mlp_%d' % feat_id)\n",
    "                x_sample = mlp(x_sample)\n",
    "            return_ids.append(patch_id)\n",
    "            x_sample = self.l2norm(x_sample)\n",
    "\n",
    "            if num_patches == 0:\n",
    "                x_sample = x_sample.permute(0, 2, 1).reshape([B, x_sample.shape[-1], H, W])\n",
    "            return_feats.append(x_sample)\n",
    "        return return_feats, return_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Normalize(nn.Module):\n",
    "    def __init__(self, power=2):\n",
    "        super(Normalize, self).__init__()\n",
    "        self.power = power\n",
    "\n",
    "    def forward(self, x):\n",
    "        norm = x.pow(self.power).sum(1, keepdim=True).pow(1. / self.power)\n",
    "        out = x.div(norm + 1e-7)\n",
    "        return out\n",
    "\n",
    "class PatchNCELoss(nn.Module):\n",
    "    def __init__(self, batch_size, nce_T = 0.07):\n",
    "        super().__init__()\n",
    "        self.batch_size = batch_size\n",
    "        self.nce_T = nce_T\n",
    "        self.cross_entropy_loss = torch.nn.CrossEntropyLoss(reduction='none')\n",
    "        self.mask_dtype = torch.bool\n",
    "\n",
    "    def forward(self, feat_q, feat_k):\n",
    "        batchSize = feat_q.shape[0]\n",
    "        dim = feat_q.shape[1]\n",
    "        feat_k = feat_k.detach()\n",
    "\n",
    "        # pos logit\n",
    "        l_pos = torch.bmm(feat_q.view(batchSize, 1, -1), feat_k.view(batchSize, -1, 1))\n",
    "        l_pos = l_pos.view(batchSize, 1)\n",
    "\n",
    "        # neg logit\n",
    "\n",
    "        # Should the negatives from the other samples of a minibatch be utilized?\n",
    "        # In CUT and FastCUT, we found that it's best to only include negatives\n",
    "        # from the same image. Therefore, we set\n",
    "        # --nce_includes_all_negatives_from_minibatch as False\n",
    "        # However, for single-image translation, the minibatch consists of\n",
    "        # crops from the \"same\" high-resolution image.\n",
    "        # Therefore, we will include the negatives from the entire minibatch.\n",
    "\n",
    "        batch_dim_for_bmm = self.batch_size\n",
    "\n",
    "        # reshape features to batch size\n",
    "        feat_q = feat_q.view(batch_dim_for_bmm, -1, dim)\n",
    "        feat_k = feat_k.view(batch_dim_for_bmm, -1, dim)\n",
    "        npatches = feat_q.size(1)\n",
    "        l_neg_curbatch = torch.bmm(feat_q, feat_k.transpose(2, 1))\n",
    "\n",
    "        # diagonal entries are similarity between same features, and hence meaningless.\n",
    "        # just fill the diagonal with very small number, which is exp(-10) and almost zero\n",
    "        diagonal = torch.eye(npatches, device=feat_q.device, dtype=self.mask_dtype)[None, :, :]\n",
    "        l_neg_curbatch.masked_fill_(diagonal, -10.0)\n",
    "        l_neg = l_neg_curbatch.view(-1, npatches)\n",
    "\n",
    "        out = torch.cat((l_pos, l_neg), dim=1) / self.nce_T\n",
    "\n",
    "        loss = self.cross_entropy_loss(out, torch.zeros(out.size(0), dtype=torch.long,\n",
    "                                                        device=feat_q.device))\n",
    "\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Contrastive Unpaired Translation:\n",
    "# Input x, output y\n",
    "# y = G_dec(G_enc(x))\n",
    "# f_q are sampled features from H(G_enc(x))\n",
    "# f_k are sampled features from H(G_enc(y))\n",
    "\n",
    "nce_layers = [0, 4, 8, 12, 16]\n",
    "lambda_NCE = 1.0\n",
    "batch_size = 1\n",
    "input_size = (3,216,384)\n",
    "res_blocks = 9\n",
    "load_weights = True\n",
    "\n",
    "G = Generator(input_size, res_blocks).to(device)\n",
    "D = Discriminator(input_size).to(device)\n",
    "Sampler = PatchSampleF(batch_size).to(device)\n",
    "epoch = 0\n",
    "\n",
    "if os.path.isdir('saved_models') and load_weights:\n",
    "    epoch = max([int(fname[7:-4]) for fname in os.listdir('saved_models') if 'Sample' in fname])\n",
    "    G.load_state_dict(torch.load('saved_models/G_%d.pth' % (epoch)))\n",
    "    D.load_state_dict(torch.load('saved_models/D_%d.pth' % (epoch)))\n",
    "    Sampler.load_state_dict(torch.load('saved_models/Sampler_%d.pth' % (epoch)))\n",
    "\n",
    "criterion_GAN = torch.nn.MSELoss().to(device)\n",
    "criterion_NCE = []\n",
    "\n",
    "for nce_layer in nce_layers:\n",
    "    criterion_NCE.append(PatchNCELoss(batch_size).to(device))\n",
    "\n",
    "G.apply(weights_init_normal)\n",
    "D.apply(weights_init_normal)\n",
    "\n",
    "optimizer_G = torch.optim.Adam(G.parameters(), lr = 0.0002, betas = (0.5, 0.999))\n",
    "optimizer_D = torch.optim.Adam(D.parameters(), lr = 0.0002, betas = (0.5, 0.999))\n",
    "\n",
    "for i, batch in enumerate(dataloader):\n",
    "    x, y = Variable(batch[\"game\"].type(Tensor)), Variable(batch[\"movie\"].type(Tensor))\n",
    "    \n",
    "    real = Variable(Tensor(np.ones((x.size(0), *D.output_shape))), requires_grad = False)\n",
    "    fake = Variable(Tensor(np.zeros((x.size(0), *D.output_shape))), requires_grad = False)\n",
    "    \n",
    "    # get the fake loss\n",
    "    fake_y = G(x)\n",
    "    D_fake = D(fake_y.detach())\n",
    "    loss_D_fake = criterion_GAN(D_fake, fake).mean()\n",
    "    # get the real loss\n",
    "    D_real = D(y)\n",
    "    loss_D_real = criterion_GAN(D_real, real).mean()\n",
    "    # combine loss and calculate gradients\n",
    "    loss_D = (loss_D_fake + loss_D_real) * 0.5\n",
    "    loss_D.backward()\n",
    "\n",
    "    # get the fake GAN loss\n",
    "    D_fake = D(fake_y)\n",
    "    loss_G_GAN = criterion_GAN(D_fake, real).mean()\n",
    "    # get the NCE loss\n",
    "    feat_q = G(fake_y, nce_layers, encode_only = True)\n",
    "    feat_k = G(x, nce_layers, encode_only = True)\n",
    "\n",
    "    feat_k_pool, sample_ids = Sampler(feat_k, 256, None)\n",
    "    feat_q_pool, _ = Sampler(feat_q, 256, sample_ids)\n",
    "\n",
    "    total_nce_loss = 0.0\n",
    "    for f_q, f_k, crit, nce_layer in zip(feat_q_pool, feat_k_pool, criterion_NCE, nce_layers):\n",
    "        loss = crit(f_q, f_k) * lambda_NCE\n",
    "        total_nce_loss += loss.mean()\n",
    "\n",
    "    nce_loss = total_nce_loss / len(nce_layers)\n",
    "\n",
    "    loss_G = loss_G_GAN + nce_loss\n",
    "    loss_G.backward()\n",
    "    \n",
    "    break\n",
    "\n",
    "optimizer_Sampler = torch.optim.Adam(Sampler.parameters(), lr = 0.0002, betas = (0.5, 0.999))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.utils import save_image, make_grid\n",
    "\n",
    "def sample_images(batches_done):\n",
    "    \"\"\"Saves a generated sample from the test set\"\"\"\n",
    "    imgs = next(iter(val_dataloader))\n",
    "    G.eval()\n",
    "    real_A = Variable(imgs[\"game\"].type(Tensor))\n",
    "    fake_B = G(real_A)\n",
    "    # Arange images along x-axis\n",
    "    real_A = make_grid(real_A, nrow=5, normalize=True)\n",
    "    fake_B = make_grid(fake_B, nrow=5, normalize=True)\n",
    "    # Arange images along y-axis\n",
    "    image_grid = torch.cat((real_A, fake_B), 1)\n",
    "    if not os.path.isdir('images'):\n",
    "        os.mkdir('images')\n",
    "    save_image(image_grid, \"images/%s.png\" % (batches_done), normalize=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 5/200] [Batch 90/602] [D loss: 0.228870] [GAN loss: 0.261693, NCE loss: 1.287326, Total: 1.549020] ETA: 10:09:58.778844005506"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-8-4ab14ce7e5cd>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[0mepochs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m200\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 17\u001b[1;33m     \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataloader\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     18\u001b[0m         \u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mVariable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"game\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mTensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mVariable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"movie\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mTensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\andre\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\torch\\utils\\data\\dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    433\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_sampler_iter\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    434\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 435\u001b[1;33m         \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    436\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    437\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[1;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\andre\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\torch\\utils\\data\\dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    473\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    474\u001b[0m         \u001b[0mindex\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# may raise StopIteration\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 475\u001b[1;33m         \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# may raise StopIteration\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    476\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    477\u001b[0m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\andre\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     42\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     43\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 44\u001b[1;33m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0midx\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     45\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     46\u001b[0m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\andre\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     42\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     43\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 44\u001b[1;33m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0midx\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     45\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     46\u001b[0m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-2-13a9d4dabb2b>\u001b[0m in \u001b[0;36m__getitem__\u001b[1;34m(self, index)\u001b[0m\n\u001b[0;32m     25\u001b[0m             \u001b[0mimage_movie\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mto_rgb\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimage_movie\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     26\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 27\u001b[1;33m         \u001b[0mitem_game\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimage_game\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     28\u001b[0m         \u001b[0mitem_movie\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimage_movie\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     29\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;34m\"game\"\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mitem_game\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"movie\"\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mitem_movie\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\andre\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\torchvision\\transforms\\transforms.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, img)\u001b[0m\n\u001b[0;32m     65\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mimg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     66\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransforms\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 67\u001b[1;33m             \u001b[0mimg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mt\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     68\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mimg\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     69\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\andre\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    726\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 727\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[0;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\andre\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\torchvision\\transforms\\transforms.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, img)\u001b[0m\n\u001b[0;32m    265\u001b[0m             \u001b[0mPIL\u001b[0m \u001b[0mImage\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mRescaled\u001b[0m \u001b[0mimage\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    266\u001b[0m         \"\"\"\n\u001b[1;32m--> 267\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mresize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minterpolation\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    268\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    269\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__repr__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\andre\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\torchvision\\transforms\\functional.py\u001b[0m in \u001b[0;36mresize\u001b[1;34m(img, size, interpolation)\u001b[0m\n\u001b[0;32m    308\u001b[0m     \"\"\"\n\u001b[0;32m    309\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 310\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mF_pil\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mresize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msize\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minterpolation\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minterpolation\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    311\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    312\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mF_t\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mresize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msize\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minterpolation\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minterpolation\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\andre\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\torchvision\\transforms\\functional_pil.py\u001b[0m in \u001b[0;36mresize\u001b[1;34m(img, size, interpolation)\u001b[0m\n\u001b[0;32m    425\u001b[0m             \u001b[0moh\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msize\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    426\u001b[0m             \u001b[0mow\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msize\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mw\u001b[0m \u001b[1;33m/\u001b[0m \u001b[0mh\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 427\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mimg\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mresize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mow\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moh\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minterpolation\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    428\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    429\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mimg\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mresize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minterpolation\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\andre\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\PIL\\Image.py\u001b[0m in \u001b[0;36mresize\u001b[1;34m(self, size, resample, box, reducing_gap)\u001b[0m\n\u001b[0;32m   1920\u001b[0m                 )\n\u001b[0;32m   1921\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1922\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_new\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mim\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mresize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresample\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbox\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1923\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1924\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mreduce\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfactor\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbox\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import time, datetime, sys\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython import display\n",
    "%matplotlib inline\n",
    "\n",
    "D_losses = []\n",
    "GAN_losses = []\n",
    "NCE_losses = []\n",
    "G_total_losses = []\n",
    "\n",
    "prev_time = time.time()\n",
    "\n",
    "epochs = 200\n",
    "for epoch in range(epoch, epochs):\n",
    "    for i, batch in enumerate(dataloader):\n",
    "        x, y = Variable(batch[\"game\"].type(Tensor)), Variable(batch[\"movie\"].type(Tensor))\n",
    "        \n",
    "        # Adversarial ground truths\n",
    "        real = Variable(Tensor(np.ones((x.size(0), *D.output_shape))), requires_grad = False)\n",
    "        fake = Variable(Tensor(np.zeros((x.size(0), *D.output_shape))), requires_grad = False)\n",
    "        \n",
    "        # train discriminator\n",
    "        D.train()\n",
    "        optimizer_D.zero_grad()\n",
    "        # get the fake loss\n",
    "        fake_y = G(x)\n",
    "        D_fake = D(fake_y.detach())\n",
    "        loss_D_fake = criterion_GAN(D_fake, fake).mean()\n",
    "        # get the real loss\n",
    "        D_real = D(y)\n",
    "        loss_D_real = criterion_GAN(D_real, real).mean()\n",
    "        # combine loss and calculate gradients\n",
    "        loss_D = (loss_D_fake + loss_D_real) * 0.5\n",
    "        loss_D.backward()\n",
    "        optimizer_D.step()\n",
    "        \n",
    "        # train generator\n",
    "        G.train()\n",
    "        optimizer_G.zero_grad()\n",
    "        optimizer_Sampler.zero_grad()\n",
    "        # get the fake GAN loss\n",
    "        D_fake = D(fake_y)\n",
    "        loss_G_GAN = criterion_GAN(D_fake, real).mean()\n",
    "        # get the NCE loss\n",
    "        feat_q = G(fake_y, nce_layers, encode_only = True)\n",
    "        feat_k = G(x, nce_layers, encode_only = True)\n",
    "        \n",
    "        feat_k_pool, sample_ids = Sampler(feat_k, 256, None)\n",
    "        feat_q_pool, _ = Sampler(feat_q, 256, sample_ids)\n",
    "        \n",
    "        total_nce_loss = 0.0\n",
    "        for f_q, f_k, crit, nce_layer in zip(feat_q_pool, feat_k_pool, criterion_NCE, nce_layers):\n",
    "            loss = crit(f_q, f_k) * lambda_NCE\n",
    "            total_nce_loss += loss.mean()\n",
    "        \n",
    "        nce_loss = total_nce_loss / len(nce_layers)\n",
    "        \n",
    "        loss_G = loss_G_GAN + nce_loss\n",
    "        loss_G.backward()\n",
    "        optimizer_G.step()\n",
    "        optimizer_Sampler.step()\n",
    "        \n",
    "        # --------------\n",
    "        #  Log Progress\n",
    "        # --------------\n",
    "\n",
    "        # Determine approximate time left\n",
    "        batches_done = epoch * len(dataloader) + i\n",
    "        batches_left = 200 * len(dataloader) - batches_done\n",
    "        time_left = datetime.timedelta(seconds=batches_left * (time.time() - prev_time))\n",
    "        prev_time = time.time()\n",
    "\n",
    "        # Print log\n",
    "        sys.stdout.write(\n",
    "            \"\\r[Epoch %d/%d] [Batch %d/%d] [D loss: %f] [GAN loss: %f, NCE loss: %f, Total: %f] ETA: %s\"\n",
    "            % (\n",
    "                epoch,\n",
    "                200,\n",
    "                i,\n",
    "                len(dataloader),\n",
    "                loss_D.item(),\n",
    "                loss_G_GAN.item(),\n",
    "                nce_loss.item(),\n",
    "                loss_G.item(),\n",
    "                time_left,\n",
    "            )\n",
    "        )\n",
    "        \n",
    "        # If at sample interval save image\n",
    "        if batches_done % 300 == 0:\n",
    "            sample_images(batches_done)\n",
    "            \n",
    "    D_losses.append(loss_D.item())\n",
    "    GAN_losses.append(loss_G_GAN.item())\n",
    "    NCE_losses.append(nce_loss.item())\n",
    "    G_total_losses.append(loss_G.item())\n",
    "    display.clear_output(wait = True)\n",
    "    time.sleep(1)\n",
    "    plt.clf()\n",
    "    plt.ylabel('Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.plot(D_losses, color = 'r')\n",
    "    plt.plot(GAN_losses, color = 'g')\n",
    "    plt.plot(NCE_losses, color = 'b')\n",
    "    display.display(plt.gcf())\n",
    "    \n",
    "    if epoch % 5 == 0:\n",
    "        if not os.path.isdir('saved_models'):\n",
    "            os.mkdir('saved_models')\n",
    "        torch.save(G.state_dict(), \"saved_models/G_%d.pth\" % (epoch))\n",
    "        torch.save(D.state_dict(), \"saved_models/D_%d.pth\" % (epoch))\n",
    "        torch.save(Sampler.state_dict(), \"saved_models/Sampler_%d.pth\" % (epoch))\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "\n",
    "# Initialize encoders and decoders\n",
    "E_AB = Encoder((3,1024,480), 9)\n",
    "E_BA = Encoder((3,1024,480), 9)\n",
    "D_A = Decoder((3,1024,480))\n",
    "D_B = Decoder((3,1024,480))\n",
    "\n",
    "# Losses\n",
    "criterion_GAN = torch.nn.MSELoss()\n",
    "criterion_cycle = torch.nn.L1Loss()\n",
    "criterion_identity = torch.nn.L1Loss()\n",
    "\n",
    "if cuda:\n",
    "    E_AB = E_AB.cuda()\n",
    "    E_BA = E_BA.cuda()\n",
    "    D_A = D_A.cuda()\n",
    "    D_B = D_B.cuda()\n",
    "    criterion_GAN = criterion_GAN.cuda()\n",
    "    criterion_cycle = criterion_cycle.cuda()\n",
    "    criterion_identity = criterion_identity.cuda()\n",
    "    \n",
    "# Initialize weights\n",
    "E_AB.apply(weights_init_normal)\n",
    "E_BA.apply(weights_init_normal)\n",
    "D_A.apply(weights_init_normal)\n",
    "D_B.apply(weights_init_normal)\n",
    "\n",
    "# Optimizers\n",
    "optimizer_G = torch.optim.Adam(itertools.chain(E_AB.parameters(), E_BA.parameters()), lr = 0.0002, betas = (0.5, 0.999))\n",
    "optimizer_D_A = torch.optim.Adam(D_A.parameters(), lr = 0.0002, betas = (0.5, 0.999))\n",
    "optimizer_D_B = torch.optim.Adam(D_B.parameters(), lr = 0.0002, betas = (0.5, 0.999))\n",
    "\n",
    "# Learning rate update schedulers\n",
    "lr_scheduler_G = torch.optim.lr_scheduler.LambdaLR(\n",
    "    optimizer_G, lr_lambda = LambdaLR(200, 0, 100).step\n",
    ")\n",
    "lr_scheduler_D_A = torch.optim.lr_scheduler.LambdaLR(\n",
    "    optimizer_D_A, lr_lambda = LambdaLR(200, 0, 100).step\n",
    ")\n",
    "lr_scheduler_D_B = torch.optim.lr_scheduler.LambdaLR(\n",
    "    optimizer_D_B, lr_lambda = LambdaLR(200, 0, 100).step\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time, datetime, sys\n",
    "\n",
    "prev_time = time.time()\n",
    "for epoch in range(200):\n",
    "    for i, batch in enumerate(dataloader):\n",
    "\n",
    "        # Set model input\n",
    "        real_A = Variable(batch[\"game\"].type(Tensor))\n",
    "        real_B = Variable(batch[\"movie\"].type(Tensor))\n",
    "\n",
    "        # Adversarial ground truths\n",
    "        valid = Variable(Tensor(np.ones((real_A.size(0), *D_A.output_shape))), requires_grad = False)\n",
    "        fake = Variable(Tensor(np.zeros((real_A.size(0), *D_A.output_shape))), requires_grad = False)\n",
    "\n",
    "        # ------------------\n",
    "        #  Train Generators\n",
    "        # ------------------\n",
    "\n",
    "        E_AB.train()\n",
    "        E_BA.train()\n",
    "\n",
    "        optimizer_G.zero_grad()\n",
    "\n",
    "        # Identity loss\n",
    "        loss_id_A = criterion_identity(E_BA(real_A), real_A)\n",
    "        loss_id_B = criterion_identity(E_AB(real_B), real_B)\n",
    "\n",
    "        loss_identity = (loss_id_A + loss_id_B) / 2\n",
    "\n",
    "        # GAN loss\n",
    "        fake_B = E_AB(real_A)\n",
    "        loss_GAN_AB = criterion_GAN(D_B(fake_B), valid)\n",
    "        fake_A = E_BA(real_B)\n",
    "        loss_GAN_BA = criterion_GAN(D_A(fake_A), valid)\n",
    "\n",
    "        loss_GAN = (loss_GAN_AB + loss_GAN_BA) / 2\n",
    "\n",
    "        # Cycle loss\n",
    "        recov_A = E_BA(fake_B)\n",
    "        loss_cycle_A = criterion_cycle(recov_A, real_A)\n",
    "        recov_B = E_AB(fake_A)\n",
    "        loss_cycle_B = criterion_cycle(recov_B, real_B)\n",
    "\n",
    "        loss_cycle = (loss_cycle_A + loss_cycle_B) / 2\n",
    "\n",
    "        # Total loss\n",
    "        loss_G = loss_GAN + 10 * loss_cycle + 5 * loss_identity\n",
    "\n",
    "        loss_G.backward()\n",
    "        optimizer_G.step()\n",
    "\n",
    "        # -----------------------\n",
    "        #  Train Discriminator A\n",
    "        # -----------------------\n",
    "\n",
    "        optimizer_D_A.zero_grad()\n",
    "\n",
    "        # Real loss\n",
    "        loss_real = criterion_GAN(D_A(real_A), valid)\n",
    "        # Fake loss (on batch of previously generated samples)\n",
    "        fake_A_ = fake_A_buffer.push_and_pop(fake_A)\n",
    "        loss_fake = criterion_GAN(D_A(fake_A_.detach()), fake)\n",
    "        # Total loss\n",
    "        loss_D_A = (loss_real + loss_fake) / 2\n",
    "\n",
    "        loss_D_A.backward()\n",
    "        optimizer_D_A.step()\n",
    "\n",
    "        # -----------------------\n",
    "        #  Train Discriminator B\n",
    "        # -----------------------\n",
    "\n",
    "        optimizer_D_B.zero_grad()\n",
    "\n",
    "        # Real loss\n",
    "        loss_real = criterion_GAN(D_B(real_B), valid)\n",
    "        # Fake loss (on batch of previously generated samples)\n",
    "        fake_B_ = fake_B_buffer.push_and_pop(fake_B)\n",
    "        loss_fake = criterion_GAN(D_B(fake_B_.detach()), fake)\n",
    "        # Total loss\n",
    "        loss_D_B = (loss_real + loss_fake) / 2\n",
    "\n",
    "        loss_D_B.backward()\n",
    "        optimizer_D_B.step()\n",
    "\n",
    "        loss_D = (loss_D_A + loss_D_B) / 2\n",
    "\n",
    "        # --------------\n",
    "        #  Log Progress\n",
    "        # --------------\n",
    "\n",
    "        # Determine approximate time left\n",
    "        batches_done = epoch * len(dataloader) + i\n",
    "        batches_left = 200 * len(dataloader) - batches_done\n",
    "        time_left = datetime.timedelta(seconds=batches_left * (time.time() - prev_time))\n",
    "        prev_time = time.time()\n",
    "\n",
    "        # Print log\n",
    "        sys.stdout.write(\n",
    "            \"\\r[Epoch %d/%d] [Batch %d/%d] [D loss: %f] [G loss: %f, adv: %f, cycle: %f, identity: %f] ETA: %s\"\n",
    "            % (\n",
    "                epoch,\n",
    "                200,\n",
    "                i,\n",
    "                len(dataloader),\n",
    "                loss_D.item(),\n",
    "                loss_G.item(),\n",
    "                loss_GAN.item(),\n",
    "                loss_cycle.item(),\n",
    "                loss_identity.item(),\n",
    "                time_left,\n",
    "            )\n",
    "        )\n",
    "        \n",
    "        # If at sample interval save image\n",
    "        if batches_done % 300 == 0:\n",
    "            sample_images(batches_done)\n",
    "\n",
    "    # Update learning rates\n",
    "    lr_scheduler_G.step()\n",
    "    lr_scheduler_D_A.step()\n",
    "    lr_scheduler_D_B.step()\n",
    "\n",
    "    if epoch % 10 == 0:\n",
    "        # Save model checkpoints\n",
    "        if not os.path.isdir('saved_models'):\n",
    "            os.mkdir('saved_models')\n",
    "        torch.save(E_AB.state_dict(), \"saved_models/E_AB_%d.pth\" % (epoch))\n",
    "        torch.save(E_BA.state_dict(), \"saved_models/E_BA_%d.pth\" % (epoch))\n",
    "        torch.save(D_A.state_dict(), \"saved_models/D_A_%d.pth\" % (epoch))\n",
    "        torch.save(D_B.state_dict(), \"saved_models/D_B_%d.pth\" % (epoch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Contrastive Unpaired Translation:\n",
    "# Input x, output y\n",
    "# y = G_dec(G_enc(x))\n",
    "# f_q are sampled features from H(G_enc(x))\n",
    "# f_k are sampled features from H(G_enc(y))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
